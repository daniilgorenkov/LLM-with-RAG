{"text": "Contents lists available at ScienceDirect Accident Analysis and Prevention journal homepage: www.elsevier.com/locate/aap## Railroad accident analysis using extreme gradient boosting## Raj Bridgelall[a][,] *, Denver D. Tolliver[b ]a Department of Transportation, Logistics & Finance, College of Business, North Dakota State University, Fargo, ND, 58108, United Statesb Upper Great Plains Transportation Institute, North Dakota State University, Fargo, ND, 58108, United StatesA R T I C L E I N F O A B S T R A C T Keywords: Railroads are critical to the economic health of a nation. Unfortunately, railroads lose hundreds of millions of Data cleaning dollars from accidents each year.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 0, "year": "2017"}}
{"text": "al to the economic health of a nation. Unfortunately, railroads lose hundreds of millions of Data cleaning dollars from accidents each year. Trends reveal that derailments consistently account for more than 70 % of the Feature engineering U.S. railroad industry’s average annual accident cost. Hence, knowledge of explanatory factors that distinguish Financial loss derailments from other accident types can inform more cost-effective and impactful railroad risk management Machine learning strategies. Five feature scoring methods, including ANOVA and Gini, agreed that the top four explanatory factors Principle component analysis Risk management in accident type prediction were track class, type", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 1, "year": "2017"}}
{"text": "greed that the top four explanatory factors Principle component analysis Risk management in accident type prediction were track class, type of movement authority, excess speed, and territory signali zation. Among 11 different types of machine learning algorithms, the extreme gradient boosting method was most effective at predicting the accident type with an area under the receiver operating curve (AUC) metric of 89 %. Principle component analysis revealed that relative to other accident types, derailments were more strongly associated with lower track classes, non-signalized territories, and movement authorizations within restricted limits.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 2, "year": "2017"}}
{"text": "ts were more strongly associated with lower track classes, non-signalized territories, and movement authorizations within restricted limits. On average, derailments occurred at 16 kph below the speed limit for the track class whereas other ac cident types occurred at 32 kph below the speed limit. Railroads can use the integrated data preparation, ma chine learning, and feature ranking framework presented to gain additional insights for managing risk, based on their unique operating environments.## **1. Introduction**U.S. railroads have been an important driver of economic progress for more than 150 years. Today, U.S.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 3, "year": "2017"}}
{"text": "ng environments.## **1. Introduction**U.S. railroads have been an important driver of economic progress for more than 150 years. Today, U.S. railroads carry approximately onethird of the nation’s exports (ASCE, 2017). Therefore, the safe and ’s economic health. efficient operation of railroads is crucial to the nation Unfortunately, railroads lose hundreds of millions of dollars from acci dents each year. Analysis of the Federal Railroad Administration (FRA) Rail Equipment Accident database revealed that human-factors was consistently the dominant cause of railroad accidents (Bridgelall and Tolliver, 2020).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 4, "year": "2017"}}
{"text": "ent Accident database revealed that human-factors was consistently the dominant cause of railroad accidents (Bridgelall and Tolliver, 2020). Hence, the federal government mandated that railroads deploy a positive train control (PTC) system by 2018 to help prevent accidents caused by human errors (Zhang et al., 2018). With PTC now in place, it is important for analysts to study other common causes of accidents.The **goal** of this research is to identify factors associated with the most frequent and expensive types of accidents that are not attributable to human error. Data mining of FRA accident records from January 1, 2009, to June 30, 2020, revealed that derailment accidents accounted for", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 5, "year": "2017"}}
{"text": "o human error. Data mining of FRA accident records from January 1, 2009, to June 30, 2020, revealed that derailment accidents accounted for 70.  % of the average annual financial loss (Fig. 1). The trendshowed that derailment accidents maintained a steady rate each year. Therefore, the ability to identify and rank features that increase the risk of derailments over other accident types can inform more cost-effective and impactful risk management strategies (Ghofrani et al., 2018).An **objective** of this research is to build a supervised machine learning (ML) model that can predict derailments from other accident types and to rank the importance of those features that contribute to wards the", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 6, "year": "2017"}}
{"text": " (ML) model that can predict derailments from other accident types and to rank the importance of those features that contribute to wards the classification accuracy. However, no single type of ML model performs best on all types of datasets (Murphy, 2012). Therefore, another objective is to compare the classification performance of various types of ML models on the same dataset.One of the main challenges in data science is to effectively clean datasets before using them to train ML models. Studies estimate that dirty data costs the U.S. economy trillion of dollars each year (Ilyas and Chu, 2019). A survey of data cleaning for ML found that the failure to discover and repair dirty data can we", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 7, "year": "2017"}}
{"text": "of dollars each year (Ilyas and Chu, 2019). A survey of data cleaning for ML found that the failure to discover and repair dirty data can weaken data analysis techniques (Jesmeen et al., 2018). Although a few approaches to data cleaning are common, every dataset poses unique challenges (Bridgelall et al., 2018). Hence, data scientists spend an average of 60 % of their time cleaning and organizing data (Ilyas and Chu, 2019).Although the importance of using clean data is well-known, the research community has paid little attention to the advancement of data cleaning techniques (Rahm and Do, 2000). The most commonly used techniques are those that detect and remove outliers and duplicate re cord", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 8, "year": "2017"}}
{"text": "a cleaning techniques (Rahm and Do, 2000). The most commonly used techniques are those that detect and remove outliers and duplicate re cords (Ilyas and Chu, 2019). Even so, those techniques alone cannot effectively clean all types of datasets. Other techniques that can find data entry errors use customized rules to detect violations, for example, house prices exceeding an expected range for a given neighborhood. Custom techniques tend to be heuristic, so they require good familiarity with the data and its meaning. Considering the challenges outlined above, the following are **contributions** of this research:modeling to HRGC crash data and found that gate violations were more highly associa", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 9, "year": "2017"}}
{"text": "ve, the following are **contributions** of this research:modeling to HRGC crash data and found that gate violations were more highly associated with two-quadrant than four-quadrant gates (Liu and Khattak, 2017). Karamati et al. (2020) applied random survival forest to HRGC crash data and found that adding audible alarm devices to crossings that already have gates and flashing lights can decrease crash likelihood by approximately 50 % (Keramati et al., 2020). Soleimani et al. (2019) used extreme gradient boosting to identify HRGCs that should be closed to prevent accidents (Soleimani et al., 2019). Wali et al.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 10, "year": "2017"}}
{"text": "al. (2019) used extreme gradient boosting to identify HRGCs that should be closed to prevent accidents (Soleimani et al., 2019). Wali et al. (2021) applied text mining to crash narrative data of railroad trespass ing incidents and found that confirmed suicide attempts and the use of headphones or cellphones were more likely to result in fatal injuries (Wali et al., 2021).- A customized framework to clean a relevant subset of the FRA database and to fill 100 % of missing values for the important at tributes .- Interpreting the importance ranking of the feature relevance in predicting accident type .- Visualizing and interpreting the classification power of each attri bute by principle compone", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 11, "year": "2017"}}
{"text": "ature relevance in predicting accident type .- Visualizing and interpreting the classification power of each attri bute by principle component analysis (PCA) to gain insights about the performance differences among the ML models evaluated (Sec tions 3.  and 4.3).The next section  reviews related works and their findings in relation to the contributions of this research. Section 4 mirrors sub sections of the methods section to present the results. Section 5 discusses 6 the significance and interprets the outcome. Section recaps the find ings and concludes with how future research can leverage the methods of this research to further the agenda in accident analysis.The survey of Ghofrani (2018)", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 12, "year": "2017"}}
{"text": " with how future research can leverage the methods of this research to further the agenda in accident analysis.The survey of Ghofrani (2018) demonstrated that researchers have also used ML methods to analyze other aspects of railroad operations besides safety (Ghofrani et al., 2018). For example, Li et al. (2014) used ML to learn rules from historical and real-time data to predict railroad maintenance needs (Li et al., 2014). Lasisi and Attoh-Okine (2019) proposed a combination of ensemble tree-based ML models to predict rail fatigue defects and achieved an AUC score of 0.  (Lasisi and Attoh-Okine, 2019).## **2.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 13, "year": "2017"}}
{"text": "ion of ensemble tree-based ML models to predict rail fatigue defects and achieved an AUC score of 0.  (Lasisi and Attoh-Okine, 2019).## **2. Related works**The benchmarking of ML performance is subjective because of its high relevance to the target problem of a particular study in a particular field (Olson et al., 2017). For example, a performance score considered to be “good” in the biotech industry when evaluating vaccine efficacy may be considered “poor” in the automotive industry when evaluating defective unit batches. Subjective performance assessments depend on the level of “acceptable” risk for a given application (Cook, 2007).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 14, "year": "2017"}}
{"text": "ng defective unit batches. Subjective performance assessments depend on the level of “acceptable” risk for a given application (Cook, 2007). Therefore, model evaluation often use the fuzzy academic grading sys tem to assess and compare performance levels (Echauz and Vachtseva nos, 1995).## **3. Methodology**Several studies used ML techniques to analyze highway-rail grade crossing (HRGC) accidents. Dabbour et al. (2017) applied ordered regression models to HRGC crash data and found that higher train and vehicle speeds were positively correlated with driver injury severity (Dabbour et al., 2017). Liu and Khattak (2017) applied geospatialThe input layer gathers the datasets and prepares the com", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 15, "year": "2017"}}
{"text": "r injury severity (Dabbour et al., 2017). Liu and Khattak (2017) applied geospatialThe input layer gathers the datasets and prepares the combined data by applying various methods to reduce noise, repair data entry errors, and fill in missing values. The processing layer prepares relevant attri butes to train and tune the ML models. The processing layer led to the**Fig. 1.** Annual financial loss reported for different accident types.discovery of additional errors that made some features irrelevant. In such cases, the framework logic looped back to the data preparation module to address the issue. The model building and validation pro cesses also contained a loop that converged after the ML p", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 16, "year": "2017"}}
{"text": "ta preparation module to address the issue. The model building and validation pro cesses also contained a loop that converged after the ML performance stabilized. The final layer ranked the importance of attributes in clas sification performance and used PCA to visualize the results for interpretation.in other attributes.|(Liu et al., 2012)| ||Noise|Attribute is not relevant to the target class.|(Rahm and Do,| ||||2000)| ||Dispersion\nCombinable|Attribute has low variance or carries little or\nno information.\nAttribute that can combine with others\nwithout losing information.|(Bridgelall et al.,\n2018)\n(Ilyas and Chu,\n2019)|## 3.1.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 17, "year": "2017"}}
{"text": "r\nno information.\nAttribute that can combine with others\nwithout losing information.|(Bridgelall et al.,\n2018)\n(Ilyas and Chu,\n2019)|## 3.1. Data sourceThe comprehensive Federal Railroad Administration (FRA) Rail Equipment Accident database provided the main dataset for the analysis (Bridgelall and Tolliver, 2020). Some of the data schema became inconsistent after the FRA changed reporting requirements for a few of the fields starting June 1, 2011 (FRA, 2011). For example, the report added a field to indicate if the accident occurred in a signalized territory. Hence, there was no entry for the “SIGNAL” field prior to the switchover date.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 18, "year": "2017"}}
{"text": " indicate if the accident occurred in a signalized territory. Hence, there was no entry for the “SIGNAL” field prior to the switchover date. Similarly, a field indicating the method of operation (“MOPERA”) replaced the “METHOD” field that encoded similar information.Merging 8055 records from 2009 to 2011 with 21,242 records from 2012 to June 2020 produced a total of 29,297 records with 145 attri butes. Consequently, 22 % and 79 % of the data was missing in the “MOPERA” and “METHOD” fields, respectively. The accident reporting form also added a field “SSB1” to indicate if the track was a continuously welded (CWR) or other.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 19, "year": "2017"}}
{"text": "elds, respectively. The accident reporting form also added a field “SSB1” to indicate if the track was a continuously welded (CWR) or other. Hence, the “SSB1” field was mostly empty prior to June 1, 2011.16 with_>90 % zero-flled (e.g. CABOOSE1,|118−16=102| |||EVACATE, MIDREM1)|| ||Correlated|12 with>90 % correlation with other attributes|102−12=90| ||Redundancy|(e.g. PASSINJ, PASSKLD)\n7 that were redundant with others (e.g. CNTYCD,|90−7=83| |||STATE, COUNTY)|| ||Noise|6 with no relevance to the target (e.g. train|83−6=77| ||Noise", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 20, "year": "2017"}}
{"text": "target (e.g. ADJUNCT1, DIV)\nHUMANS=|77−6=71\n71–| ||Correlated|(engineers+fremen+conductor+brakemen),\ndrop 4.\nEQATT (equipment attended) correlates with|4+1=68\n68–1=67| ||Combinable|HUMANS, drop 1\nCombine 15 narrative felds into a single feld\n(NARR), drop original 15.|67–\n15+1=53| ||Combinable|Fill missing MOPERA (method of operation) data|53–1=52| |||with METHOD, drop 1.||## 3.2. Data processing## 3.2.1. Data cleaningTable 1 describes criteria used to clean the data by eliminating irrelevant attributes or features. The classification of those criteria is a heuristic synthesis by the authors based on a broad understanding from the literature, which the table cites for further explanation.Tabl", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 0, "year": "2017"}}
{"text": " a heuristic synthesis by the authors based on a broad understanding from the literature, which the table cites for further explanation.Table 2 chronicles each criterion used to reduce the number of fields from 145 to 52.## 3.2.2. Data extractionPassenger trains accounted for a small portion 8.  % (2354) of accidents. Removing those records enhanced the consistency of the dataset, which is known to improve the ML performance (Murphy,2012 ). A side benefit was that the eliminated records also removed a few attributes that were associated with passenger trains only. Table 3 chronicles the net reduction of attributes from 52  to 50, and records from 29,297 to 15,087.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 1, "year": "2017"}}
{"text": "associated with passenger trains only. Table 3 chronicles the net reduction of attributes from 52  to 50, and records from 29,297 to 15,087. The statistics shown in the table are the number of records (N), number of attributes or variables (V), and “DERAILED” number of metadata fields (M). Adding the target attribute indicated if the accident was a derailment type or not, and it became the label for supervised ML.POS_CAR|48−1+1=48\n48+1−1=48|Log Transform: TONS, then drop old.\nRename and recode POSITON1 (position of frst| |||involved car) as the fractional position relative to| |||the number of cars.  is front, 1 is back.| |N_CARS|48+1=49|Add N_CARS as the sum of loaded and empty cars.| |CARS", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 2, "year": "2017"}}
{"text": "l position relative to| |||the number of cars.  is front, 1 is back.| |N_CARS|48+1=49|Add N_CARS as the sum of loaded and empty cars.| |CARS_LD|49+1−4=46|Add CARS_LD as proportion of N_CARS loaded.| |||Drop: LOADF1, EMPTYF1, POSITON1, PASSTRN| |CARS_HZMT|46+1−1=46|Add CARS_HZMT as proportion of CARS_LD that| |||carry Hazmat. Drop CARS (number of cars| |||carrying hazmat)| |SPD_OVR|46+1–1=46|Add to capture difference in train speed and speed\nlimit for CLASS_TRK. Dropped feld HIGHSPD.| |Metadata|46−6=40|Converted 6 attributes (REC_ID, SC, STATION,| |||RAILROAD, RR3, IYR) to metadata: 5+6=11.|## 3.2.3. Attribute transformationML algorithms tend to perform poorly on data with attributes that hav", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 3, "year": "2017"}}
{"text": "ROAD, RR3, IYR) to metadata: 5+6=11.|## 3.2.3. Attribute transformationML algorithms tend to perform poorly on data with attributes that have a highly skewed distribution because the model could treat data in long tails as outliers or because extreme values provide insufficient ex amples (Manning and Mullahy, 2001). A shifted natural logarithm LN(1 + x ) reduced the skew and prevented an undefined number if attribute value x is zero.Another transformation that can help to reduce the dimension of a dataset is to replace a set of related attributes with proportions of a base attribute. The proportion transformation retained information about the relative relationship among attributes while nor", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 4, "year": "2017"}}
{"text": "oportions of a base attribute. The proportion transformation retained information about the relative relationship among attributes while normalizing the values within the  range. Table 4 chronicles the transformation of attri butes and their effect on reducing the number of attributes from 50  to 40.## 3.2.4. Feature selectionLOADED_1|40−1=39\n39−1=38|Relative position of the frst involved car in the train.\nBoolean: Is frst involved car loaded? Missing (22 %,| |||6568)| |ACCDMG|38−1=37|Total reported damage in U.S. dollars.| |CASKLD|37−1=36|Total killed for all involved railroads.| |CASINJ|36−1=35|Total injured for all involved railroads.| |CARSHZD|35−1=34|Number of cars that released hazardo", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 5, "year": "2017"}}
{"text": "or all involved railroads.| |CASINJ|36−1=35|Total injured for all involved railroads.| |CARSHZD|35−1=34|Number of cars that released hazardous materials.| |CARSDMG|34−1=33|Number of cars damaged or derailed.| |POSITON2|33−1=32|Position of car on the train that caused the accident.| |EMPTYF2|32−1=31|Number of empty freight cars that derailed.| |LOADF2|31−1=30|Number of loaded freight cars that derailed.| |HEADEND2|30−1=29|Number of headend locomotives that derailed.| |ACC_TYPE|29−1=28|Type of accident. Missing (0%, 83).| |ACC_CAT|28−1=27|Accident cause category.| |CAUSE|27−1=26|Accident cause code.| |MATCH|26−1=25|Temporary geospatial flter fag for county mismatch.|Predictive modeling should", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 6, "year": "2017"}}
{"text": "tegory.| |CAUSE|27−1=26|Accident cause code.| |MATCH|26−1=25|Temporary geospatial flter fag for county mismatch.|Predictive modeling should not contain attributes where values are known only after the outcome. Therefore, the cleaning procedure eliminated post-event attributes such as the number of people injured, killed, or evacuated. Table 5 chronicles the feature reduction from 40  to 25.- 1) Packaged similar features of an attribute to simplify the categories. 2) Converted categorical attributes that have some ranking to ordinal attributes.- 3) Binarized categorical attributes that contained only two values by replacing one value with zero and the other with one.- 4) Replaced nominal valu", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 7, "year": "2017"}}
{"text": "zed categorical attributes that contained only two values by replacing one value with zero and the other with one.- 4) Replaced nominal values with a single word label to enhance the ease of interpreting trends with more descriptive legends.work with missing data, but most cannot (Abidin et al., 2018). There fore, data scientists developed a few methods to impute or guess missing values. Common approaches are to replace missing values with the mean, median, most frequent, random, or zero value. More intelligent approaches use tree-based ML techniques to fill missing values with those of their nearest neighbors in feature space .", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 8, "year": "2017"}}
{"text": "ue. More intelligent approaches use tree-based ML techniques to fill missing values with those of their nearest neighbors in feature space . However, the existing methods did not enhance the contribution of the affected attributes to wards predicting the target class. Therefore, this research developed a new method, dubbed local association pivot (LAP), to replace missing values based on spatial proximity rather than feature space proximity. The LAP method first creates a pivot table that aggregates non-missing values by a spatial location identifier and by sub-location identifiers if available. The method then merges the pivot table with the dataset by using the main location identifier as", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 9, "year": "2017"}}
{"text": " by sub-location identifiers if available. The method then merges the pivot table with the dataset by using the main location identifier as the unique merge key. The aggre gation method for the pivot depends on the type of missing data. For example, for numerical values such as track density, the method used the maximum of the aggregated value for a location. The method did not use the average value because zero or missing values created an undesirable bias in the aggregation. A fringe benefit of using the LAP method is that it is easy to spot data entry or spelling errors by examining a sorted list of the unique location keys.## 3.2.6.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 10, "year": "2017"}}
{"text": "ing the LAP method is that it is easy to spot data entry or spelling errors by examining a sorted list of the unique location keys.## 3.2.6. Data imputationV: 52+1=53|Add accident category based on accident cause\ncode:\n{Track, Equipment, Human, Signal,| ||M: 5|Miscellaneous}.| ||N: 26,943 (92|Dropped accidents involving passenger type trains| |PASSTRN|%)\nV: 53−4=49|(PASSTRN=Y).\nDropped associated attributes:| ||M: 5|LOADP1, LOADP2, EMPTYP1, EMPTYP2.| ||N: 26,943 (92|| |DERAILED|%)\nV: 49+1=50|Added“Derailed”as the target attribute.| ||M: 5|| ||N: 25,035|Dropped records where the accident cause was\nmissing, 7% (1908)| ||N: 15,088|Dropped records where human factors were a\ncause, 39.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 11, "year": "2017"}}
{"text": "N: 25,035|Dropped records where the accident cause was\nmissing, 7% (1908)| ||N: 15,088|Dropped records where human factors were a\ncause, 39.  % (9947)| ||N: 15,087|Dropped 1 record with a missing value for\nWEATHER.|Table 7 summarizes the results of the imputing missing values and the impact of each method. Missing or erroneous geospatial coordinates are impossible to impute or correct if no other spatial information is available in the dataset. The state or county name provides a coarseSummary of Feature Engineering.|**Table 7**\nSummary of Data Imputation.| |---|---| |location identifer that can be helpful for visualizing data on maps.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 12, "year": "2017"}}
{"text": "eature Engineering.|**Table 7**\nSummary of Data Imputation.| |---|---| |location identifer that can be helpful for visualizing data on maps.\nHowever, a coarse location such as a large state may introduce bias in\nthe ML process. Fortunately, the FRA database contains the station name\nthat is closest to the accident location, so its location can be a surrogate\nfor missing geospatial coordinates.\n3.2.7. Geospatial coordinate repair\nAside from missing geospatial coordinates, data entry errors resulted\nin erroneous or highly skewed geospatial locations.Fig. 3shows the\npositions of the recorded geospatial coordinates relative to a map of the\ncontinental United States.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 13, "year": "2017"}}
{"text": "ed geospatial locations.Fig. 3shows the\npositions of the recorded geospatial coordinates relative to a map of the\ncontinental United States. There is an observable systematic skew to\nwards the southeast This skew suggested that there was a lack of res\nAttribute\nProcedure\nCWR\nRenamed SSB1 to CWR (continuously welded rail); binarized as“1′′ =\n“CWR”and“0”otherwise.\nLOADED1\nBinarized as“1”=“Y”(frst involved car loaded?) and“0”=“N”for\nnon-empty values.\nWEATHER\nRecoded nominal values in WEATHER as labels {Clear, Cloudy, Rain,\nFog, Sleet, Snow}\nTRK_TYP\nRenamed TYPTRK (track type) and labeled nominal codes as {Main,\nYard, Siding, Industry}\nVISION", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 14, "year": "2017"}}
{"text": "lear, Cloudy, Rain,\nFog, Sleet, Snow}\nTRK_TYP\nRenamed TYPTRK (track type) and labeled nominal codes as {Main,\nYard, Siding, Industry}\nVISION\nRenamed VISIBLTY and replaced nominal codes as descriptive {Dawn,\nDay, Dusk, Dark}\nCLASS_RR\nRenamed TYPRR (railroad class) and cleaned to contain only values\nfrom 1 to 6.\nCLASS_TRK\nRenamed TRKCLAS (track class) and cleaned to contain ordinal values\nfrom 0 to 9 (X→0)\nCONSIST\nRenamed TYPEQ (consist type); repackaged as {freight, passenger,\nlocomotive, cars, work, yard}.\n{1}→ “Freight”, {2, 3, B, C}→ “Passenger”, {8, D, E}→ “Locomotive”,\n{5, 6}→ “Cars”, {4, 9, A}→ “Work”, {7}→ “Yard”\nACC_TYPE\nRenamed TYPE (accident type); repackaged as category labels:", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 15, "year": "2017"}}
{"text": "8, D, E}→ “Locomotive”,\n{5, 6}→ “Cars”, {4, 9, A}→ “Work”, {7}→ “Yard”\nACC_TYPE\nRenamed TYPE (accident type); repackaged as category labels:\n{1}→ “Derail”, {2, 3, 6}→ “Collide”, {4}→ “Collide (Side)”, {5}→\n“Collide (Rake)”,\n{7, 8}→ “RGC”, {9}→ “Obstruct”, {10, 11}→ “Fire”, {12, 13}→\n“Other”\nMOVEx\nRenamed MOPERA; repackaged as labels {signal, control, restrict,\nblocks, not main}\n{1, D}→ “Signal”, {2, A, B, C, P}→ “Control”, {3, L, M, I}→\n“Restrict”,\n{4, E, F, G, H, J, K}→ “Blocks”, {5, N, O}→ “Not Main”|Attribute\nMissing\nBefore\nMissing\nAfter\nProcedure (N=29,297, V=49, M=3)| ||TRK_DEN\n51 %\n(15,176)\n0% (0)\nPivot STATION by TRK_TYP, aggregated\nas maximum TRK_DNSTY (track density).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 16, "year": "2017"}}
{"text": "r\nProcedure (N=29,297, V=49, M=3)| ||TRK_DEN\n51 %\n(15,176)\n0% (0)\nPivot STATION by TRK_TYP, aggregated\nas maximum TRK_DNSTY (track density).\nFill missing data associated with the track\ntype if defned, otherwise use the\nmaximum value.\nSIG\n22 %\n(6473)\n0%, 0\nPivot STATION by TRK_TYP, aggregated\nas net count SIGNAL (signalized\nterritory). Fill missing data as“1”if net\ncount associated with the track type is\ngreater than 0, otherwise fll with“0”\nCONSIST\n39 %\n(11,537)\n8%\n(2605)\nLayer 1: Fill missing CONSIST with:", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "6 with>_20 % missing or no relevance to the", "level": 1, "chunk_id": 17, "year": "2017"}}
{"text": "8%\n(2605)\n2% (844)\nLayer 2: Fill missing CONSIST with:\n“Freight”if CLASS_RR is“1”(except\n“Amtrak”) otherwise\n“Passenger”if RAILROAD (reporting\nrailroad) is“Amtrak”\n2% (844)\n1% (377)\nLayer 3: Fill missing CONSIST with:\n“Work Train”if TRK_TYP is not“Main”\n1% (377)\n0% (0)\nLayer 4: Fill missing CONSIST with:\n“Work Train”if TONS (gross tons,\nexcluding locomotives) is 0\notherwise fll missing CONSIST with\n“Freight”if TONS_>_0\nCWR\n21 %\n(6378)\n0% (0)\nFill missing values with“1”if TRK_TYP is\n“main”and“0”otherwise.\nMOVEx\n0% (518)\n0% (0)\nFill missing MOVEx based on SIGNAL or\nTRK_TYP.\nPASSTRN\n6%,\n(2049)\n0% (0)\nFill missing PASSTRN based on CONSIST.\nCheck original fag for consistency with", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 0, "year": "2017"}}
{"text": "g MOVEx based on SIGNAL or\nTRK_TYP.\nPASSTRN\n6%,\n(2049)\n0% (0)\nFill missing PASSTRN based on CONSIST.\nCheck original fag for consistency with\nthe type CONSIST and the sum of freight\nand passenger cars (loaded or empty).\nFlip the fag accordingly.\nCLASS_RR\n0%, (37)\n0% (0)\nFill missing CLASS_RR (railroad class) by\ninternet search:\nBLF→2, {DD, METC}→3, CN→1\nTRK_TYP\n0%, (15)\n0% (0)\nFill missing TRK_TYP (track type) by\ninference from the metadata.\nCLASS_TRK\n0%, (25)\n0% (0)\nFill missing CLASS_TRK (track class) by\ninference from the metadata.|Aside from missing geospatial coordinates, data entry errors resulted in erroneous or highly skewed geospatial locations. Fig.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 1, "year": "2017"}}
{"text": "the metadata.|Aside from missing geospatial coordinates, data entry errors resulted in erroneous or highly skewed geospatial locations. Fig.  shows the positions of the recorded geospatial coordinates relative to a map of the continental United States. There is an observable systematic skew to wards the southeast. This skew suggested that there was a lack of res olution for those coordinates because in North America, lower resolution latitude and longitude coordinates would bias towards the south and east, respectively. The result was that 21.  % of the records had erroneous geospatial coordinates because their locations on the map did not match the counties reported for the accidents.## 3.2", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 2, "year": "2017"}}
{"text": "records had erroneous geospatial coordinates because their locations on the map did not match the counties reported for the accidents.## 3.2.8. Outlier removalSacrificing a few outlier data points to reduce bias can improve the generalization of a model. Outlier data instances are few and different from the bulk of the dataset (Liu et al., 2012). They could represent noisy data entries or rare events that can bias the training of an ML model, resulting in poor predictive performance. The framework used four methods to compare their effect on the model performance:- One class SVM (OCS) with a radial basis function (RBF) kernel (OCSRBF)Table 8 chronicles the progress of filling missing geospat", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 3, "year": "2017"}}
{"text": "rformance:- One class SVM (OCS) with a radial basis function (RBF) kernel (OCSRBF)Table 8 chronicles the progress of filling missing geospatial co ordinates in each step of the procedure. The LAP method used all records prior to data reduction and filled missing values with the mean value of the non-zero latitude and longitude values for that track type near the station, otherwise the method used the maximum value.Table 10 summarizes the AUC performance metric for a random forest classifier after removing outliers using each of the four methods, with the various hyperparameter selections shown. All algorithm and parameter selection produced similar performance.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 4, "year": "2017"}}
{"text": "h of the four methods, with the various hyperparameter selections shown. All algorithm and parameter selection produced similar performance. The framework used the LOF algorithm with 20 nearest neighbors and 1% outliers because of its slight AUC performance edge. The method removed 126 outliers to result in 15,087–126 = 14,961 records used to train and evaluate the ML models.Table 9 summarizes the final set of 25 attributes used to build the ML models. One-hot-encoding the categorical attributes increased the num ber of features from 25 to 51. Dispersion represents the relative amount of variability (information) that each attribute contributes to the overall variance in the data.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 5, "year": "2017"}}
{"text": " Dispersion represents the relative amount of variability (information) that each attribute contributes to the overall variance in the data. The dispersion measure is the entropy and coeffi cient of variation (CV) for categorical and numerical attributes,**Fig. 3.** Positions of the recorded geospatial coordinates in the FRA database.## 3.3.1. Supervised classification modelsTable 11 summarizes the 11 different types of ML models used in this analysis. The table provides a brief description of how each algorithm works, their most important hyperparameters (HP), their overall ad vantages (A) and disadvantages (D).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 6, "year": "2017"}}
{"text": "ief description of how each algorithm works, their most important hyperparameters (HP), their overall ad vantages (A) and disadvantages (D). The table groups the models into four broader categories based on their underlying theory of operation: tree-based methods, statistical models, decision boundaries, and learned functions. Numerous excellent books describe the mathematics and theory of operations for each model; they are incorporated here by reference. G´eron (2017) discusses both the theory and practical imple mentation of decision tree (DT), random forest (RF), AdaBoost (AB), logistic regression (LR), support vector machine (SVM), stochasticgradient descent (SGD), and artificial neural", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 7, "year": "2017"}}
{"text": " forest (RF), AdaBoost (AB), logistic regression (LR), support vector machine (SVM), stochasticgradient descent (SGD), and artificial neural network methods (G´eron, 2017). Jame et al. (2013) discusses both the theory and practical implementation of Naïve Bayes (NB), knearest-neighbors (kNN), and tree-based boosting methods (James et al., 2013). Hastie et al. (2016) provides similar coverage for all the models used in this analysis, including some key ML concepts such as bootstrapping, boosting, bagging, and ensemble learning (Hastie et al., 2016). Murphy (2012) covers the various methods from a more theoretical and probabilistic perspective (Murphy, 2012).(FP) rate as a function of the clas", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 8, "year": "2017"}}
{"text": "y (2012) covers the various methods from a more theoretical and probabilistic perspective (Murphy, 2012).(FP) rate as a function of the class membership probability (Fawcett, 2006). Intuitively, AUC measures the power of a model to distinguish among classes in the target attribute. An AUC score of 0.  indicates that the model has no ability to distinguish among classes of the target whereas a value approaching 1.  indicates that the model offers a large increase in TP rate for a small price of slightly increasing the FP rate.One class SVM\nOne class SVM|Nu: 1%, Kernel Coeffcient: 0.01\nNu: 1%, Kernel Coeffcient: 0.1\nNu: 10 %, Kernel Coeffcient: 0.01|0.881\n0.878", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 9, "year": "2017"}}
{"text": "ate.One class SVM\nOne class SVM|Nu: 1%, Kernel Coeffcient: 0.01\nNu: 1%, Kernel Coeffcient: 0.1\nNu: 10 %, Kernel Coeffcient: 0.01|0.881\n0.878\n0.879| |Local Outlier Factor|C: 1%, Neighbors: 10, Euclidean|0.879| |Local Outlier Factor|C: 1%, Neighbors: 20, Euclidean|0.882| |Local Outlier Factor|C: 1%, Neighbors: 50, Euclidean|0.880| |Isolation Forest|C: 0%|0.881| |Isolation Forest|C: 1%|0.880| |Isolation Forest|C: 5%|0.880| |Covariance Estimator|C: 1%|0.817|The performance evaluation procedure also monitored the classifi cation accuracy (CA), precision (Pc), recall (Rc), and F1 scores. Table 12 describes each metric and summarizes their advantages and disadvan tages.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 10, "year": "2017"}}
{"text": "ccuracy (CA), precision (Pc), recall (Rc), and F1 scores. Table 12 describes each metric and summarizes their advantages and disadvan tages. All performance metric except the AUC was sensitive to class imbalance in the dataset.CA is one of the most often cited performance metric for ML classi fiers. However, a high CA score can be misleading if the dataset has high class imbalanced. For example, a no-skill algorithm applied to a dataset with only 5% of the instances from one class and the rest from the other class will appear to have a 95 % accuracy if it picks the dominant class for every prediction. Stratified sampling of both the training and testing datasets helps to reduce the imbalance", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 11, "year": "2017"}}
{"text": "t picks the dominant class for every prediction. Stratified sampling of both the training and testing datasets helps to reduce the imbalance (Krawczyk, 2016).## 3.3.2. Hyperparameter tuningEach model requires that the user select values for key parameters (hyperparameters) that affect their performance. Tuning hyper parameters require incremental adjustments while observing a perfor mance metric. The optimization loop uses k-fold cross validation to maximize the model generalization on the entire dataset while reducing any tendency towards overfitting or underfitting . Models that have regu larization parameters provide a means to balance the unavoidable tradeoff between bias and variance ,", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 12, "year": "2017"}}
{"text": " underfitting . Models that have regu larization parameters provide a means to balance the unavoidable tradeoff between bias and variance , which improves generalization on unseen data. James et al. (2013) provides an excellent description of the above ML terminologies and concepts, so the book is incorporated here by reference James et al. (2013). The performance evaluation metric used was the area under the curve (AUC) of the receiver operating characteristic (ROC). The AUC trends with hyperparameter value ad justments show where each model achieved its best regularized performance.## 3.4. Feature rankingAttributes that contain noisy, irrelevant, or redundant information can diminish the p", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 13, "year": "2017"}}
{"text": "s best regularized performance.## 3.4. Feature rankingAttributes that contain noisy, irrelevant, or redundant information can diminish the performance of ML methods (Yu and Liu, 2003). Hence, data scientists developed various methods to score features based on the amount of information they contribute towards distinguishing the target classes. This section compares five methods that rank features based on the strength of their association with the classes in the target attribute. Table 13 provides a short description of each method and a reference that provides details about their theory of operations.All methods work best with normalized attributes because their magnitudes become comparable", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 14, "year": "2017"}}
{"text": "rovides details about their theory of operations.All methods work best with normalized attributes because their magnitudes become comparable. The diversity of methods result in(DT)|Recursive tree node splitting to maximize the purity of sub-trees.\nHP: Minimum number of instances in leaves (N), and minimum size\nof subsets (S).|A: Simple to interpret and to visualize. Works with non-numerical\ncategorical attributes. D: Tends to overft, resulting in low predictive\npower on new data.| ||Random Forest\n(RF)|Build many full trees for voting. Each tree grows from a\nbootstrapped dataset and a random subset of attributes. HP:", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 15, "year": "2017"}}
{"text": ".| ||Random Forest\n(RF)|Build many full trees for voting. Each tree grows from a\nbootstrapped dataset and a random subset of attributes. HP:\nNumber of trees (N) and minimum size of subsets (S).|A: Offers the simplicity and intuition of decision trees but with less\ntendency to overft, therefore, improves generalization on unseen\ndata. D: Incomplete trees diminish insights that full trees might\notherwise provide.| |**Tree-Based**\n**Methods**|Ada Boost (AB)\nExtreme Gradient\nBoost (XGB)\nGradient Boost\n(GB)|Sequentially build improved shallow trees for voting. HP: Number\nof estimators (N), learning rate (R), boosting algorithm, and\nregression loss function.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 16, "year": "2017"}}
{"text": " build improved shallow trees for voting. HP: Number\nof estimators (N), learning rate (R), boosting algorithm, and\nregression loss function.\nA highly confgurable version of gradient boosting. HP: Number of\nestimators (N), learning rate (R), maximum tree depth (S), loss\nfunction.\nSequentially build improved models that ft the errors of previous\nmodels. HP: Number of estimators (N), learning rate (R), maximum\ntree depth (S), loss function.|A: Selects only those features that improve predictive power, hence,\nreducing the computational burden for datasets with very large\ndimensionality. Less sensitive to overftting. D: Sensitive to the\npresence of outliers and data with high incoherence.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 17, "year": "2017"}}
{"text": "asets with very large\ndimensionality. Less sensitive to overftting. D: Sensitive to the\npresence of outliers and data with high incoherence.\nA: Improved performance over gradient boosting and more effcient.\nD: Sensitive to hyperparameter selection; requires manual\nintervention to achieve the best confguration for a given dataset.\nA: Effcient and good performance on large datasets; inherently\nsupports missing values. D: Sensitive to hyperparameter selection but\nhas fewer to tune than extreme gradient boosting.| |**Statistical**\n**Models**|k-Nearest\nNeighbors (k-NN)\nNaïve Bayes (NB)|Determine the class of an instance based on the majority class of its\nk nearest neighbors.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 18, "year": "2017"}}
{"text": "els**|k-Nearest\nNeighbors (k-NN)\nNaïve Bayes (NB)|Determine the class of an instance based on the majority class of its\nk nearest neighbors. HP: Number of neighbors (k), distance\nmethod.\nApplies Bayes theorem to determine the class probability, given\nprobabilities of the observations. HP: None|A: Method simplicity. D: Sensitive to a skewed class distribution. The\ncomputational intensity grows exponentially with the number of\ninstances and attributes.\nA: Fast and simple method. D: Poor performance when attributes are\nnot independent.| |**Decision**\n**Boundaries**\n**Learned**\n**Functions**|Logistic\nRegression (LR)\nSupport Vector\nMachine (SVM)\nStochastic\nGradient Descent\n(SGD)\nArtifcial Neural", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 19, "year": "2017"}}
{"text": "ndaries**\n**Learned**\n**Functions**|Logistic\nRegression (LR)\nSupport Vector\nMachine (SVM)\nStochastic\nGradient Descent\n(SGD)\nArtifcial Neural\nNetwork (ANN)|Establish a decision boundary by using a logistic function to\nmaximally separate classes. HP: Regularization function and\nstrength (C), and probability threshold.\nEstablish a decision boundary by fnding a multidimensional\nhyperplane to maximally separate classes. HP: Kernel type, cost (C),\nand regression loss (ε)\nAn optimization technique that fts a linear multivariate function to\nthe data. It works best when all features are scaled. HP: Loss\nfunction, learning rate method and parameters.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 20, "year": "2017"}}
{"text": "inear multivariate function to\nthe data. It works best when all features are scaled. HP: Loss\nfunction, learning rate method and parameters.\nA weighted multilayer linear network that represents a function.\nHP: Hidden layer neurons (N), solver type, regularization\nparameter (α), number of iterations (I).|A: Inherits many of the advantages of linear regression; precisions are\neasy to make. D: Sensitive to noise in the data such as outliers and\nincorrectly classifed instances. Model ftting may fail to converge if\nthere are many highly correlated features.\nA: High accuracy with low computational complexity. D: Sensitive to\nnoisy data and multidimensional planes that lack clear boundaries.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 21, "year": "2017"}}
{"text": "ures.\nA: High accuracy with low computational complexity. D: Sensitive to\nnoisy data and multidimensional planes that lack clear boundaries.\nA: An effcient technique on large datasets.\nD: Sensitive to feature scaling; many hyperparameters; and the true\nminima may not be achieved because the gradient is only an\napproximation.\nA: Accuracy improves with use and feedback about classifcation\naccuracy. D: Requires many training examples to improve\nclassifcation accuracy.|## 3.5. Principle component analysisThe method of principle component analysis (PCA) creates a set of new orthogonal basis vectors, each maximally spanning the dimensions of feature space, in the order of the data variance (Jollif", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 22, "year": "2017"}}
{"text": "es a set of new orthogonal basis vectors, each maximally spanning the dimensions of feature space, in the order of the data variance (Jolliffe and Cadima, 2016). Each principle component (PC) is a linear combination of all numerical features in the dataset. Intuitively, the first two principle components form a plane in feature space that is closest to all the data instances, as measured by the Euclidean distance. Data clusters tend to form along the directions of maximum variance. Hence, attributes that most influence the formation of data clusters contribute to inherent structure in the data. The terminology used in the literature is that each PC “explains” some proportion of the total var", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 23, "year": "2017"}}
{"text": "ribute to inherent structure in the data. The terminology used in the literature is that each PC “explains” some proportion of the total variance (information) in the dataset. Therefore, features that are weak components of most PCs tend to be associated with noise in the data. Analyst also use PCA to transform high dimensional data into a lower dimension feature space to enable the visualization of both structure and noise in the dataset (Anowar et al., 2021).Classifer|\nPerformance Metric.||| |---|---|---|---| |Metric|Description|Advantages|Disadvantages| |CA|The proportion of|Simply calculation.|Sensitive to data| ||predictions that were\ncorrect.||imbalance where a no-", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 24, "year": "2017"}}
{"text": "vantages|Disadvantages| |CA|The proportion of|Simply calculation.|Sensitive to data| ||predictions that were\ncorrect.||imbalance where a no-\nskill classifer can| ||||appear to provide| ||||better performance by| ||||predicting the| ||||dominant class every| ||||time. For example, a\nno-skill classifer will| ||||score CA at 90 % if the| ||||database labels 90 % of| ||||the accidents as| ||||derailments.| |Pc|The proportion of|Measures the|A bias towards the| ||observations correctly|probability of|majority class can be| ||predicted as positives|mislabeling a|misleading.| ||(TP) to the total|negative sample as|| ||number of|positive.|| ||observations||| ||predicted as positives||| ||(TP+FP).|||", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 25, "year": "2017"}}
{"text": " a|misleading.| ||(TP) to the total|negative sample as|| ||number of|positive.|| ||observations||| ||predicted as positives||| ||(TP+FP).||| |Rc|Measures the|Measures the|A bias towards the| ||proportion of positive|probability of|majority class can be| ||predictions (TP) to the|correctly labeling all|misleading.| ||total number of|the positive|| ||positive observations|observations.|| ||(TP+FN)||| |F1|The harmonic mean of|Measures the|Less bias but as a| ||Pc and Rc, scaled from|balance between|function of Pc and Rc| ||0 to 1.|precision and recall.|will retain some bias.| |AUC|Area under the ROC|Removes biased|More complex| ||curve that plots TP|scores for|calculation than a| ||against FP a", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 26, "year": "2017"}}
{"text": " retain some bias.| |AUC|Area under the ROC|Removes biased|More complex| ||curve that plots TP|scores for|calculation than a| ||against FP as a|imbalanced|simple ratio. Requires| ||function of class|datasets.|the class membership| ||membership||probability for every| ||probability.||prediction, which may| ||||not be inherently| ||||available from a model.|## **4. Results**The subsections of this section present the results of applying ma chine learning, attribute ranking, and PCA to the cleaned and trans formed dataset presented in the previous section.## 4.1. Machine learningTable 14 summarizes the stabilized performance of each ML algo rithm, sorted by the AUC metric.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 27, "year": "2017"}}
{"text": "the previous section.## 4.1. Machine learningTable 14 summarizes the stabilized performance of each ML algo rithm, sorted by the AUC metric. The null model is a no-skill model that predicts the dominant class each time. It provided a baseline to compare the performance score of skilled classifiers. As expected, the CA score for the no-skill classifier reflected the class imbalance of 67.  % for derailment type accidents versus non-derailment type accidents. How ever, the AUC performance of the null classifier was lowest as expected.Tracking the AUC trend with 10-fold cross validation and stratified sampling produced the optimum hyperparameter values shown in the table.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 28, "year": "2017"}}
{"text": ".Tracking the AUC trend with 10-fold cross validation and stratified sampling produced the optimum hyperparameter values shown in the table. Hyperparameters with common names across some models were the learning rate (L), loss function (LF), regularization (R) parameters,and optimizer algorithm (OA). To demonstrate the effect of hyperparameter tuning, Fig.  plots the AUC score for a range of hyperparameter N associated with RF, kNN, and DT.As noted in Table 14, the hyperparameter N represents the number of trees of a RF, the minimum number of samples to retain in the leaves of a DT, and the number of nearest neighbors for the kNN algorithm.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 29, "year": "2017"}}
{"text": " of trees of a RF, the minimum number of samples to retain in the leaves of a DT, and the number of nearest neighbors for the kNN algorithm. The asymptotic trend was similar for all hyperparameters tuned.## 4.2. Feature rankingTable 15 shows the importance ranking of the first 30 features in their strength of association with the target class. The rank by each of the five scoring methods are correlated as indicated by their pairwise cor relation coefficients listed in Table 16. The correlation ranges from 84.  % for the gini and chi-squared methods to 94.  % for the ANOVA and chi-squared methods.The distributions show that these attributes have some power to separate derailment from non-dera", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 30, "year": "2017"}}
{"text": ".  % for the ANOVA and chi-squared methods.The distributions show that these attributes have some power to separate derailment from non-derailment type accidents, but with un certainty based on the amount of overlap in their class distributions. For example, the class probability was higher for derailment type accidents on class 0, 1, 2, 7, 8, and 9 tracks (Fig. 5a). The distinction is significant for class 1 tracks because it has the highest frequency of occurrence (Fig. 5b). Similarly, the class probability was higher for derailment type accidents where movement authority was within restricted limits (restricted) or where movement was not on main tracks (Fig. 5c).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 31, "year": "2017"}}
{"text": "ilment type accidents where movement authority was within restricted limits (restricted) or where movement was not on main tracks (Fig. 5c). Similarly, the class probability was higher for derailment type accidents in non-signalized territories (Fig. 5d). The probability difference was much lower for the lower ranking attributes, but taken together, they improve the ML classification performance.All accidents tended to occur below the speed limit for the track class on which they operated. However, derailment type accidents tended to occur closer to the speed limit than non-derailment type accidents. A student’s t -test shows that the p-value was near zero, which indicated that the mean diff", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 32, "year": "2017"}}
{"text": "speed limit than non-derailment type accidents. A student’s t -test shows that the p-value was near zero, which indicated that the mean difference of 10 mph (16 kph) was statistically significant. The highlighted boxes in the figure indicates the values of the first quartile (25 %) through the third quartile (75 %) of the dataset. The solid vertical and horizontal lines indicate the mean and standard de viation, respectively. The lighter solid vertical lines indicate the median values.## 4.3. Principle component analysis## **5. Discussion****----- Start of picture text -----**\n Fig. 5. Class probability for the top two and fourth ranking attributes.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 33, "year": "2017"}}
{"text": " analysis## **5. Discussion****----- Start of picture text -----**\n Fig. 5. Class probability for the top two and fourth ranking attributes.\n**----- End of picture text -----****----- Start of picture text -----**\n Fig. 6. Distribution and statistics for excess speed.\n**----- End of picture text -----****----- Start of picture text -----**\n Fig. 7. The proportion of variance in the data that each PC explains.\n**----- End of picture text -----****Fig. 8.** Data clusters for attributes with high power to distinguish among the target classes.effectiveness of the custom data cleaning procedures, including the LAP technique introduced for imputing missing values.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 34, "year": "2017"}}
{"text": "the target classes.effectiveness of the custom data cleaning procedures, including the LAP technique introduced for imputing missing values. The LAP method was most effective in filling missing values for track density, but that attri bute ranked low in importance for classification. Although effective, one limitation of the LAP technique is that it provided a course imputation of the geospatial coordinates, based on an aggregation of entries from other records where a value was present for the track type near that station. However, the LAP method provided a more intelligent and effective scheme to impute missing values such as track density based on those of spatial neighbors rather than gu", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 35, "year": "2017"}}
{"text": "ed a more intelligent and effective scheme to impute missing values such as track density based on those of spatial neighbors rather than guessing values based on the mean, most frequent, or nearest neighbors in feature space. The geospatial join method provided the next best alternative to repair erroneous or lowresolution geospatial data. The distinctive southeast skew pattern revealed those records with low-resolution data entry.relative power to separate the distributions of the categories in the target class. That is, an exceptionally high overlap of the two class distributions ranked the attribute exceptionally low in importance towards classifier performance.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 36, "year": "2017"}}
{"text": "ceptionally high overlap of the two class distributions ranked the attribute exceptionally low in importance towards classifier performance. It is rare that any one attribute can completely distinguish among class members with 100 % accuracy, otherwise there would be no need to use additional attributes as explanatory factors for classifi cation. Rather, a combination of attributes contributes their ability to help determine the probability of class membership. Poor classification results with all types of classification models may indicate that all at tributes have a high degree of overlap in their class probability distributions.The PCA result (Fig.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 37, "year": "2017"}}
{"text": "ication models may indicate that all at tributes have a high degree of overlap in their class probability distributions.The PCA result (Fig. 7) shows that the first 6 PCs explain more than half the variance in the dataset but that it takes the remaining PCs, which accounted for 88 % of the PCs, to explain the remaining half of the variance in the data set. This outcome indicates that the first six PCs represented the bulk of the information in the dataset. By extension, the remaining PCs likely account for noise in the dataset based on the slow accumulation of the variance they explained. This result suggests that just under half of the variance in the dataset lack structure and, there fore,", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 38, "year": "2017"}}
{"text": "ion of the variance they explained. This result suggests that just under half of the variance in the dataset lack structure and, there fore, constitutes the noise in the dataset.The top four algorithms of XGB, GB, RF, and DT were all based on the theories of decision trees. Using the traditional academic grading system for performance, the top four models provided “good” overall perfor mance based on an AUC score greater than 85 %. The highest AUC score of nearly 89 % for XGB was associated with a classification accuracy and balanced precision-recall scores (F1) of nearly 83 % and 88 %, respec tively. All methods were sensitive to hyperparameter tuning as demonstrated in the performance impr", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 39, "year": "2017"}}
{"text": "res (F1) of nearly 83 % and 88 %, respec tively. All methods were sensitive to hyperparameter tuning as demonstrated in the performance improvement trends of Fig. 4. The hyperparameter tuning sensitivity cautions against using the default values suggested for each method.All feature ranking methods and PCA pointed to track class (TRK_CL), signalized movement authority (MOVEx = Signal), speed excess, and signalized territory (SIG) as the most important features in ML classifier performance. The interpretation of an attribute rank is its**Fig. 9.** Data clusters for attributes with low power to distinguish among the target classes.main track types (Fig. 8c).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 40, "year": "2017"}}
{"text": "ibute rank is its**Fig. 9.** Data clusters for attributes with low power to distinguish among the target classes.main track types (Fig. 8c). This result suggests that features that align with the cluster where the derailment class is biased associates more with derailment than non-derailment type accidents.attribute similarity. Consequently, noisy neighborhoods can hamper classification performance as evidenced by the low performance rank of kNN. Methods such as SVM and LR seek clear decision boundaries in multidimensional feature space. Hence, the lack of clear hyperplanes between the target classes hampered their performance.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 41, "year": "2017"}}
{"text": "on boundaries in multidimensional feature space. Hence, the lack of clear hyperplanes between the target classes hampered their performance. In fact, SVM achieved the lowest performance.Overall, the analysis suggested that derailments were more strongly associated with lower track classes, non-signalized territories, and movement authorizations with restricted limits. Derailments also tended to occur at 10 mph (16 kph) below the speed limit of the track class whereas non-derailment type accidents tended to occur at 20 mph (32 kph) below the limit. Those findings correspond with the intuition that lower-class tracks, which has lower speed limits, and movements with restricted limits are so de", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 42, "year": "2017"}}
{"text": "findings correspond with the intuition that lower-class tracks, which has lower speed limits, and movements with restricted limits are so designated because those operations are associ ated with higher safety risks, which the ML confirmed. Similarly, there is less guidance for movements in territories without signalization, so the risk of derailments due to track irregularities or switch problems is higher. However, it may not be wise to go beyond probabilities and statistical associations by assuming general latent reasons for the ML outcome because there are no exclusive distinctions between accident causes for each accident type.The above insights about the location of structure and noise", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 43, "year": "2017"}}
{"text": "re are no exclusive distinctions between accident causes for each accident type.The above insights about the location of structure and noise in the dataset provided clues to understand the reason for the performance differences of each ML method. Randomized tree-based methods tend to train on various cross-sections of a dataset and use voting to determine the class likelihood. In contrast, the other methods tend to leverage structure in the dataset. Hence, the randomized tree-based methods such as XGB, GB, and RF performed better by discovering patterns across noisy neighborhoods in the dataset. On the other hand, kNN seeks local neighborhoods in feature space to predict class membership bas", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 44, "year": "2017"}}
{"text": "across noisy neighborhoods in the dataset. On the other hand, kNN seeks local neighborhoods in feature space to predict class membership based onOne limitation of the railroad accident database is that it does not necessarily list accidents where the financial loss was below $10,500 because the FRA does not require railroads to report those. A second limitation is that the financial loss includes only the costs of repairing equipment, signal systems, and infrastructure structures. Losses do not include costs associated with cleanup, lost freight, societal damages, fatalities, injuries, and line closures. Nevertheless, financial loss was nota pre-incident explanatory variable, but any future", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 45, "year": "2017"}}
{"text": "l damages, fatalities, injuries, and line closures. Nevertheless, financial loss was nota pre-incident explanatory variable, but any future analysis that uses it should be aware of this limitation in the dataset.## **6. Conclusions**## **References**Recent advancements in computing capacity and their cost reduction has enabled machine learning (ML) methods to uncover patterns in large multidimensional datasets that are difficult to analyze with common rule-based and statistical methods. However, there are many types of ML techniques, and no single method works best for all types of datasets. Therefore, this work applied 11 different types of ML models to a large multidimensional dataset of r", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 46, "year": "2017"}}
{"text": " works best for all types of datasets. Therefore, this work applied 11 different types of ML models to a large multidimensional dataset of railroad accidents to compare their per formance in predicting derailments from other accident types. The extreme gradient boosting (XGB) classifier provided the best predictive performance with an AUC score of 89 %. The model could distinguish accident type with an accuracy of 83 %. Principle component analysis (PCA) revealed that high feature contamination noise and isolation noise would prevent significant further gains in classification accuracy by any algorithm.The good ML performance affirmed the relevance and sufficiency of the attributes in their", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 47, "year": "2017"}}
{"text": "gains in classification accuracy by any algorithm.The good ML performance affirmed the relevance and sufficiency of the attributes in their contribution towards distinguishing derailments from other accident types. Hence, knowing the relative importance of those attributes towards classification accuracy can lead to insights for decision-making in railroad risk management. The importance ranking used five different methods that agreed on the ranking with correlations – ranging from 84.2% 94.5%. The ANOVA and chi-squared methods agreed with the highest correlation that the top four attributes were track class, the type of movement authority, the excess speed, and the presence of signalization", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 48, "year": "2017"}}
{"text": "rrelation that the top four attributes were track class, the type of movement authority, the excess speed, and the presence of signalization in the territory. The feature distribution for each target class and the PCA agreed that relative to non-derailment type accidents, derailments were more strongly associated with lower track classes, non-signalized territories, and movement authorizations with restricted limits. Derailments also tended to occur at 10 mph (16 kph) below the speed limit of the track class whereas non-derailment type accidents tended to occur at 20 mph (32 kph) below the limit.The good ML performance also suggests that the custom data imputation techniques presented were e", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 49, "year": "2017"}}
{"text": "o occur at 20 mph (32 kph) below the limit.The good ML performance also suggests that the custom data imputation techniques presented were effective in filling missing values. The data-cleaning framework also demonstrated a spatial join technique that addressed 21.  % of the geospatial data entry errors. The detailed chronicle of the cleaning procedures will help other researchers save a substantial amount of time in data preparation when using the same dataset. Future work will leverage the framework to examine trends in accidents caused by human error to determine the effectiveness of PTC deployments relative to historic accident rates.## **Data availability**This paper cited the sources o", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "0 or PASSTRN is“Y”", "level": 1, "chunk_id": 50, "year": "2017"}}
