{"text": "Introduction**U.S. railroads have been an important driver of economic progress for more than 150 years. Today, U.S. railroads carry approximately onethird of the nation’s exports (ASCE, 2017). Therefore, the safe and ’s economic health. efficient operation of railroads is crucial to the nation Unfortunately, railroads lose hundreds of millions of dollars from acci dents each year. Analysis of the Federal Railroad Administration (FRA) Rail Equipment Accident database revealed that human-factors was consistently the dominant cause of railroad accidents (Bridgelall and Tolliver, 2020). Hence, the federal government mandated that railroads deploy a positive train control (PTC) system by 2018 to", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 0, "year": "2017"}}
{"text": "ridgelall and Tolliver, 2020). Hence, the federal government mandated that railroads deploy a positive train control (PTC) system by 2018 to help prevent accidents caused by human errors (Zhang et al., 2018). With PTC now in place, it is important for analysts to study other common causes of accidents.The **goal** of this research is to identify factors associated with the most frequent and expensive types of accidents that are not attributable to human error. Data mining of FRA accident records from January 1, 2009, to June 30, 2020, revealed that derailment accidents accounted for 70.  % of the average annual financial loss (Fig. 1).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 1, "year": "2017"}}
{"text": "om January 1, 2009, to June 30, 2020, revealed that derailment accidents accounted for 70.  % of the average annual financial loss (Fig. 1). The trendshowed that derailment accidents maintained a steady rate each year. Therefore, the ability to identify and rank features that increase the risk of derailments over other accident types can inform more cost-effective and impactful risk management strategies (Ghofrani et al., 2018).An **objective** of this research is to build a supervised machine learning (ML) model that can predict derailments from other accident types and to rank the importance of those features that contribute to wards the classification accuracy.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 2, "year": "2017"}}
{"text": "ict derailments from other accident types and to rank the importance of those features that contribute to wards the classification accuracy. However, no single type of ML model performs best on all types of datasets (Murphy, 2012). Therefore, another objective is to compare the classification performance of various types of ML models on the same dataset.One of the main challenges in data science is to effectively clean datasets before using them to train ML models. Studies estimate that dirty data costs the U.S. economy trillion of dollars each year (Ilyas and Chu, 2019). A survey of data cleaning for ML found that the failure to discover and repair dirty data can weaken data analysis techni", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 3, "year": "2017"}}
{"text": "as and Chu, 2019). A survey of data cleaning for ML found that the failure to discover and repair dirty data can weaken data analysis techniques (Jesmeen et al., 2018). Although a few approaches to data cleaning are common, every dataset poses unique challenges (Bridgelall et al., 2018). Hence, data scientists spend an average of 60 % of their time cleaning and organizing data (Ilyas and Chu, 2019).Although the importance of using clean data is well-known, the research community has paid little attention to the advancement of data cleaning techniques (Rahm and Do, 2000). The most commonly used techniques are those that detect and remove outliers and duplicate re cords (Ilyas and Chu, 2019).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 4, "year": "2017"}}
{"text": "ahm and Do, 2000). The most commonly used techniques are those that detect and remove outliers and duplicate re cords (Ilyas and Chu, 2019). Even so, those techniques alone cannot effectively clean all types of datasets. Other techniques that can find data entry errors use customized rules to detect violations, for example, house prices exceeding an expected range for a given neighborhood. Custom techniques tend to be heuristic, so they require good familiarity with the data and its meaning. Considering the challenges outlined above, the following are **contributions** of this research:modeling to HRGC crash data and found that gate violations were more highly associated with two-quadrant th", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 5, "year": "2017"}}
{"text": "contributions** of this research:modeling to HRGC crash data and found that gate violations were more highly associated with two-quadrant than four-quadrant gates (Liu and Khattak, 2017). Karamati et al. (2020) applied random survival forest to HRGC crash data and found that adding audible alarm devices to crossings that already have gates and flashing lights can decrease crash likelihood by approximately 50 % (Keramati et al., 2020). Soleimani et al. (2019) used extreme gradient boosting to identify HRGCs that should be closed to prevent accidents (Soleimani et al., 2019). Wali et al. (2021) applied text mining to crash narrative data of railroad trespass ing incidents and found that confir", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 6, "year": "2017"}}
{"text": "imani et al., 2019). Wali et al. (2021) applied text mining to crash narrative data of railroad trespass ing incidents and found that confirmed suicide attempts and the use of headphones or cellphones were more likely to result in fatal injuries (Wali et al., 2021).- A customized framework to clean a relevant subset of the FRA database and to fill 100 % of missing values for the important at tributes .- Interpreting the importance ranking of the feature relevance in predicting accident type .- Visualizing and interpreting the classification power of each attri bute by principle component analysis (PCA) to gain insights about the performance differences among the ML models evaluated (Sec tion", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 7, "year": "2017"}}
{"text": " attri bute by principle component analysis (PCA) to gain insights about the performance differences among the ML models evaluated (Sec tions 3.  and 4.3).The next section  reviews related works and their findings in relation to the contributions of this research. Section 4 mirrors sub sections of the methods section to present the results. Section 5 discusses 6 the significance and interprets the outcome. Section recaps the find ings and concludes with how future research can leverage the methods of this research to further the agenda in accident analysis.The survey of Ghofrani (2018) demonstrated that researchers have also used ML methods to analyze other aspects of railroad operations bes", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 8, "year": "2017"}}
{"text": "is.The survey of Ghofrani (2018) demonstrated that researchers have also used ML methods to analyze other aspects of railroad operations besides safety (Ghofrani et al., 2018). For example, Li et al. (2014) used ML to learn rules from historical and real-time data to predict railroad maintenance needs (Li et al., 2014). Lasisi and Attoh-Okine (2019) proposed a combination of ensemble tree-based ML models to predict rail fatigue defects and achieved an AUC score of 0.  (Lasisi and Attoh-Okine, 2019).## **2. Related works**The benchmarking of ML performance is subjective because of its high relevance to the target problem of a particular study in a particular field (Olson et al., 2017).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 9, "year": "2017"}}
{"text": "formance is subjective because of its high relevance to the target problem of a particular study in a particular field (Olson et al., 2017). For example, a performance score considered to be “good” in the biotech industry when evaluating vaccine efficacy may be considered “poor” in the automotive industry when evaluating defective unit batches. Subjective performance assessments depend on the level of “acceptable” risk for a given application (Cook, 2007). Therefore, model evaluation often use the fuzzy academic grading sys tem to assess and compare performance levels (Echauz and Vachtseva nos, 1995).## **3.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 10, "year": "2017"}}
{"text": "el evaluation often use the fuzzy academic grading sys tem to assess and compare performance levels (Echauz and Vachtseva nos, 1995).## **3. Methodology**Several studies used ML techniques to analyze highway-rail grade crossing (HRGC) accidents. Dabbour et al. (2017) applied ordered regression models to HRGC crash data and found that higher train and vehicle speeds were positively correlated with driver injury severity (Dabbour et al., 2017). Liu and Khattak (2017) applied geospatialThe input layer gathers the datasets and prepares the combined data by applying various methods to reduce noise, repair data entry errors, and fill in missing values.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 11, "year": "2017"}}
{"text": "e datasets and prepares the combined data by applying various methods to reduce noise, repair data entry errors, and fill in missing values. The processing layer prepares relevant attri butes to train and tune the ML models. The processing layer led to the**Fig. 1.** Annual financial loss reported for different accident types.discovery of additional errors that made some features irrelevant. In such cases, the framework logic looped back to the data preparation module to address the issue. The model building and validation pro cesses also contained a loop that converged after the ML performance stabilized. The final layer ranked the importance of attributes in clas sification performance and", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 12, "year": "2017"}}
{"text": "p that converged after the ML performance stabilized. The final layer ranked the importance of attributes in clas sification performance and used PCA to visualize the results for interpretation.Duplication: Correlated\nAttribute contains the same information as: Attribute is more than 90 % correlated with\n(Ilyas and Chu,: (G´eron, 2017)Duplication: Redundancy\nAttribute contains the same information as: Attribute contains information that is inherent in other attributes.\n(Ilyas and Chu,: (Liu et al., 2012)Duplication: Dispersion Combinable\nAttribute contains the same information as: Attribute has low variance or carries little or no information.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 13, "year": "2017"}}
{"text": "plication: Dispersion Combinable\nAttribute contains the same information as: Attribute has low variance or carries little or no information. Attribute that can combine with others without losing information.\n(Ilyas and Chu,: (Bridgelall et al., 2018) (Ilyas and Chu, 2019)## 3.1. Data sourceThe comprehensive Federal Railroad Administration (FRA) Rail Equipment Accident database provided the main dataset for the analysis (Bridgelall and Tolliver, 2020). Some of the data schema became inconsistent after the FRA changed reporting requirements for a few of the fields starting June 1, 2011 (FRA, 2011). For example, the report added a field to indicate if the accident occurred in a signalized terri", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 14, "year": "2017"}}
{"text": "e fields starting June 1, 2011 (FRA, 2011). For example, the report added a field to indicate if the accident occurred in a signalized territory. Hence, there was no entry for the “SIGNAL” field prior to the switchover date. Similarly, a field indicating the method of operation (“MOPERA”) replaced the “METHOD” field that encoded similar information.Merging 8055 records from 2009 to 2011 with 21,242 records from 2012 to June 2020 produced a total of 29,297 records with 145 attri butes. Consequently, 22 % and 79 % of the data was missing in the “MOPERA” and “METHOD” fields, respectively. The accident reporting form also added a field “SSB1” to indicate if the track was a continuously welded (C", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 15, "year": "2017"}}
{"text": "d “METHOD” fields, respectively. The accident reporting form also added a field “SSB1” to indicate if the track was a continuously welded (CWR) or other. Hence, the “SSB1” field was mostly empty prior to June 1, 2011.Duplication: Sparsity\n8 with duplicated information (e.g. IMO, IYR,: MONTH, YEAR). 16 with>90 % zero-flled (e.g. CABOOSE1,\n126 - 8=118: 118−16=102\nDuplication: Correlated\n8 with duplicated information (e.g. IMO, IYR,: 12 with>90 % correlation with other attributes\n126 - 8=118: 102−12=90\nDuplication: Redundancy\n8 with duplicated information (e.g. IMO, IYR,: (e.g. PASSINJ, PASSKLD) 7 that were redundant with others (e.g.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 16, "year": "2017"}}
{"text": "12=90\nDuplication: Redundancy\n8 with duplicated information (e.g. IMO, IYR,: (e.g. PASSINJ, PASSKLD) 7 that were redundant with others (e.g. CNTYCD,\n126 - 8=118: 90−7=83\nDuplication: Noise Combinable\n8 with duplicated information (e.g. IMO, IYR,: number, car number) 6 with>20 % missing or no relevance to the target (e.g. ADJUNCT1, DIV) HUMANS=\n126 - 8=118: 77−6=71 71–Duplication: Correlated\n8 with duplicated information (e.g. IMO, IYR,: (engineers+fremen+conductor+brakemen), drop 4. EQATT (equipment attended) correlates with\n126 - 8=118: 4+1=68 68–1=67\nDuplication: Combinable\n8 with duplicated information (e.g.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 17, "year": "2017"}}
{"text": ", drop 4. EQATT (equipment attended) correlates with\n126 - 8=118: 4+1=68 68–1=67\nDuplication: Combinable\n8 with duplicated information (e.g. IMO, IYR,: HUMANS, drop 1\nCombine 15 narrative felds into a single feld (NARR), drop original 15.\n126 - 8=118: 67– 15+1=53## 3.2. Data processing## 3.2.1. Data cleaningTable 1 describes criteria used to clean the data by eliminating irrelevant attributes or features. The classification of those criteria is a heuristic synthesis by the authors based on a broad understanding from the literature, which the table cites for further explanation.Table 2 chronicles each criterion used to reduce the number of fields from 145 to 52.## 3.2.2.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 18, "year": "2017"}}
{"text": "which the table cites for further explanation.Table 2 chronicles each criterion used to reduce the number of fields from 145 to 52.## 3.2.2. Data extractionPassenger trains accounted for a small portion 8.  % (2354) of accidents. Removing those records enhanced the consistency of the dataset, which is known to improve the ML performance (Murphy,2012 ). A side benefit was that the eliminated records also removed a few attributes that were associated with passenger trains only. Table 3 chronicles the net reduction of attributes from 52  to 50, and records from 29,297 to 15,087. The statistics shown in the table are the number of records (N), number of attributes or variables (V), and “DERAILED", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 19, "year": "2017"}}
{"text": "from 29,297 to 15,087. The statistics shown in the table are the number of records (N), number of attributes or variables (V), and “DERAILED” number of metadata fields (M). Adding the target attribute indicated if the accident was a derailment type or not, and it became the label for supervised ML.TONSLG POSCAR: SPDOVR\n48−1+1=48 48+1−1=48: 46+1–1=46\nLog Transform: TONS, then drop old. Rename and recode POSITON1 (position of frst: Add to capture difference in train speed and speed limit for CLASSTRK. Dropped feld HIGHSPD.## 3.2.3. Attribute transformationML algorithms tend to perform poorly on data with attributes that have a highly skewed distribution because the model could treat data in lo", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 20, "year": "2017"}}
{"text": "ML algorithms tend to perform poorly on data with attributes that have a highly skewed distribution because the model could treat data in long tails as outliers or because extreme values provide insufficient ex amples (Manning and Mullahy, 2001). A shifted natural logarithm LN(1 + x ) reduced the skew and prevented an undefined number if attribute value x is zero.Another transformation that can help to reduce the dimension of a dataset is to replace a set of related attributes with proportions of a base attribute. The proportion transformation retained information about the relative relationship among attributes while normalizing the values within the  range.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 21, "year": "2017"}}
{"text": "portion transformation retained information about the relative relationship among attributes while normalizing the values within the  range. Table 4 chronicles the transformation of attri butes and their effect on reducing the number of attributes from 50  to 40.## 3.2.4. Feature selectionPOSCAR LOADED1: POSITON2\n40−1=39 39−1=38: 33−1=32\nRelative position of the frst involved car in the train. Boolean: Is frst involved car loaded? Missing (22 %,: Position of car on the train that caused the accident.Predictive modeling should not contain attributes where values are known only after the outcome. Therefore, the cleaning procedure eliminated post-event attributes such as the number of people in", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 22, "year": "2017"}}
{"text": " values are known only after the outcome. Therefore, the cleaning procedure eliminated post-event attributes such as the number of people injured, killed, or evacuated. Table 5 chronicles the feature reduction from 40  to 25.- 1) Packaged similar features of an attribute to simplify the categories. 2) Converted categorical attributes that have some ranking to ordinal attributes.- 3) Binarized categorical attributes that contained only two values by replacing one value with zero and the other with one.- 4) Replaced nominal values with a single word label to enhance the ease of interpreting trends with more descriptive legends.work with missing data, but most cannot (Abidin et al., 2018).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 23, "year": "2017"}}
{"text": "abel to enhance the ease of interpreting trends with more descriptive legends.work with missing data, but most cannot (Abidin et al., 2018). There fore, data scientists developed a few methods to impute or guess missing values. Common approaches are to replace missing values with the mean, median, most frequent, random, or zero value. More intelligent approaches use tree-based ML techniques to fill missing values with those of their nearest neighbors in feature space . However, the existing methods did not enhance the contribution of the affected attributes to wards predicting the target class. Therefore, this research developed a new method, dubbed local association pivot (LAP), to replace", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 24, "year": "2017"}}
{"text": "tes to wards predicting the target class. Therefore, this research developed a new method, dubbed local association pivot (LAP), to replace missing values based on spatial proximity rather than feature space proximity. The LAP method first creates a pivot table that aggregates non-missing values by a spatial location identifier and by sub-location identifiers if available. The method then merges the pivot table with the dataset by using the main location identifier as the unique merge key. The aggre gation method for the pivot depends on the type of missing data. For example, for numerical values such as track density, the method used the maximum of the aggregated value for a location.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 25, "year": "2017"}}
{"text": "f missing data. For example, for numerical values such as track density, the method used the maximum of the aggregated value for a location. The method did not use the average value because zero or missing values created an undesirable bias in the aggregation. A fringe benefit of using the LAP method is that it is easy to spot data entry or spelling errors by examining a sorted list of the unique location keys.## 3.2.6. Data imputationACCCAT: PASSTRN\nN: 29,297 V: 52+1=53: %) V: 53−4=49\nAdd accident category based on accident cause code: {Track, Equipment, Human, Signal,: (PASSTRN=Y). Dropped associated attributes:ACCCAT: N: 25,035\nN: 29,297 V: 52+1=53: Dropped records where the accident caus", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 26, "year": "2017"}}
{"text": ", Human, Signal,: (PASSTRN=Y). Dropped associated attributes:ACCCAT: N: 25,035\nN: 29,297 V: 52+1=53: Dropped records where the accident cause was missing, 7% (1908)ACCCAT: N: 15,088\nN: 29,297 V: 52+1=53: Dropped records where human factors were a cause, 39.  % (9947)Table 7 summarizes the results of the imputing missing values and the impact of each method. Missing or erroneous geospatial coordinates are impossible to impute or correct if no other spatial information is available in the dataset. The state or county name provides a coarse**Table 6** Summary of Feature Engineering.: location identifer that can be helpful for visualizing data on maps.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 27, "year": "2017"}}
{"text": "unty name provides a coarse**Table 6** Summary of Feature Engineering.: location identifer that can be helpful for visualizing data on maps. However, a coarse location such as a large state may introduce bias in the ML process. Fortunately, the FRA database contains the station name that is closest to the accident location, so its location can be a surrogate for missing geospatial coordinates. 3.2.7. Geospatial coordinate repair Aside from missing geospatial coordinates, data entry errors resulted in erroneous or highly skewed geospatial locations.Fig. 3shows the positions of the recorded geospatial coordinates relative to a map of the continental United States.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 28, "year": "2017"}}
{"text": "ed geospatial locations.Fig. 3shows the positions of the recorded geospatial coordinates relative to a map of the continental United States. There is an observable systematic skew to wards the southeast This skew suggested that there was a lack of res Attribute Procedure CWR Renamed SSB1 to CWR (continuously welded rail); binarized as“1′′ = “CWR”and“0”otherwise. LOADED1\nBinarized as“1”=“Y”(frst involved car loaded?) and“0”=“N”for non-empty values. WEATHER Recoded nominal values in WEATHER as labels {Clear, Cloudy, Rain, Fog, Sleet, Snow} TRKTYP Renamed TYPTRK (track type) and labeled nominal codes as {Main, Yard, Siding, Industry} VISION Renamed VISIBLTY and replaced nominal codes as descrip", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 29, "year": "2017"}}
{"text": "YPTRK (track type) and labeled nominal codes as {Main, Yard, Siding, Industry} VISION Renamed VISIBLTY and replaced nominal codes as descriptive {Dawn, Day, Dusk, Dark} CLASSRR Renamed TYPRR (railroad class) and cleaned to contain only values from 1 to 6. CLASSTRK Renamed TRKCLAS (track class) and cleaned to contain ordinal values from 0 to 9 (X→0) CONSIST Renamed TYPEQ (consist type); repackaged as {freight, passenger, locomotive, cars, work, yard}. {1}→ “Freight”, {2, 3, B, C}→ “Passenger”, {8, D, E}→ “Locomotive”, {5, 6}→ “Cars”, {4, 9, A}→ “Work”, {7}→ “Yard” ACCTYPE Renamed TYPE (accident type); repackaged as category labels: {1}→ “Derail”, {2, 3, 6}→ “Collide”, {4}→ “Collide (Side)”, {", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 30, "year": "2017"}}
{"text": "}→ “Yard” ACCTYPE Renamed TYPE (accident type); repackaged as category labels: {1}→ “Derail”, {2, 3, 6}→ “Collide”, {4}→ “Collide (Side)”, {5}→ “Collide (Rake)”, {7, 8}→ “RGC”, {9}→ “Obstruct”, {10, 11}→ “Fire”, {12, 13}→ “Other” MOVEx Renamed MOPERA; repackaged as labels {signal, control, restrict, blocks, not main} {1, D}→ “Signal”, {2, A, B, C, P}→ “Control”, {3, L, M, I}→ “Restrict”, {4, E, F, G, H, J, K}→ “Blocks”, {5, N, O}→ “Not Main”\n**Table 7** Summary of Data Imputation.: Attribute Missing Before Missing After Procedure (N=29,297, V=49, M=3)**Table 6** Summary of Feature Engineering.: TRKDEN 51 % (15,176) 0% (0) Pivot STATION by TRKTYP, aggregated as maximum TRKDNSTY (track density", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 31, "year": "2017"}}
{"text": "able 6** Summary of Feature Engineering.: TRKDEN 51 % (15,176) 0% (0) Pivot STATION by TRKTYP, aggregated as maximum TRKDNSTY (track density). Fill missing data associated with the track type if defned, otherwise use the maximum value. SIG 22 % (6473) 0%, 0\nPivot STATION by TRKTYP, aggregated as net count SIGNAL (signalized territory). Fill missing data as“1”if net count associated with the track type is greater than 0, otherwise fll with“0” CONSIST 39 % (11,537) 8% (2605) Layer 1: Fill missing CONSIST with: “Freight”if (LOADF1+EMPTYF1)> 0 otherwise “Passenger”if (LOADP1+EMPTYP1)> 0 or PASSTRN is“Y” 8% (2605) 2% (844) Layer 2: Fill missing CONSIST with: “Freight”if CLASSRR is“1”(except “Amtr", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 32, "year": "2017"}}
{"text": "senger”if (LOADP1+EMPTYP1)> 0 or PASSTRN is“Y” 8% (2605) 2% (844) Layer 2: Fill missing CONSIST with: “Freight”if CLASSRR is“1”(except “Amtrak”) otherwise “Passenger”if RAILROAD (reporting railroad) is“Amtrak” 2% (844) 1% (377) Layer 3: Fill missing CONSIST with: “Work Train”if TRKTYP is not“Main” 1% (377) 0% (0) Layer 4: Fill missing CONSIST with: “Work Train”if TONS (gross tons, excluding locomotives) is 0 otherwise fll missing CONSIST with “Freight”if TONS>0 CWR 21 % (6378) 0% (0) Fill missing values with“1”if TRKTYP is “main”and“0”otherwise. MOVEx 0% (518) 0% (0) Fill missing MOVEx based on SIGNAL or TRKTYP. PASSTRN 6%, (2049) 0% (0) Fill missing PASSTRN based on CONSIST.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 33, "year": "2017"}}
{"text": "erwise. MOVEx 0% (518) 0% (0) Fill missing MOVEx based on SIGNAL or TRKTYP. PASSTRN 6%, (2049) 0% (0) Fill missing PASSTRN based on CONSIST. Check original fag for consistency with the type CONSIST and the sum of freight and passenger cars (loaded or empty). Flip the fag accordingly. CLASSRR 0%, (37) 0% (0) Fill missing CLASSRR (railroad class) by internet search: BLF→2, {DD, METC}→3, CN→1 TRKTYP 0%, (15) 0% (0) Fill missing TRKTYP (track type) by inference from the metadata. CLASSTRK 0%, (25) 0% (0) Fill missing CLASSTRK (track class) by inference from the metadata.Aside from missing geospatial coordinates, data entry errors resulted in erroneous or highly skewed geospatial locations. Fig.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 34, "year": "2017"}}
{"text": " the metadata.Aside from missing geospatial coordinates, data entry errors resulted in erroneous or highly skewed geospatial locations. Fig.  shows the positions of the recorded geospatial coordinates relative to a map of the continental United States. There is an observable systematic skew to wards the southeast. This skew suggested that there was a lack of res olution for those coordinates because in North America, lower resolution latitude and longitude coordinates would bias towards the south and east, respectively. The result was that 21.  % of the records had erroneous geospatial coordinates because their locations on the map did not match the counties reported for the accidents.## 3.2", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 35, "year": "2017"}}
{"text": "records had erroneous geospatial coordinates because their locations on the map did not match the counties reported for the accidents.## 3.2.8. Outlier removalSacrificing a few outlier data points to reduce bias can improve the generalization of a model. Outlier data instances are few and different from the bulk of the dataset (Liu et al., 2012). They could represent noisy data entries or rare events that can bias the training of an ML model, resulting in poor predictive performance. The framework used four methods to compare their effect on the model performance:- One class SVM (OCS) with a radial basis function (RBF) kernel (OCSRBF)Table 8 chronicles the progress of filling missing geospat", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 36, "year": "2017"}}
{"text": "rformance:- One class SVM (OCS) with a radial basis function (RBF) kernel (OCSRBF)Table 8 chronicles the progress of filling missing geospatial co ordinates in each step of the procedure. The LAP method used all records prior to data reduction and filled missing values with the mean value of the non-zero latitude and longitude values for that track type near the station, otherwise the method used the maximum value.Table 10 summarizes the AUC performance metric for a random forest classifier after removing outliers using each of the four methods, with the various hyperparameter selections shown. All algorithm and parameter selection produced similar performance.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 37, "year": "2017"}}
{"text": "h of the four methods, with the various hyperparameter selections shown. All algorithm and parameter selection produced similar performance. The framework used the LOF algorithm with 20 nearest neighbors and 1% outliers because of its slight AUC performance edge. The method removed 126 outliers to result in 15,087–126 = 14,961 records used to train and evaluate the ML models.Table 9 summarizes the final set of 25 attributes used to build the ML models. One-hot-encoding the categorical attributes increased the num ber of features from 25 to 51. Dispersion represents the relative amount of variability (information) that each attribute contributes to the overall variance in the data.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 38, "year": "2017"}}
{"text": " Dispersion represents the relative amount of variability (information) that each attribute contributes to the overall variance in the data. The dispersion measure is the entropy and coeffi cient of variation (CV) for categorical and numerical attributes,**Fig. 3.** Positions of the recorded geospatial coordinates in the FRA database.**Table 8** Chronicle of Geospatial Coordinate Cleaning.: DERAILED 0.631\nCategorical Target attribute: 1 if the accident type was derailment. REGION 0.400\nCategorical Cleaned FRA region code for accident location. LAT 0.133\nContinuous Cleaned latitude coordinate LON −0.126\nContinuous Cleaned longitude coordinate CLASSRR 0.796\nOrdinal Cleaned railroad class.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 39, "year": "2017"}}
{"text": ".133\nContinuous Cleaned latitude coordinate LON −0.126\nContinuous Cleaned longitude coordinate CLASSRR 0.796\nOrdinal Cleaned railroad class. MONTH 0.549\nOrdinal Incident month. DAY 0.561\nOrdinal Incident day. HR24 0.541\nContinuous Transformed time to fractional 24 -h. TEMP 0.391\nContinuous Temperature (degrees Fahrenheit) VISION 1.110\nCategorical Visibility: {Dawn, Day, Dusk, Dark} WEATHER 0.977\nCategorical Weather: {Clear, Cloudy, Rain, Fog, Sleet, Snow} TRKTYP 1.050\nCategorical Track Type: {Main, Yard, Siding, Industry} TRKCL 0.753\nOrdinal Track Class: {X as 0, 1 through 9} CWR 0.685\nBinary 1 if the rail type was continuously welded, 0 otherwise.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 40, "year": "2017"}}
{"text": ", Industry} TRKCL 0.753\nOrdinal Track Class: {X as 0, 1 through 9} CWR 0.685\nBinary 1 if the rail type was continuously welded, 0 otherwise. MOVEx 1.250\nCategorical Movement: {Blocks, Control, Signal, Not Main, Restrict} TRKDENLG 0.972\nContinuous log(1+x) of annual track density in millions of gross tons. SIG 0.590\nBinary 1 if used signals to control train movements, 0 otherwise. TRNSPDLG 0.589\nContinuous log(1+x) of train speed in miles per hour (mph). SPDOVR −1.304\nContinuous Difference between train speed and limit for track class. CONSIST 0.950\nCategorical Consist: {Freight, Locomotive, Cars, Work, Yard} TONSLG 0.757\nContinuous log(1+x) of gross tonnage, excluding power units.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 41, "year": "2017"}}
{"text": "0.950\nCategorical Consist: {Freight, Locomotive, Cars, Work, Yard} TONSLG 0.757\nContinuous log(1+x) of gross tonnage, excluding power units. LOCOS 0.704\nOrdinal Number of headend locomotives. NCARS 0.915\nOrdinal Total number of cars. CARSLD 0.704\nContinuous Proportion of the number of cars that were loaded (0–1) CARSHZMT 2.800\nContinuous Proportion of loaded cars carrying hazardous materials (0–1) HUMANS 0.562\nContinuous Number of humans present on the train.## 3.3.1. Supervised classification modelsTable 11 summarizes the 11 different types of ML models used in this analysis. The table provides a brief description of how each algorithm works, their most important hyperparameters (HP), their", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 42, "year": "2017"}}
{"text": " used in this analysis. The table provides a brief description of how each algorithm works, their most important hyperparameters (HP), their overall ad vantages (A) and disadvantages (D). The table groups the models into four broader categories based on their underlying theory of operation: tree-based methods, statistical models, decision boundaries, and learned functions. Numerous excellent books describe the mathematics and theory of operations for each model; they are incorporated here by reference. G´eron (2017) discusses both the theory and practical imple mentation of decision tree (DT), random forest (RF), AdaBoost (AB), logistic regression (LR), support vector machine (SVM), stochast", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 43, "year": "2017"}}
{"text": "l imple mentation of decision tree (DT), random forest (RF), AdaBoost (AB), logistic regression (LR), support vector machine (SVM), stochasticgradient descent (SGD), and artificial neural network methods (G´eron, 2017). Jame et al. (2013) discusses both the theory and practical implementation of Naïve Bayes (NB), knearest-neighbors (kNN), and tree-based boosting methods (James et al., 2013). Hastie et al. (2016) provides similar coverage for all the models used in this analysis, including some key ML concepts such as bootstrapping, boosting, bagging, and ensemble learning (Hastie et al., 2016). Murphy (2012) covers the various methods from a more theoretical and probabilistic perspective (Mu", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 44, "year": "2017"}}
{"text": " ensemble learning (Hastie et al., 2016). Murphy (2012) covers the various methods from a more theoretical and probabilistic perspective (Murphy, 2012).(FP) rate as a function of the class membership probability (Fawcett, 2006). Intuitively, AUC measures the power of a model to distinguish among classes in the target attribute. An AUC score of 0.  indicates that the model has no ability to distinguish among classes of the target whereas a value approaching 1.  indicates that the model offers a large increase in TP rate for a small price of slightly increasing the FP rate.The performance evaluation procedure also monitored the classifi cation accuracy (CA), precision (Pc), recall (Rc), and F1", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 45, "year": "2017"}}
{"text": "asing the FP rate.The performance evaluation procedure also monitored the classifi cation accuracy (CA), precision (Pc), recall (Rc), and F1 scores. Table 12 describes each metric and summarizes their advantages and disadvan tages. All performance metric except the AUC was sensitive to class imbalance in the dataset.CA is one of the most often cited performance metric for ML classi fiers. However, a high CA score can be misleading if the dataset has high class imbalanced. For example, a no-skill algorithm applied to a dataset with only 5% of the instances from one class and the rest from the other class will appear to have a 95 % accuracy if it picks the dominant class for every prediction.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 46, "year": "2017"}}
{"text": "es from one class and the rest from the other class will appear to have a 95 % accuracy if it picks the dominant class for every prediction. Stratified sampling of both the training and testing datasets helps to reduce the imbalance (Krawczyk, 2016).## 3.3.2. Hyperparameter tuningEach model requires that the user select values for key parameters (hyperparameters) that affect their performance. Tuning hyper parameters require incremental adjustments while observing a perfor mance metric. The optimization loop uses k-fold cross validation to maximize the model generalization on the entire dataset while reducing any tendency towards overfitting or underfitting .", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 47, "year": "2017"}}
{"text": "ross validation to maximize the model generalization on the entire dataset while reducing any tendency towards overfitting or underfitting . Models that have regu larization parameters provide a means to balance the unavoidable tradeoff between bias and variance , which improves generalization on unseen data. James et al. (2013) provides an excellent description of the above ML terminologies and concepts, so the book is incorporated here by reference James et al. (2013). The performance evaluation metric used was the area under the curve (AUC) of the receiver operating characteristic (ROC). The AUC trends with hyperparameter value ad justments show where each model achieved its best regulari", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 48, "year": "2017"}}
{"text": "eiver operating characteristic (ROC). The AUC trends with hyperparameter value ad justments show where each model achieved its best regularized performance.## 3.4. Feature rankingAttributes that contain noisy, irrelevant, or redundant information can diminish the performance of ML methods (Yu and Liu, 2003). Hence, data scientists developed various methods to score features based on the amount of information they contribute towards distinguishing the target classes. This section compares five methods that rank features based on the strength of their association with the classes in the target attribute. Table 13 provides a short description of each method and a reference that provides details", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 49, "year": "2017"}}
{"text": "ciation with the classes in the target attribute. Table 13 provides a short description of each method and a reference that provides details about their theory of operations.All methods work best with normalized attributes because their magnitudes become comparable. The diversity of methods result inDecision Tree (DT): Random Forest (RF)\nRecursive tree node splitting to maximize the purity of sub-trees. HP: Minimum number of instances in leaves (N), and minimum size of subsets (S).: Build many full trees for voting. Each tree grows from a bootstrapped dataset and a random subset of attributes. HP: Number of trees (N) and minimum size of subsets (S).\nA: Simple to interpret and to visualize.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 50, "year": "2017"}}
{"text": "dataset and a random subset of attributes. HP: Number of trees (N) and minimum size of subsets (S).\nA: Simple to interpret and to visualize. Works with non-numerical categorical attributes. D: Tends to overft, resulting in low predictive power on new data.: A: Offers the simplicity and intuition of decision trees but with less tendency to overft, therefore, improves generalization on unseen data. D: Incomplete trees diminish insights that full trees might otherwise provide.Decision Tree (DT): **Tree-Based** **Methods**\nRecursive tree node splitting to maximize the purity of sub-trees. HP: Minimum number of instances in leaves (N), and minimum size of subsets (S).: Ada Boost (AB) Extreme Grad", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 51, "year": "2017"}}
{"text": "ximize the purity of sub-trees. HP: Minimum number of instances in leaves (N), and minimum size of subsets (S).: Ada Boost (AB) Extreme Gradient Boost (XGB) Gradient Boost (GB)\nA: Simple to interpret and to visualize. Works with non-numerical categorical attributes. D: Tends to overft, resulting in low predictive power on new data.: Sequentially build improved shallow trees for voting. HP: Number of estimators (N), learning rate (R), boosting algorithm, and regression loss function. A highly confgurable version of gradient boosting. HP: Number of estimators (N), learning rate (R), maximum tree depth (S), loss function. Sequentially build improved models that ft the errors of previous models.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 52, "year": "2017"}}
{"text": "ors (N), learning rate (R), maximum tree depth (S), loss function. Sequentially build improved models that ft the errors of previous models. HP: Number of estimators (N), learning rate (R), maximum tree depth (S), loss function.Decision Tree (DT): **Statistical** **Models**\nRecursive tree node splitting to maximize the purity of sub-trees. HP: Minimum number of instances in leaves (N), and minimum size of subsets (S).: k-Nearest Neighbors (k-NN) Naïve Bayes (NB)\nA: Simple to interpret and to visualize. Works with non-numerical categorical attributes. D: Tends to overft, resulting in low predictive power on new data.: Determine the class of an instance based on the majority class of its k nea", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 53, "year": "2017"}}
{"text": "Tends to overft, resulting in low predictive power on new data.: Determine the class of an instance based on the majority class of its k nearest neighbors. HP: Number of neighbors (k), distance method. Applies Bayes theorem to determine the class probability, given probabilities of the observations. HP: NoneDecision Tree (DT): **Decision** **Boundaries** **Learned** **Functions**\nRecursive tree node splitting to maximize the purity of sub-trees. HP: Minimum number of instances in leaves (N), and minimum size of subsets (S).: Logistic Regression (LR) Support Vector Machine (SVM) Stochastic Gradient Descent (SGD) Artifcial Neural Network (ANN)\nA: Simple to interpret and to visualize.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 54, "year": "2017"}}
{"text": " (LR) Support Vector Machine (SVM) Stochastic Gradient Descent (SGD) Artifcial Neural Network (ANN)\nA: Simple to interpret and to visualize. Works with non-numerical categorical attributes. D: Tends to overft, resulting in low predictive power on new data.: Establish a decision boundary by using a logistic function to maximally separate classes. HP: Regularization function and strength (C), and probability threshold. Establish a decision boundary by fnding a multidimensional hyperplane to maximally separate classes. HP: Kernel type, cost (C), and regression loss (ε) An optimization technique that fts a linear multivariate function to the data. It works best when all features are scaled.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 55, "year": "2017"}}
{"text": "gression loss (ε) An optimization technique that fts a linear multivariate function to the data. It works best when all features are scaled. HP: Loss function, learning rate method and parameters. A weighted multilayer linear network that represents a function. HP: Hidden layer neurons (N), solver type, regularization parameter (α), number of iterations (I).## 3.5. Principle component analysisThe method of principle component analysis (PCA) creates a set of new orthogonal basis vectors, each maximally spanning the dimensions of feature space, in the order of the data variance (Jolliffe and Cadima, 2016). Each principle component (PC) is a linear combination of all numerical features in the d", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 56, "year": "2017"}}
{"text": "r of the data variance (Jolliffe and Cadima, 2016). Each principle component (PC) is a linear combination of all numerical features in the dataset. Intuitively, the first two principle components form a plane in feature space that is closest to all the data instances, as measured by the Euclidean distance. Data clusters tend to form along the directions of maximum variance. Hence, attributes that most influence the formation of data clusters contribute to inherent structure in the data. The terminology used in the literature is that each PC “explains” some proportion of the total variance (information) in the dataset.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 57, "year": "2017"}}
{"text": " data. The terminology used in the literature is that each PC “explains” some proportion of the total variance (information) in the dataset. Therefore, features that are weak components of most PCs tend to be associated with noise in the data. Analyst also use PCA to transform high dimensional data into a lower dimension feature space to enable the visualization of both structure and noise in the dataset (Anowar et al., 2021).## **4. Results**The subsections of this section present the results of applying ma chine learning, attribute ranking, and PCA to the cleaned and trans formed dataset presented in the previous section.## 4.1.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 58, "year": "2017"}}
{"text": " of applying ma chine learning, attribute ranking, and PCA to the cleaned and trans formed dataset presented in the previous section.## 4.1. Machine learningTable 14 summarizes the stabilized performance of each ML algo rithm, sorted by the AUC metric. The null model is a no-skill model that predicts the dominant class each time. It provided a baseline to compare the performance score of skilled classifiers. As expected, the CA score for the no-skill classifier reflected the class imbalance of 67.  % for derailment type accidents versus non-derailment type accidents. How ever, the AUC performance of the null classifier was lowest as expected.Tracking the AUC trend with 10-fold cross validati", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 59, "year": "2017"}}
{"text": "pe accidents. How ever, the AUC performance of the null classifier was lowest as expected.Tracking the AUC trend with 10-fold cross validation and stratified sampling produced the optimum hyperparameter values shown in the table. Hyperparameters with common names across some models were the learning rate (L), loss function (LF), regularization (R) parameters,some compensating for the weaknesses of the other; therefore, they do not provide identical rankings (Wang et al., 2010). However, a strong correlation among rankings indicates that the top-ranking attributes do contribute most towards ML classifcation performance.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 60, "year": "2017"}}
{"text": "ever, a strong correlation among rankings indicates that the top-ranking attributes do contribute most towards ML classifcation performance. Method Description Reference ANOVA Analysis of Variance (ANOVA) measures the difference between average values of a feature in different classes of the target, based on the F distribution. (Agresti, 2018) Chi-Squared Measures a dependency or association between the feature and the target class by using a chi- square statistic. (Wang et al., 2010) Information Gain The expected amount of entropy reduction. A decrease in entropy (uncertainty) based on the presence of other features will increase information.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 61, "year": "2017"}}
{"text": "expected amount of entropy reduction. A decrease in entropy (uncertainty) based on the presence of other features will increase information. (Yu and Liu, 2003) Gain Ratio Reduces the bias of Information Gain towards features that have many values by taking the ratio of Information Gain to the intrinsic information (entropy) of the feature. (Quinlan, 1986) Gini Decrease A measure of the inequality among values of a frequency distribution based on their statistical dispersion. A value of zero and one represents perfect equality and inequality, respectively, of the distribution of a feature within each target class.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 62, "year": "2017"}}
{"text": "A value of zero and one represents perfect equality and inequality, respectively, of the distribution of a feature within each target class. (Han et al., 2016): XGB 0.888 0.828 0.875 0.859 0.892 γ:0, Max Depth: 6, Min Child Weight: 1, R:1, w:1, L:0.2, GB 0.884 0.824 0.872 0.854 0.891 LF: LR, Trees (N): 100, L: 0.2, Min Samples Leaf: 1 RF 0.882 0.821 0.817 0.817 0.821\nTrees (N): 60, Attributes/ Split: 5, Min Subset: 5 DT 0.854 0.803 0.801 0.800 0.803\nMax Depth: 10, Min Samples Leaf (N): 90, Min Subset: 5 ANN 0.838 0.786 0.785 0.784 0.786\nHidden Nodes: 100, Activation: ReLu, OA: Adam (α:10−4) LR 0.832 0.783 0.777 0.777 0.783 R (L2, C:5) SGD 0.828 0.783 0.776 0.776 0.783 LF: (LR,ε:1), R: E.Net", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 63, "year": "2017"}}
{"text": ", Activation: ReLu, OA: Adam (α:10−4) LR 0.832 0.783 0.777 0.777 0.783 R (L2, C:5) SGD 0.828 0.783 0.776 0.776 0.783 LF: (LR,ε:1), R: E.Net (α:10−5, 0.15), L: IVS (η0:10-2,t:0.25) kNN 0.803 0.765 0.759 0.758 0.764 N: 30, Distance (Euclidean, Weights: Uniform) NB 0.794 0.725 0.730 0.740 0.725\nNo parameters to tune ADB 0.713 0.746 0.746 0.747 0.746\nTrees (N): 50, LF: Linear, OA: SAMME.R, LR: 1.0 SVM 0.626 0.654 0.639 0.633 0.654\nKernel: Sigmoid, R (C:0.2, ε:1.0) Null 0.500 0.674 0.543 0.455 0.674\nNo parameters to tuneand optimizer algorithm (OA). To demonstrate the effect of hyperparameter tuning, Fig.  plots the AUC score for a range of hyperparameter N associated with RF, kNN, and DT.As note", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 64, "year": "2017"}}
{"text": "trate the effect of hyperparameter tuning, Fig.  plots the AUC score for a range of hyperparameter N associated with RF, kNN, and DT.As noted in Table 14, the hyperparameter N represents the number of trees of a RF, the minimum number of samples to retain in the leaves of a DT, and the number of nearest neighbors for the kNN algorithm. The asymptotic trend was similar for all hyperparameters tuned.## 4.2. Feature rankingTable 15 shows the importance ranking of the first 30 features in their strength of association with the target class. The rank by each of the five scoring methods are correlated as indicated by their pairwise cor relation coefficients listed in Table 16.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 65, "year": "2017"}}
{"text": "ss. The rank by each of the five scoring methods are correlated as indicated by their pairwise cor relation coefficients listed in Table 16. The correlation ranges from 84.  % for the gini and chi-squared methods to 94.  % for the ANOVA and chi-squared methods.The distributions show that these attributes have some power to separate derailment from non-derailment type accidents, but with un certainty based on the amount of overlap in their class distributions. For example, the class probability was higher for derailment type accidents on class 0, 1, 2, 7, 8, and 9 tracks (Fig. 5a). The distinction is significant for class 1 tracks because it has the highest frequency of occurrence (Fig. 5b).", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 66, "year": "2017"}}
{"text": ", 8, and 9 tracks (Fig. 5a). The distinction is significant for class 1 tracks because it has the highest frequency of occurrence (Fig. 5b). Similarly, the class probability was higher for derailment type accidents where movement authority was within restricted limits (restricted) or where movement was not on main tracks (Fig. 5c). Similarly, the class probability was higher for derailment type accidents in non-signalized territories (Fig. 5d). The probability difference was much lower for the lower ranking attributes, but taken together, they improve the ML classification performance.All accidents tended to occur below the speed limit for the track class on which they operated.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 67, "year": "2017"}}
{"text": "ey improve the ML classification performance.All accidents tended to occur below the speed limit for the track class on which they operated. However, derailment type accidents tended to occur closer to the speed limit than non-derailment type accidents. A student’s t -test shows that the p-value was near zero, which indicated that the mean difference of 10 mph (16 kph) was statistically significant. The highlighted boxes in the figure indicates the values of the first quartile (25 %) through the third quartile (75 %) of the dataset. The solid vertical and horizontal lines indicate the mean and standard de viation, respectively.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 68, "year": "2017"}}
{"text": "h the third quartile (75 %) of the dataset. The solid vertical and horizontal lines indicate the mean and standard de viation, respectively. The lighter solid vertical lines indicate the median values.## 4.3. Principle component analysis## **5. Discussion****Fig. 8.** Data clusters for attributes with high power to distinguish among the target classes.effectiveness of the custom data cleaning procedures, including the LAP technique introduced for imputing missing values. The LAP method was most effective in filling missing values for track density, but that attri bute ranked low in importance for classification.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 69, "year": "2017"}}
{"text": " LAP method was most effective in filling missing values for track density, but that attri bute ranked low in importance for classification. Although effective, one limitation of the LAP technique is that it provided a course imputation of the geospatial coordinates, based on an aggregation of entries from other records where a value was present for the track type near that station. However, the LAP method provided a more intelligent and effective scheme to impute missing values such as track density based on those of spatial neighbors rather than guessing values based on the mean, most frequent, or nearest neighbors in feature space.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 70, "year": "2017"}}
{"text": "ity based on those of spatial neighbors rather than guessing values based on the mean, most frequent, or nearest neighbors in feature space. The geospatial join method provided the next best alternative to repair erroneous or lowresolution geospatial data. The distinctive southeast skew pattern revealed those records with low-resolution data entry.relative power to separate the distributions of the categories in the target class. That is, an exceptionally high overlap of the two class distributions ranked the attribute exceptionally low in importance towards classifier performance. It is rare that any one attribute can completely distinguish among class members with 100 % accuracy, otherwise", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 71, "year": "2017"}}
{"text": "ards classifier performance. It is rare that any one attribute can completely distinguish among class members with 100 % accuracy, otherwise there would be no need to use additional attributes as explanatory factors for classifi cation. Rather, a combination of attributes contributes their ability to help determine the probability of class membership. Poor classification results with all types of classification models may indicate that all at tributes have a high degree of overlap in their class probability distributions.The PCA result (Fig. 7) shows that the first 6 PCs explain more than half the variance in the dataset but that it takes the remaining PCs, which accounted for 88 % of the PC", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 72, "year": "2017"}}
{"text": "t the first 6 PCs explain more than half the variance in the dataset but that it takes the remaining PCs, which accounted for 88 % of the PCs, to explain the remaining half of the variance in the data set. This outcome indicates that the first six PCs represented the bulk of the information in the dataset. By extension, the remaining PCs likely account for noise in the dataset based on the slow accumulation of the variance they explained. This result suggests that just under half of the variance in the dataset lack structure and, there fore, constitutes the noise in the dataset.The top four algorithms of XGB, GB, RF, and DT were all based on the theories of decision trees.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 73, "year": "2017"}}
{"text": " fore, constitutes the noise in the dataset.The top four algorithms of XGB, GB, RF, and DT were all based on the theories of decision trees. Using the traditional academic grading system for performance, the top four models provided “good” overall perfor mance based on an AUC score greater than 85 %. The highest AUC score of nearly 89 % for XGB was associated with a classification accuracy and balanced precision-recall scores (F1) of nearly 83 % and 88 %, respec tively. All methods were sensitive to hyperparameter tuning as demonstrated in the performance improvement trends of Fig. 4. The hyperparameter tuning sensitivity cautions against using the default values suggested for each method.Al", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 74, "year": "2017"}}
{"text": "e improvement trends of Fig. 4. The hyperparameter tuning sensitivity cautions against using the default values suggested for each method.All feature ranking methods and PCA pointed to track class (TRKCL), signalized movement authority (MOVEx = Signal), speed excess, and signalized territory (SIG) as the most important features in ML classifier performance. The interpretation of an attribute rank is its**Fig. 9.** Data clusters for attributes with low power to distinguish among the target classes.main track types (Fig. 8c). This result suggests that features that align with the cluster where the derailment class is biased associates more with derailment than non-derailment type accidents.att", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 75, "year": "2017"}}
{"text": "ures that align with the cluster where the derailment class is biased associates more with derailment than non-derailment type accidents.attribute similarity. Consequently, noisy neighborhoods can hamper classification performance as evidenced by the low performance rank of kNN. Methods such as SVM and LR seek clear decision boundaries in multidimensional feature space. Hence, the lack of clear hyperplanes between the target classes hampered their performance. In fact, SVM achieved the lowest performance.Overall, the analysis suggested that derailments were more strongly associated with lower track classes, non-signalized territories, and movement authorizations with restricted limits.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 76, "year": "2017"}}
{"text": "ents were more strongly associated with lower track classes, non-signalized territories, and movement authorizations with restricted limits. Derailments also tended to occur at 10 mph (16 kph) below the speed limit of the track class whereas non-derailment type accidents tended to occur at 20 mph (32 kph) below the limit. Those findings correspond with the intuition that lower-class tracks, which has lower speed limits, and movements with restricted limits are so designated because those operations are associ ated with higher safety risks, which the ML confirmed. Similarly, there is less guidance for movements in territories without signalization, so the risk of derailments due to track irre", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 77, "year": "2017"}}
{"text": "onfirmed. Similarly, there is less guidance for movements in territories without signalization, so the risk of derailments due to track irregularities or switch problems is higher. However, it may not be wise to go beyond probabilities and statistical associations by assuming general latent reasons for the ML outcome because there are no exclusive distinctions between accident causes for each accident type.The above insights about the location of structure and noise in the dataset provided clues to understand the reason for the performance differences of each ML method. Randomized tree-based methods tend to train on various cross-sections of a dataset and use voting to determine the class li", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 78, "year": "2017"}}
{"text": " each ML method. Randomized tree-based methods tend to train on various cross-sections of a dataset and use voting to determine the class likelihood. In contrast, the other methods tend to leverage structure in the dataset. Hence, the randomized tree-based methods such as XGB, GB, and RF performed better by discovering patterns across noisy neighborhoods in the dataset. On the other hand, kNN seeks local neighborhoods in feature space to predict class membership based onOne limitation of the railroad accident database is that it does not necessarily list accidents where the financial loss was below $10,500 because the FRA does not require railroads to report those.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 79, "year": "2017"}}
{"text": "t does not necessarily list accidents where the financial loss was below $10,500 because the FRA does not require railroads to report those. A second limitation is that the financial loss includes only the costs of repairing equipment, signal systems, and infrastructure structures. Losses do not include costs associated with cleanup, lost freight, societal damages, fatalities, injuries, and line closures. Nevertheless, financial loss was nota pre-incident explanatory variable, but any future analysis that uses it should be aware of this limitation in the dataset.## **6. Conclusions**## **References**Recent advancements in computing capacity and their cost reduction has enabled machine learni", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 80, "year": "2017"}}
{"text": " dataset.## **6. Conclusions**## **References**Recent advancements in computing capacity and their cost reduction has enabled machine learning (ML) methods to uncover patterns in large multidimensional datasets that are difficult to analyze with common rule-based and statistical methods. However, there are many types of ML techniques, and no single method works best for all types of datasets. Therefore, this work applied 11 different types of ML models to a large multidimensional dataset of railroad accidents to compare their per formance in predicting derailments from other accident types. The extreme gradient boosting (XGB) classifier provided the best predictive performance with an AUC sc", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 81, "year": "2017"}}
{"text": "erailments from other accident types. The extreme gradient boosting (XGB) classifier provided the best predictive performance with an AUC score of 89 %. The model could distinguish accident type with an accuracy of 83 %. Principle component analysis (PCA) revealed that high feature contamination noise and isolation noise would prevent significant further gains in classification accuracy by any algorithm.The good ML performance affirmed the relevance and sufficiency of the attributes in their contribution towards distinguishing derailments from other accident types. Hence, knowing the relative importance of those attributes towards classification accuracy can lead to insights for decision-mak", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 82, "year": "2017"}}
{"text": "dent types. Hence, knowing the relative importance of those attributes towards classification accuracy can lead to insights for decision-making in railroad risk management. The importance ranking used five different methods that agreed on the ranking with correlations – ranging from 84.2% 94.5%. The ANOVA and chi-squared methods agreed with the highest correlation that the top four attributes were track class, the type of movement authority, the excess speed, and the presence of signalization in the territory. The feature distribution for each target class and the PCA agreed that relative to non-derailment type accidents, derailments were more strongly associated with lower track classes, no", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 83, "year": "2017"}}
{"text": "ss and the PCA agreed that relative to non-derailment type accidents, derailments were more strongly associated with lower track classes, non-signalized territories, and movement authorizations with restricted limits. Derailments also tended to occur at 10 mph (16 kph) below the speed limit of the track class whereas non-derailment type accidents tended to occur at 20 mph (32 kph) below the limit.The good ML performance also suggests that the custom data imputation techniques presented were effective in filling missing values. The data-cleaning framework also demonstrated a spatial join technique that addressed 21.  % of the geospatial data entry errors.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 84, "year": "2017"}}
{"text": "ng values. The data-cleaning framework also demonstrated a spatial join technique that addressed 21.  % of the geospatial data entry errors. The detailed chronicle of the cleaning procedures will help other researchers save a substantial amount of time in data preparation when using the same dataset. Future work will leverage the framework to examine trends in accidents caused by human error to determine the effectiveness of PTC deployments relative to historic accident rates.## **Data availability**This paper cited the sources of all the data used, which are currently publicly available.", "metadata": {"doc_id": "Railroad accident analysis using extreme gradient boosting", "section": "ROOT", "level": 0, "chunk_id": 85, "year": "2017"}}
