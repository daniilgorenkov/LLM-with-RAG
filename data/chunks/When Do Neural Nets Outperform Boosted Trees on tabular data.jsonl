{"text": "# **When Do Neural Nets Outperform Boosted Trees on Tabular Data?****Duncan McElfresh** [∗] [,] **, Sujay Khandagale** **, Jonathan Valverde** **, Vishak Prasad C** **, Ganesh Ramakrishnan** **, Micah Goldblum** **, Colin White** [,]## **Abstract**## **1 Introduction**Tabular datasets are data organized into rows and columns, consisting of distinct features that are typically continuous, categorical, or ordinal. They are the oldest and among the most ubiquitous dataset types in machine learning in practice , due to their numerous applications across medicine , finance , online advertising , and many other areas .Despite recent advances in neural network (NN) architectures for tabular data ,", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "ROOT", "level": 0, "chunk_id": 0}}
{"text": "cine , finance , online advertising , and many other areas .Despite recent advances in neural network (NN) architectures for tabular data , there is still an active debate over whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with multiple works arguing either for  or against  NNs. This is in stark contrast to other areas such as computer vision and natural language understanding, in which NNs have far outpaced competing methods .Nearly all prior studies of tabular data use fewer than 50 datasets or do not properly tune baselines , putting the generalizability of these findings into question.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "ROOT", "level": 0, "chunk_id": 1}}
{"text": "of tabular data use fewer than 50 datasets or do not properly tune baselines , putting the generalizability of these findings into question. Furthermore, the bottom line of many prior works is to answer the question, ‘which performs better, NNs or GBDTs, in terms of the average rank across datasets’ without searching for more fine-grained insights.**----- Start of picture text -----**\n “NNs vs. GBDTs” Metafeatures\nAlgorithms Datasets TabZilla Suite\nNNs GBDTs", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "ROOT", "level": 0, "chunk_id": 2}}
{"text": "Neural nets, OpenML Suite,\nGBDTs, CC-18, GBDTs handle No datasets s.t.\nbaselines addtl. OpenML irregularity better many methods win\nΔAlgo. selection Neural nets Large variety\n500K total evaluations HPO is just as important as handle small 10 folds per dataset\nchoosing NNs/GBDTs datasets better\nIrregularity\nΔHPO\n**----- End of picture text -----**In this work, we take a completely different approach by focusing on the following points. First, we question the importance of the ‘NN vs. GBDT’ debate, by investigating the significance of algorithm selection. Second, we analyze what properties of a dataset make NNs or GBDTs better-suited to perform well.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 0}}
{"text": "ng the significance of algorithm selection. Second, we analyze what properties of a dataset make NNs or GBDTs better-suited to perform well. We take a data-driven approach to answering these questions, conducting the largest tabular data analysis to date, by comparing **19 algorithms each with up to 30 hyperparameter settings, across 176 datasets** , including datasets from the OpenML-CC18 suite  and the OpenML Benchmarking Suite . To assess performance differences across datasets, we consider dozens of metafeatures. We use 10 folds for each dataset to further reduce the uncertainty of our results.We find that for a surprisingly high fraction of datasets, either a simple baseline (such as a", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 1}}
{"text": "urther reduce the uncertainty of our results.We find that for a surprisingly high fraction of datasets, either a simple baseline (such as a decision tree or KNN) performs on par with the top algorithms; furthermore, for roughly one-third of all datasets, light hyperparameter tuning on CatBoost or ResNet increases performance more than choosing among GBDTs and NNs. These results show that for many tabular datasets, it is not necessary to try out many different NNs and GBDTs: **in many cases, a strong baseline or a well-tuned GBDT will suffice** . While NNs are the best approach for a non-negligible fraction of the datasets in this study, we do find that GBDTs outperform NNs on average over al", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 2}}
{"text": "s are the best approach for a non-negligible fraction of the datasets in this study, we do find that GBDTs outperform NNs on average over all datastes.Next, we run analyses to discover what properties of datasets explain which methods, or families of methods, do or do not succeed. We compute the correlations of various metafeatures with algorithm performance, and we demonstrate that these correlations are predictive. Our main findings are as follows (also see Figure 5): **dataset** **regularity** **is predictive of NNs outperforming GBDTs** (for example, feature distributions that are less skewed and less heavy-tailed).", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 3}}
{"text": "**regularity** **is predictive of NNs outperforming GBDTs** (for example, feature distributions that are less skewed and less heavy-tailed). Furthermore, GBDTs tend to perform better on larger datasets.Finally, with the goal of accelerating tabular data research, we release the **TabZilla Benchmark Suite** : a collection of the ‘hardest’ of the 176 datasets we studied. We select datasets on which a simple baseline does not win, as well as datasets such that most algorithms do not reach top performance.- We conduct the largest analysis of tabular data to date, comparing 19 methods on 176 datasets, with more than half a million models trained.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 4}}
{"text": "We conduct the largest analysis of tabular data to date, comparing 19 methods on 176 datasets, with more than half a million models trained. We show that for a surprisingly high fraction of datasets, either a simple baseline performs the best, or light hyperparameter tuning of a GBDT is more important than choosing among NNs and GBDTs, suggesting that the ‘NN vs. GBDT’ debate is overemphasized.- After analyzing dozens of metafeatures, we present a number of insights into the properties that make a dataset better-suited for GBDTs or NNs.- We release the TabZilla Suite: a collection of 36 ‘hard’ datasets, with the goal of accelerating tabular data research.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 5}}
{"text": "d for GBDTs or NNs.- We release the TabZilla Suite: a collection of 36 ‘hard’ datasets, with the goal of accelerating tabular data research. We open-source our benchmark suite, codebase, and raw results.**Related work.** Tabular datasets are the oldest and among the most common dataset types in machine learning in practice , due to their wide variety of applications . GBDTs  iteratively build an ensemble of decision trees, with each new tree fitting the residual of the loss from the previous trees, using gradient descent to minimize the losses. XGBoost , LightGBM , and Catboost  are three widely-used, high-performing variants. Borisov et al.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 6}}
{"text": " gradient descent to minimize the losses. XGBoost , LightGBM , and Catboost  are three widely-used, high-performing variants. Borisov et al. described three types of tabular data approaches for neural networks : data transformation methods , architecture-based methods  (including transformers ), and regularization-based methods . Several recent works compare GBDTs to NNs on tabular data, often finding that either NNs  or GBDTs  perform best.Perhaps the most related work to ours is by Grinsztajn et al. , who investigate why tree-based methods outperform neural nets on tabular data. There are a few differences between their work and ours.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 7}}
{"text": "l. , who investigate why tree-based methods outperform neural nets on tabular data. There are a few differences between their work and ours. First, they consider seven algorithms and 45 datasets, compared to our 19 algorithms and 176 datasets. Second, their dataset sizes range from 3 000 to 10 000, or seven that are exactly 50 000, in contrast to our dataset sizes which range from 32 to 1 025 009 . Additionally, they further control their study, for example by upper bounding the ratio of size to features, by removing high-cardinality categorical feature, and by removing low-cardinality numerical features. While this has the benefit of being a more controlled study, their analysis misses out", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 8}}
{"text": " and by removing low-cardinality numerical features. While this has the benefit of being a more controlled study, their analysis misses out on some of our observations, such as GBDTs performing better than NNs on ‘irregular’ datasets. Finally, while Grinsztajn et al. focused in depth on a few metafeatures such as dataset smoothness and number of uninformative features, our work considers orders of magnitude more metafeatures. Again, while each approach has its own strengths, our work is able to discover more potential insights, correlations, and takeaways for practitioners. To the best of our knowledge, the only related work has considered more than 50 datasets is TabPFN , which considered 1", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 9}}
{"text": "s for practitioners. To the best of our knowledge, the only related work has considered more than 50 datasets is TabPFN , which considered 179 datasets which are size 2 000 or smaller. See Appendix C for a longer discussion of related work.## **2 Analysis of Algorithms for Tabular Data**In this section, we present a large-scale study of techniques for tabular data across a wide variety of datasets. Our analysis seeks to answer the following two questions.1. How do algorithms (and algorithm families) compare across a wide variety of datasets?2. What properties of a dataset are associated with algorithms (and families) outperforming others?**Algorithms and datasets implemented.** We present re", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 10}}
{"text": "rties of a dataset are associated with algorithms (and families) outperforming others?**Algorithms and datasets implemented.** We present results for 19 algorithms, including popular recent techniques and baselines. The methods include three GBDTs: CatBoost , LightGBM , and XGBoost ; 11 neural networks: DANet , FT-Transformer , two MLPs , NODE , ResNet , SAINT , STG , TabNet , TabPFN , and VIME ; and five baselines: Decision Tree , KNN , Logistic Regression , Random Forest , and SVM . We choose these algorithms because of their popularity, diversity, and strong performance.We run the algorithms on 176 classification datasets from OpenML .", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 11}}
{"text": "algorithms because of their popularity, diversity, and strong performance.We run the algorithms on 176 classification datasets from OpenML . Our aim is to include most classification datasets from popular recent papers that study tabular data , including datasets from the OpenML-CC18 suite , the OpenML Benchmarking Suite , and additional OpenML datasets. Due to the scale of our experiments (538 650 total models trained), we limit the run-time for each experiment (described below), which precluded the use of datasets of size larger than 1.1M. Table 6 shows summary statistics for all datasets. CC-18 and OpenML Benchmarking Suite are both seen as the go-to standards for conducting a fair, diver", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 12}}
{"text": "s summary statistics for all datasets. CC-18 and OpenML Benchmarking Suite are both seen as the go-to standards for conducting a fair, diverse evaluation across algorithms due to their rigorous selection criteria and wide diversity of datasets . To the best of our knowledge, our 19 algorithms and 176 datasets are the largest number of either algorithms or datasets (with the exception of TabPFN ) considered by recent tabular dataset literature, and the largest number available in a single open-source repository.**Metafeatures.** We extract metafeatures using the Python library PyMFE , which contains 965 metafeatures.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 13}}
{"text": " a single open-source repository.**Metafeatures.** We extract metafeatures using the Python library PyMFE , which contains 965 metafeatures. The categories of metafeatures include: ‘general’ (such as number of datapoints, classes, or numeric/categorical features), ‘statistical’ (such as the min, mean, or max skewness, or kurtosis, of all feature distributions), ‘information theoretic’ (such as the Shannon entropy of the target), ‘landmarking’ (the performance of a baseline such as 1-Nearest Neighbor on a subsample of the dataset), and ‘model-based’ (summary statistics for some model fit on the data, such as number of leaf nodes in a decision tree model).", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 14}}
{"text": " the dataset), and ‘model-based’ (summary statistics for some model fit on the data, such as number of leaf nodes in a decision tree model). Since some of these features have long-tailed distributions, we also include the log of each strictly-positive metafeature in our analysis.**Experimental design.** For each dataset, we use the ten train/test folds provided by OpenML, which allows our results on the test folds to be compared with other works that used the same OpenML datasets. Since we also need validation splits in order to run hyperparameter tuning, we divide each training fold into a training and validation set.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 15}}
{"text": "Since we also need validation splits in order to run hyperparameter tuning, we divide each training fold into a training and validation set. For each algorithm, and for each dataset split, we run the algorithm for up to 10 hours. During this time, we train and evaluate the algorithm with at most 30 hyperparameter sets (one default set and 29 random sets, using Optuna ). Each parameterized algorithm is given at most two hours on a 32GiB V100 to complete a single train/evaluation cycle. In line with prior work, our main metric of interest is accuracy , and we report the test performance of the hyperparameter setting that had the maximum performance on the validation set.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 16}}
{"text": "erest is accuracy , and we report the test performance of the hyperparameter setting that had the maximum performance on the validation set. We also consider log loss, which is highly correlated with accuracy but contains significantly fewer ties. We also include results for F1-score and ROC AUC in Appendix D. Similar to prior work , whenever we average across datasets, we use the average distance to the minimum (ADTM) metric, which consists of 0-1 scaling (after selecting the best hyperparameters, which helps protect against outliers ). Finally, in order to see the variance of each method on different folds of the same dataset, we report the average (scaled) standard deviation of each metho", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 17}}
{"text": "r to see the variance of each method on different folds of the same dataset, we report the average (scaled) standard deviation of each method across all 10 folds.## **2.  Relative Algorithm Performance**In this section, we answer the question, “How do individual algorithms, and families of algorithms, perform across a wide variety of datasets?” We especially consider whether the difference between GBDTs and NNs is significant.**No individual algorithm dominates.** We start by comparing the average rank of all algorithms across all datasets, while excluding datasets which ran into memory or timeout issues on a nontrivial number of algorithms using the experimental setup described above.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 18}}
{"text": "xcluding datasets which ran into memory or timeout issues on a nontrivial number of algorithms using the experimental setup described above. Therefore, we consider a setof 104 datasets (and we include results on all 176 datasets in the next section and in Appendix D.2). As mentioned in the previous section, for each algorithm and dataset split, we report the test set performance after tuning on the validation set; see Table 1.Surprisingly, nearly every algorithm ranks first on at least one dataset and last on at least one other dataset . As expected, baseline methods tend to perform poorly while neural nets and GBDTs tend to perform better on average.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 19}}
{"text": "east one other dataset . As expected, baseline methods tend to perform poorly while neural nets and GBDTs tend to perform better on average. The fact that the best out of all algorithms, CatBoost, only achieved an average rank of 5.06, shows that there is not a single approach that dominates across most datasets. In Table 2, we compute the same table for the 57 datasets with size at most 1250 (training set at most 1000), so that we can include TabPFN  in our rankings. **We find that TabPFN achieves the best average performance of all algorithms, while also having the fastest training time.** However, with an average rank of 4.88, it still does not dominate all other approaches across differe", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 20}}
{"text": "so having the fastest training time.** However, with an average rank of 4.88, it still does not dominate all other approaches across different datasets. Furthermore, the inference time for TabPFN is higher than other algorithms.**Performance vs. runtime.** In Figure 2, we plot the accuracy vs. runtime for all algorithms, averaged across all datasets. Overall, neural nets require the longest runtime, and often outperform baseline methods. On the other hand, GBDTs simultaneously require little runtime while also achieving strong performance: they consistently outperform baseline methods, and consistently require less runtime than neural nets.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 21}}
{"text": "le also achieving strong performance: they consistently outperform baseline methods, and consistently require less runtime than neural nets. There is one caveat: our experiments train each neural net for a pre-determined number of epochs (100 in most cases), with early stopping if there is no improvement for 20 epochs. It is possible that these neural nets can achieve**----- Start of picture text -----**\n 1.  XGBoost CatBoost FTTransformer SAINT\n0.  DecisionTree\n0.  SVM Res Net\n0.  RandomForest\nVIME\n0.  KNN LinearModel\n0.  neural baseline gbdt\n10 2 10 1 100 101 102\nTraining time per 1000 instances (sec)\nAcaruccy\nd ilNeamroz", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 22}}
{"text": "omForest\nVIME\n0.  KNN LinearModel\n0.  neural baseline gbdt\n10 2 10 1 100 101 102\nTraining time per 1000 instances (sec)\nAcaruccy\nd ilNeamroz\n**----- End of picture text -----**strong performance with less runtime, e.g., with a more aggressive early-stopping criterion.**----- Start of picture text -----**\n Log Loss\n18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1\nVIME CatBoost\nDecisionTree XGBoost\nKNN SAINT\nMLP ResNet\nNODE LightGBM\nMLP-rtdl SVM\nTabNet FTTransformer\nLinearModel RandomForest\nSTG DANet\n**----- End of picture text -----****----- Start of picture text -----**\n High - Performing Algorithm Family over 176 Datasets\nThreshold = 0.  100\nNeural Nets GBDTs\n10 1\n46 15 60 10 2", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 23}}
{"text": "Start of picture text -----**\n High - Performing Algorithm Family over 176 Datasets\nThreshold = 0.  100\nNeural Nets GBDTs\n10 1\n46 15 60 10 2\n(26%) (9%) (34%)\n13 10 3\n(7%)\n(59%) (58%) 10 4\n10 5\n25\n(14%) 0 10 4 10 2 100\nBaselines CatBoost tuning improvement\nD\nB\nG\n-\nN\nN\nlt u\nfae\nd\nt\ns\ne\n**----- End of picture text -----****Statistically significant performance differences.** Table 1 shows that many algorithms have similar performance. Next, we determine statistically significant ( p  0 (the best neural net beats the best GBDT), and 0 otherwise. Table 15 shows the performance accuracy of decision trees trained on this task, with varying depth levels; we also include an XGBoost model for comparis", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 24}}
{"text": "ws the performance accuracy of decision trees trained on this task, with varying depth levels; we also include an XGBoost model for comparison. Finally, we include a visual depiction of a simple depth-3 decision tree, in Figure 13. The decision tree classifies which of the top five algorithms performs the best. Note that the decision splits are based purely on maximizing information gain at that point in the tree.**----- Start of picture text -----**\n CatBoost minus TabNet CatBoost minus SAINT SAINT minus TabNet\n1.0\n0.5\n0.0\n0.5\n1.0\n102 103 104 102 103 104 102 103 104\nDataset Size Dataset Size Dataset Size\nbi)(CLLNTBt t teas unmsooa i)(SCLLT NIABt ts unmsooa bi)(SLLNTT NIAt eas unm", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 25}}
{"text": " 102 103 104 102 103 104\nDataset Size Dataset Size Dataset Size\nbi)(CLLNTBt t teas unmsooa i)(SCLLT NIABt ts unmsooa bi)(SLLNTT NIAt eas unm\n**----- End of picture text -----****----- Start of picture text -----**\n 1.  corr. = 0.  corr. = 0.  corr. = - 0.  corr. = - 0.33\n0.75\n0.50\n0.25\n0.00\n104 103 10 2 10 1 100 10 3\nDataset Size Inst -T o - Attributes Med. Canon. - Corr. Min. Class Freq.\n)T\nD\nB\nG\nl -\naru\ne\n(N\nll\n**----- End of picture text -----****----- Start of picture text -----**\n Is the number of instances <= 4211?\nYes No\nIs the number of instances <= 1132? Is the min. of the sparsity metric\nover all features <= 0.044?\nYes No Yes No", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 26}}
{"text": "f instances <= 4211?\nYes No\nIs the number of instances <= 1132? Is the min. of the sparsity metric\nover all features <= 0.044?\nYes No Yes No\nIs the number of numerical Is the median of the concentration Is the max. of the relative frequency of Is the ratio of features\nfeatures without NaNs <= 65? coefficient between all features and all target classes <= 0.792? to instances <= 0.003?\ntarget classes <= 0.025?\nYes No Yes No Yes No Yes No\nTabPFN ResNet XGBoost CatBoost XGBoost CatBoost SAINT ResNet\nTabPFN 189 1 0 0 0 0 0 0\nResNet 111 28 60 13 32 24 1 11\nSAINT 131 1 54 5 3 10 56 2\nCatBoost 142 0 51 81 32 69 0 0\nXGBoost 115 2 79 27 293 67 13 7", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 27}}
{"text": "N 189 1 0 0 0 0 0 0\nResNet 111 28 60 13 32 24 1 11\nSAINT 131 1 54 5 3 10 56 2\nCatBoost 142 0 51 81 32 69 0 0\nXGBoost 115 2 79 27 293 67 13 7\n101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103\n**----- End of picture text -----**Finally, we present the metafeatures most-correlated with the difference in log loss between pairs of algorithms. We consider the two best-performing algorithms from each family: CatBoost, XGBoost, ResNet, and SAINT. See Table 19 and Table 20 for statistical and general metafeatures, respectively.## **D.  Experiments on Regression Datasets**While our experiments focus on classification datasets, the TabZilla codebase is also", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 28}}
{"text": "pectively.## **D.  Experiments on Regression Datasets**While our experiments focus on classification datasets, the TabZilla codebase is also equipped to handle regression datasets—which have a continuous target variable rather than categorical or binary. We run experiments using 12 algorithms with 17 tabular regression datasets, using the same experiment design and parameters described in Section 2. Each algorithm is tuned for each dataset by maximizing the R-squared (R2) metric. The regression datasets used in these experiments have been used in recent studies of machine learning with tabular data ; each dataset corresponds to an OpenML task, and can be preprocessed exactly like the classif", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 29}}
{"text": "studies of machine learning with tabular data ; each dataset corresponds to an OpenML task, and can be preprocessed exactly like the classification datasets used in other experiments. The datasets used in these experiments are “Bank-Note-AuthenticationUCI” (OpenML task 361002), “EgyptianSkulls” (5040), “Wine” (190420), “Wisconsin-breastcancer-cytology-features” (361003), “bodyfat” (5514), “california” (361089), “chscase-foot” (5012),(mean_±stddev)|Num.\nMetafeatures| |---|---|---| |DT= 1|0.54±0.28|3| |DT≤3|0.60±0.29|21| |DT≤5|0.59±0.28|183| |DT≤7|0.64±0.29|382| |DT≤9|0.66±0.27|423| |DT≤11|0.66±0.30|529| |DT≤13|0.65±0.29|561| |DT≤15|0.68±0.30|594| |DT≤∞|0.67±0.30|685| |XGBoost|0.74±_0.33|675|“", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 30}}
{"text": "9|382| |DT≤9|0.66±0.27|423| |DT≤11|0.66±0.30|529| |DT≤13|0.65±0.29|561| |DT≤15|0.68±0.30|594| |DT≤∞|0.67±0.30|685| |XGBoost|0.74±_0.33|675|“cleveland” (2285), “colleges” (359942), “cpu-small” (4883), “dataset-sales” (190418), “kin8nm” (2280), “liver-disorders” (52948), “meta” (4729), “mv” (4774), “pbc” (4850), and “veteran” (4828).Table 16 shows the rankings of 12 algorithms on these 17 regression datasets, according to the R2 metric calculated on the test set. The general conclusions are similar to our findings with classification datasets: most algorithms perform well and poorly on at least one dataset; however, GBDTs perform particularly well, especially CatBoost.## **D.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 31}}
{"text": "sets: most algorithms perform well and poorly on at least one dataset; however, GBDTs perform particularly well, especially CatBoost.## **D.  Additional Results with Quantile Scaling**In this section, we discuss dataset preprocessing. Different papers use a variety of different preprocessing methods, and there is also a wide range in the amount of ‘built-in’ preprocessing techniques inside the algorithms themselves. Therefore, our main results minimize confounding factors by having a consistent, lightweight preprocessing (imputing NaN values). In this section, we compare 13 algorithms on the tabzilla benchmark suite with and without quantile scaling, one of the most popular techniques, for a", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 32}}
{"text": "ction, we compare 13 algorithms on the tabzilla benchmark suite with and without quantile scaling, one of the most popular techniques, for all continuous features. We use QuantileTransformer from scikit-learn . We use the same computational setup and experiment design as in our main experiments. See Table 17. We find that quantile scaling improves the simple algorithms: decision tree, MLP, random forest, and SVM, while it has little effect on the high-performing algorithms.## **D.  Experiments with Additional Hyperparameter Optimization**In our main experiments, for hyperparameter optimization (HPO), we ran 30 iterations of random search for all algorithms.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 33}}
{"text": "eter Optimization**In our main experiments, for hyperparameter optimization (HPO), we ran 30 iterations of random search for all algorithms. In this section, we test the impact of additional HPO for four algorithms: XGBoost, CatBoost, LightGBM, and RandomForest. We did not run additional HPO experiments on any neural net methods due to the substantial compute resources required.Table 18 shows the performance of these HPO experiments (algorithm suffix “(HPO)”), compared with with the performance of the default hyperparameters (suffix “(default)”), and the performance after 30 iterations of random hyperparameter search as in our main results (no suffix).", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 34}}
{"text": "parameters (suffix “(default)”), and the performance after 30 iterations of random hyperparameter search as in our main results (no suffix). As expected, additional hyperparameter tuning improves the performance of XGBoost, CatBoost, LightGBM, and RandomForest.## **D.  Forward Feature Selection for Identifying Important Dataset Attributes**In this section, we present a different method for determining which dataset attributes are related to performance differences between algorithms. Here we use greedy forward feature selection  to identify important dataset attributes. In these experiments, we study the problem of predicting the difference in normalized log loss between CatBoost and ResNet", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 35}}
{"text": "aset attributes. In these experiments, we study the problem of predicting the difference in normalized log loss between CatBoost and ResNet (two very effective GBDT and NN algorithms), using metafeatures.At a high level, greedy forward feature selection selects metafeatures sequentially which improve the performance of the meta-model. To evaluate performance we use leave-one-dataset-out cross validation: each dataset contributes 10 folds to the overall metadataset, so each fold includes 10 instances for validation and all remaining instances for training.1. Number of features normally-distributed, according to the Shapiro-Wilk test.2. Median value of the minimum of all features.3.", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 36}}
{"text": "or training.1. Number of features normally-distributed, according to the Shapiro-Wilk test.2. Median value of the minimum of all features.3. Median value of the sparsity of all features.4. Interquartile range of the mean value of all features.5. Mean of the harmonic mean of all features.i Si −\ni MI(i, y))/ \ni MI(i, y), where_Si_| ||||is the entropy of feature_i_, and_MI_(i, y)is the mutual information between| ||||feature_i_and the target_y_.| |XGBoost|SAINT|-0.22|Log of the standard deviation of the kurtosis of all features.| |XGBoost|SAINT|-0.20|Log of the standard deviation of the skewness of all features.| |XGBoost|SAINT|-0.20|Best performance over 10-fold CV of a single-node decision tr", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 37}}
{"text": " the standard deviation of the skewness of all features.| |XGBoost|SAINT|-0.20|Best performance over 10-fold CV of a single-node decision tree ft using| ||||the most-informative feature.| |XGBoost|SAINT|-0.20|Maximum skewness of all features.| |XGBoost|SAINT|0.19|Log of the Shannon entropy of the target.| |XGBoost|SAINT|-0.19|Log of the standard deviation of the absolute correlation between all pairs of| ||||features.| |XGBoost|SAINT|-0.18|Range of the skewness of all features.| |XGBoost|SAINT|0.18|Log of the range of the performance of a decision tree trained on a random| ||||attribute, over 10-fold CV.|", "metadata": {"doc_id": "When Do Neural Nets Outperform Boosted Trees on tabular data", "section": "19 Algorithms 176 Datasets Size baselines win", "level": 1, "chunk_id": 38}}
