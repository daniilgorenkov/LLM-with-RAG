networks for tabular data classification and regression. In Proceedings of the AAAI ConferenceTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings
of the International Conference on Learning Representations (ICLR), 2021.Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter.
Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A
Regularizing tabular neural networks through gradient orthogonalization and specialization. In
Proceedings of the International Conference on Learning Representations (ICLR), 2023.Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii,
a freely accessible critical care database. Scientific data, 3(1):1–9, 2016.Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular datasets. Proceedings of the Annual Conference on Neural Information ProcessingGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Proceedings of
the Annual Conference on Neural Information Processing Systems (NeurIPS), 2017.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Proceedings of the Annual Conference on Neural InformationRoman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. Transfer learning with deep tabularRoman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. Transfer learning with deep tabularDiscovery, PKDD ’99, page 418–423, Berlin, Heidelberg, 1999. Springer-Verlag.Zachary C Lipton and Jacob Steinhardt. Troubling trends in machine learning scholarship:
Some ml papers suffer from flaws that could mislead the public and stymie future research.H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady,
Saint: Improved neural networks for tabular data via row attention and contrastive pre-training.Dennis Ulmer, Lotta Meijerink, and Giovanni Cinà. Trust issues: Uncertainty estimation does not enable reliable ood detection on medical tabular data. In Machine Learning for Health,Christopher J Urban and Kathleen M Gates. Deep learning: A primer for psychologists.Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked scienceAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
vs. NNs’, as well as to provide a large set of tools for researchers and practitioners working on tabular
data. We do not see any negative broader societal impacts of our work. In fact, our work shows that
on a surprisingly large fraction of datasets, it is not necessary to train a resource-intensive neural net: a simple baseline, or tuning CatBoost, is enough to reach top performance. We even predict which datasets are more amenable to GBDTs: larger datasets, datasets with a high size-to-number-offeatures ratio, and ‘irregular’ datasets. Our hope is that our work will have a positive impact for both
practitioners and researchers: by providing the largest analysis and open-source codebase to date,
along with a benchmark suite of ‘hard’ datasets, our work can both accelerate future research andIn this section, we give an overview of the documentation for our dataset. For the full details,We, the authors, bear all responsibility in case of violation of rights. The license of our repository isWe plan to actively maintain the benchmark suite, and we welcome contributions from the community.Our Code of Conduct is from the Contributor Covenant, version 2.0. See“We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity
and expression, level of experience, education, socio-economic status, nationality,
personal appearance, race, caste, color, religion, or sexual identity and orientation.”*Why was the datasheet created? (e.g., was there a specific task in mind? was there a specific gap that
needed to be filled?) The goal of releasing the TabZilla Benchmark Suite is to accelerate research in tabular data by introducing a set of ‘hard’ datasets. Specifically, simple baselines cannot reach top performance, and most algorithms (out of the 19 we tried) cannot reach top performance. We found*Has the dataset been used already? If so, where are the results so others can compare (e.g., links to
published papers)? All of the individual datasets are already released in OpenML, and many have been used in prior work on tabular data. However, our work gathers these datasets into a single ‘hard’*What (other) tasks could the dataset be used for? All of these datasets are tabular classification
datasets, and so to the best of our knowledge, they cannot be used for anything other than tabular*Who funded the creation dataset? This benchmark suite was created by researchers at Abacus.AI,
Stanford, Pinterest, University of Maryland, IIT Bombay, New York University, and Caltech. Funding*What are the instances?(that is, examples; e.g., documents, images, people, countries) Are there
multiple types of instances? (e.g., movies, users, ratings; people, interactions between them; nodes,
edges) Each instance is a tabular datapoint. The makeup of each point depends on its dataset. For example, three of the datasets consist of poker hands, electricity usage, and plant textures.*How many instances are there in total (of each type, if appropriate)? See Table 4 for a breakdown of*What data does each instance consist of ? “Raw” data (e.g., unprocessed text or images)? Features/attributes? Is there a label/target associated with instances? If the instances related to people,
are subpopulations identified (e.g., by age, gender, etc.) and what is their distribution? The raw data is hosted on OpenML. In our repository, we also contain scripts for the standard preprocessing we
ran before training tabular data models. The data are not related to people.*Is any information missing from individual instances? If so, please provide a description, explaining
why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. There is no missing information.*Are relationships between individual instances made explicit (e.g., users’ movie ratings, social
network links)? If so, please describe how these relationships are made explicit. There are no*Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was
validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a
more diverse range of instances, because instances were withheld or unavailable).We selected the datasets for our benchmark suite as follows. We started with 176 datasets, which study tabular data , including datasets from the OpenML-CC18 suite , the OpenML Benchmarking Suite , and additional OpenML datasets . Due to the scale of our experiments
(538 650 total models trained), we limited to datasets smaller than 1.1M. CC-18 and OpenML Benchmarking Suite are both seen as the go-to standards for conducting a fair, diverse evaluation
across algorithms due to their rigorous selection criteria and wide diversity of datasets . Out of
these 176 datasets, we selected 36 datasets for our suite as described in Section 3.*Are there recommended data splits (e.g., training, development/validation, testing)? If so, please
provide a description of these splits, explaining the rationale behind them.We use the 10 folds from OpenML, and it is recommended to report performance averaged over these
10 folds, as we do and as OpenML does. If a validation set is required, we recommend additionally*Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a
description. There are no known errors, sources of noise, or redundancies.*Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites,
tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset
(i.e., including the external resources as they existed at the time the dataset was created); c) are there
any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated
with them, as well as links or other access points, as appropriate. The dataset is self-contained.*What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor,
manual human curation, software program, software API)? How were these mechanisms or procedures validated? We did not create the individual datasets. However, we selected the datasets for our benchmark suite as follows. We started with 176 datasets, which we selected with the aim to include most classification datasets from popular recent papers that study tabular data ,
including datasets from the OpenML-CC18 suite , the OpenML Benchmarking Suite , and
additional OpenML datasets . Due to the scale of our experiments (538 650 total models trained),
we limited to datasets smaller than 1.1M. CC-18 and OpenML Benchmarking Suite are both seen as the go-to standards for conducting a fair, diverse evaluation across algorithms due to their rigorousOut of these 176 datasets, we selected 36 datasets for our suite as described in Section 3.*How was the data associated with each instance acquired? Was the data directly observable (e.g.,
raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was
reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If
so, please describe how. The datasets were selected using the three criteria from Section 3.*If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,
probabilistic with specific sampling probabilities)? As described earlier, the datasets were selected*Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The creation of the TabZilla*Over what timeframe was the data collected? Does this timeframe match the creation timeframe
of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. The timeframe
for constructing the TabZilla Benchmark Suite was from April 15, 2023 to June 1, 2023.*Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing
values)? If so, please provide a description. If not, you may skip the remainder of the questions inWe include both the raw data and the preprocessed data. We preprocessed the data by imputing each NaN to the mean of the respective feature. We left all other preprocessing (such as scaling) to the*Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support
unanticipated future uses)? If so, please provide a link or other access point to the “raw” data.*Is the software used to preprocess/clean/label the instances available? If so, please provide a link or*Does this dataset collection/processing procedure achieve the motivation for creating the dataset
stated in the first section of this datasheet? If not, what are the limitations? We hope that the release of this benchmark suite will achieve our goal of accelerating research in tabular data, as well as
making it easier for researchers and practitioners to devise and compare algorithms. Time will tell*How will the dataset be distributed? (e.g., tarball on website, API, GitHub; does the data have a*When will the dataset be released/first distributed? What license (if any) is it distributed under? The
data community. If new algorithms are created, the authors may open a pull request to include their*How will updates be communicated? (e.g., mailing list, GitHub) Updates will be communicated on*If the dataset becomes obsolete how will this be communicated? If the dataset becomes obsolete, it*If others want to extend/augment/build on this dataset, is there a mechanism for them to do so? If so,
is there a process for tracking/assessing the quality of those contributions. What is the process for communicating/distributing these contributions to users? Others can create a pull request on GitHub
with possible extensions to our benchmark suite, which will be approved case-by-case. For example,
an author of a new hard tabular dataset may create a PR in our codebase with the new dataset. These*Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. There was no ethical review process. We note that our
benchmark suite consists of existing datasets that are already publicly available on OpenML.*Does the dataset contain data that might be considered confidential (e.g., data that is protected
by legal privilege or by doctorpatient confidentiality, data that includes the content of individuals non-public communications)? If so, please provide a description. The datasets do not contain any*Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,
or might otherwise cause anxiety? If so, please describe why None of the data might be offensive,*Does the dataset relate to people? If not, you may skip the remaining questions in this section. The**Gradient-boosted decision trees.** GBDTs iteratively build an ensemble of decision trees, with each
new tree fitting the residual of the loss from the previous trees, using gradient descent to minimize the losses. GBDTs have been a powerful tool for modeling tabular data ever since their creation
in 2001 , and numerous works propose high-performing GBDT variants. XGBoost (eXtreme Gradient Boosting) uses weighted quantile sketching and sparsity-awareness, allowing it to scale to large datasets. LightGBM (Light Gradient Boosting Machine) uses gradient-based one-sided sampling and exclusive feature bundling to create a faster and more lightweight GBDT
implementation. CatBoost (Categorical Boosting) introduces ordered boosting, a new method
for handling categorical features, as well as better methods for handling missing values and outliers.**Neural networks for tabular data.** Borisov et al. described three types of tabular data approaches
for neural networks . Data transformation methods seek to encode the data into a format
that is better-suited for neural nets. Architecture-based methods design specialized neural architectures for tabular data , a large sub-class of which are transformer-based architectures. Regularization-based methods specially tailor regularizers methods to improve the
performance of a given architecture . Notably, one recent work designs a new framework
for regularization in the tabular setting built on latent unit attributions , and another recent work shows that searching for the optimal combination/cocktail of 13 regularization techniques applied
to a simple neural net achieves strong performance. While regularization was not the focus of our current work, including regularization methods would be an exciting direction for follow-up work.**GBDTs versus NNs.** Several recent works compare GBDTs to NNs on tabular data, often finding
Armon compare GBDTs and NNs on 30 datasets, finding that GBDTs perform better on average,
and ensembling both achieves better performance . Kadra et al. compare GBDTs and NNs on 40
datasets, finding that properly-tuned neural networks perform best on average .Gorishniy et al. introduce a ResNet-like architecture, and FT-Transformer, a transformerbased architecture. Across experiments on eleven datasets, they conclude that there is still no
universal winner among GBDTs and NNs. Borisov et al. compare classical machine learning methods with eleven deep learning approaches on five tabular datasets, concluding that GBDTs still
have the edge. In the transfer learning setting, Levin et al. find that neural networks have aPerhaps the most related work to ours is by Grinsztajn et al. , who investigate why tree-based methods outperform neural nets on tabular data. There are a few differences between their work and
ours. First, they only consider seven algorithms and 45 datasets, compared to our 19 algorithms and
176 datasets. Second, their dataset sizes range from 3 000 to 10 000, or seven that are exactly 50 000,
in contrast to our dataset sizes which range from 32 to 1 025 009 . Additionally, theynumber of instances, number of features, number of target classes, and the ratio of the mimum frerquency of any class to the maximum frequency of any class (Min-Max Class Freq.). Right
columns show the number of feature types. All statistics except for class frequency ratio are rounded# Inst. # Feats. # Classes Min-Max Class Freq. # Feature Typesfurther control their study, for example by upper bounding the ratio of size to features, by removing high-cardinality categorical feature, and by removing low-cardinality numerical features. While this
has the benefit of being a more controlled study, their analysis misses out on some of our observations,
such as GBDTs performing better than NNs on ‘irregular’ datasets. Finally, while Grinsztajn et al.
features, our work considers orders of magnitude more metafeatures. Again, while each approach has its own strengths, our work is able to discover more potential insights, correlations, and takeawaysWe give additional experiments, including dataset statistics (Appendix D.1), additional results from
Section 2. (Appendix D.2), and additional results from Section 2. (Appendix D.3).Table 6 shows summary statistics for all 176 datasets used in our experiments. Roughly half of our datasets have a binary classification target (and as many as 100 target classes), and roughly half of all training sets have fewer than 2300 instances—though many datasets have tens of thousands ofIn this section, we give additional experiments from Section 2.1, including relative performanceFirst, we show the ranking of all tuned algorithms according to each performance metric, averaged
over datasets: log loss , F1 score , and ROC-AUC . These are similar to
Table 1, but with different metrics. Rankings are calculated by first averaging tuned performance over all 10 splits of each dataset, and then ranking each algorithm for each dataset according to their
average performance. For these rankings, a tie between two algorithms is indicated by the lowest
(best) ranking. So if multiple algorithms achieve the highest average accuracy for a dataset, they bothSome algorithms perform well even without hyperparameter tuning. Next, we calculate the ranking of all datasets using the same procedure as in Table 1 and the previous tables, but with their default
hyperparameter set displayed as a separate algorithm. We show ranking results for test accuracy. See
nearly the same as in Table 1. We also include partial results for NAM , DeepFM , andIn this section we analyze the relative training time required by each algorithm. Here we only consider algorithms with their default hyperparameters, so no tuning is used. Table 12 shows a ranking of
all algorithms, according to the average training time per 1 000 training samples; as before, we separately present results for datasets of size less than or equal to 1250 in Table 13. These rankingsdatasets, and then ranking each algorithm for each dataset according to this average train time.In Figure 4, we plotted the performance improvement of hyperparameter tuning on CatBoost,
using default hyperparameters. Now, we also give the same plot for ResNet. See Figure 6.Figure 7 shows a critical difference plot according to F1 score. Note that we repeat the Friedman test four times with rankings of the same datasets and algorithms. However it is unlikely that our findings
would change, given that p-values for these tests without correction are extremely small (p < 10 [−] ).We compare the performance of each algorithm family (GBDTs, NNs, and baselines). Here we use all 176 datasets. We use the same methodology here as in previous sections. See Figure 8 (log loss)Recall from Section 2 that for our Venn diagram plots, we split the 19 algorithms into three families :
GBDTs (CatBoost, XGBoost, LightGBM), NNs (listed in Section 2), and baselines (Decision Tree,
KNN, LinearModel, RandomForest, SVM). To compare algorithm performance across datasets, we(left) and ResNet (right) compared to the absolute performance difference between the best neural net and the best GBDT, using default hyperparameters (vertical axis). Each point indicates a
different dataset, and all values are in normalized log loss. Points on the dotted line indicate thatalgorithm’s average rank is shown as a horizontal line on the axis. Algorithms which are notalgorithm’s average rank is shown as a horizontal line on the axis. Algorithms which are notaverage normalized accuracy (Mean Acc.), the standard deviation of normalized accuracy across folds (Std. Acc.), and the train time per 1000 instances. Min/max/mean/median are taken over allFTTransformer (default) 1 35 18. 20. 0. 0. 0. 0. 25. 15.use min-max scaling as described in Section 2. In Figure 4, we said that an algorithm is ‘highperforming’ if it achieves a scaled test-set accuracy of at least 0 . 99, and then we determine which
algorithm families (GBDTs, NNs, baselines) have a high-performing algorithm. Now we compute the same Venn diagram, when tightening the definition of high-performing to 0 . 9999 scaled accuracy.
See Figure 10. In this case, GBDTs are the sole high-performing algorithm family for 42% of datasets,
while NNs are the sole high-performing algorithm family for 30% of datasets. However, since these
differences are smaller than 0.1%, they may not be significant to practitioners.In this section, we investigate the association between dataset size and performance. In Section 2. we show that, when compared to NNs and baselines, GBDTs perform relatively better with larger datasets;
this is based on a negative correlations between normalized log loss and dataset size. However, it is more informative to compare performance of individual algorithms rather than algorithm families.
For example, Figure 11 compares the rank of three algorithms: CatBoost, SAINT, and TabNet,
with dataset size. In the left panel (CatBoost minus TabNet), CatBoost outperforms TabNet for all datasets up to size roughly 1 500. For larger datasets there is little difference between CatBoost andthe rank over all datasets, the average normalized accuracy (Mean Acc.), and the standard deviation of normalized accuracy across folds (Std. Acc.). Min/max/mean/median are taken over all datasets.
The rightmost column show the number of datasets where each algorithm ran successfully, out ofover all 176 datasets. An algorithm is high-performing if its test accuracy after 0-1 scaling is above a
certain threshold. While Figure 4 used a threshold 0.99, this figure uses threshold 0.9999.samples. Lower ranks indicate lower training times. Rank columns show min, max, and mean ranks over all datasets. Right columns show average training time per 1 000 samples over all 10 trainingTabNet; this indicates that CatBoost should be chosen over TabNet for smaller datasets, and that both algorithms are comparable for larger datasets On the other hand, the center panel (CatBoost minus
SAINT), indicates that both CatBoost and SAINT have comparable performance for all datasets
up to those with size 1 500; for larger datasets, CatBoost outperforms SAINT. This indicates that CatBoost should be chosen over SAINT for very large datasets, but for small datasets both algorithms**The main takeaway** from these findings is that practitioners should not focus on choosing an
algorithm family, such as NNs or GBDTs, to focus on. For example, TabPFN and TabNet are both neural nets, but TabPFN does comparatively better on smaller datasets, while TabNet does
comparatively better on larger datasets. Rather they can consult our metadataset of results to decide which algorithm is appropriate for their specific use case. General trends are helpful, but not sufficient,Recall from Section 2. that ∆ ℓℓ denotes the difference in normalized log loss between the best neural net and the best GBDT method. Table 14 and Figure 12 shows the dataset properties with theTo evaluate the predictive power of dataset properties, we train several decision tree models using the train/test procedure above, with a binary outcome: 1 if ∆ ℓℓ> 0 (the best neural net beats the best
GBDT), and 0 otherwise. Table 15 shows the performance accuracy of decision trees trained on this
task, with varying depth levels; we also include an XGBoost model for comparison. Finally, we
include a visual depiction of a simple depth-3 decision tree, in Figure 13. The decision tree classifies which of the top five algorithms performs the best. Note that the decision splits are based purely onSAINT, and TabNet, plotted with dataset size. Points above the dotted line indicate that the second
algorithm has lower log loss, meaning better performance than the first.properties, for all 10 splits of all 176 datasets. Larger correlations mean that a larger value of the property corresponds to larger log loss (worse performance) for the best neural net compared to theFN 189 1 0 0 et 111 28 60 1 NT 131 1 54 5 st 142 0 51 st 115 2 79 101 102 103 101 102 103 101 102 103 101: 101 102 103 st st NT et FN 115 142 131 111 189 101 102 103 28 2 0 1 101 102 103 79 51 54 60 101 1
0 0 0 3 32 24 1 3 10 5 81 32 69 0 27 293 67 13: 102 103 101 102 103 101 102 103 101 1
0 11 6 2 0 7: 2 103 101 102 1datasets. The decision tree chooses among the five best-performing algorithms from our experiments:
ResNet, SAINT, TabPFN, CatBoost, and XGBoost. Note that the decision splits are based purely on maximizing information gain at that point in the tree, across 176 datasets. IQR denotes interquartileaccording to average training time per 1 000 samples. Lower ranks indicate lower training times.
Rank columns show min, max, and mean ranks over all datasets. Right columns show average
training time per 1 000 samples over all 10 training folds, and the number of datasets considered forlog loss between the best NN and GBDT (∆ ℓℓ ), over all all 176 datasets. Higher correlation values indicate that larger values of the metafeature are associated with worse NN performance and strongerLog of the median canonical correlation between each feature and the target. -0.Finally, we present the metafeatures most-correlated with the difference in log loss between pairs of algorithms. We consider the two best-performing algorithms from each family: CatBoost, XGBoost,
ResNet, and SAINT. See Table 19 and Table 20 for statistical and general metafeatures, respectively.While our experiments focus on classification datasets, the TabZilla codebase is also equipped to handle regression datasets—which have a continuous target variable rather than categorical or
binary. We run experiments using 12 algorithms with 17 tabular regression datasets, using the same experiment design and parameters described in Section 2. Each algorithm is tuned for each dataset
by maximizing the R-squared (R2) metric. The regression datasets used in these experiments have
been used in recent studies of machine learning with tabular data ; each dataset
corresponds to an OpenML task, and can be preprocessed exactly like the classification datasets
used in other experiments. The datasets used in these experiments are “Bank-Note-AuthenticationUCI” (OpenML task 361002), “EgyptianSkulls” (5040), “Wine” (190420), “Wisconsin-breastcancer-cytology-features” (361003), “bodyfat” (5514), “california” (361089), “chscase-foot” (5012),outperform the best GBDT model on a tabular dataset. Results are aggregated over 176 train/test splits with one dataset family held out for testing in each split. The number of dataset properties used
by any model of each model type is listed in the leftmost column. Top rows include decision trees
(DT ≤ n ) with maximum depth n ; the bottom row is an XGBoost model for comparison.rank over all datasets, the average normalized R2 (Mean R2), and the std. dev. of normalized R2
across folds (Std. R2). Min/max/mean/median of these quantities are taken over all datasets.“cleveland” (2285), “colleges” (359942), “cpu-small” (4883), “dataset-sales” (190418), “kin8nm”
(2280), “liver-disorders” (52948), “meta” (4729), “mv” (4774), “pbc” (4850), and “veteran” (4828).Table 16 shows the rankings of 12 algorithms on these 17 regression datasets, according to the R2
metric calculated on the test set. The general conclusions are similar to our findings with classification datasets: most algorithms perform well and poorly on at least one dataset; however, GBDTs performIn this section, we discuss dataset preprocessing. Different papers use a variety of different preprocessing methods, and there is also a wide range in the amount of ‘built-in’ preprocessing techniques
inside the algorithms themselves. Therefore, our main results minimize confounding factors by having a consistent, lightweight preprocessing (imputing NaN values). In this section, we compare 13
algorithms on the tabzilla benchmark suite with and without quantile scaling, one of the most popular techniques, for all continuous features. We use QuantileTransformer from scikit-learn . We use
the same computational setup and experiment design as in our main experiments. See Table 17. We find that quantile scaling improves the simple algorithms: decision tree, MLP, random forest, andwith and without applying quantile scaling to each continuous feature. Algorithms with the suffix
“-QSCALE” use quantile scaling, while those without this suffix use the raw continuous features.
Algorithms are ranked according to normalized log loss, and columns show the rank, normalized logIn our main experiments, for hyperparameter optimization (HPO), we ran 30 iterations of random search for all algorithms. In this section, we test the impact of additional HPO for four algorithms:
XGBoost, CatBoost, LightGBM, and RandomForest. We did not run additional HPO experiments on
any neural net methods due to the substantial compute resources required.For each algorithm, we run 100 iterations of HPO using the default Optuna algorithm (treestructured Parzen Estimator), optimizing log loss. We run these HPO experiments on all 36 datasets
in the TabZilla benchmark suite. All hyperparameter ranges can be viewed in our repository in theTable 18 shows the performance of these HPO experiments (algorithm suffix “(HPO)”), compared with with the performance of the default hyperparameters (suffix “(default)”), and the performance
after 30 iterations of random hyperparameter search as in our main results (no suffix). As expected,
additional hyperparameter tuning improves the performance of XGBoost, CatBoost, LightGBM, and**D.7** **Forward Feature Selection for Identifying Important Dataset Attributes**In this section, we present a different method for determining which dataset attributes are related to performance differences between algorithms. Here we use greedy forward feature selection to
identify important dataset attributes. In these experiments, we study the problem of predicting the difference in normalized log loss between CatBoost and ResNet (two very effective GBDT and NNincluding the performance after 100 iterations of HPO (algorithms with suffix “(HPO)”), and the default hyperparameters (algorithms with suffix “(default)”). Algorithm names without any suffix
indicate performance after 30 iterations of random hyperparameter search, as in our main results.
Algorithms are ranked according to normalized log loss, and columns show the rank, normalized logAt a high level, greedy forward feature selection selects metafeatures sequentially which improve the performance of the meta-model. To evaluate performance we use leave-one-dataset-out cross
validation: each dataset contributes 10 folds to the overall metadataset, so each fold includes 10
In these experiments, we first use an XGB regressor fit on the entire metadataset to select 200 features for consideration. Then we use greedy forward selection, implemented in the python package mlxtend1. Number of features normally-distributed, according to the Shapiro-Wilk test.2. Median value of the minimum of all features.3. Median value of the sparsity of all features.4. Interquartile range of the mean value of all features.5. Mean of the harmonic mean of all features.between pairs of algorithms (the loss of Alg. minus Alg. 2). Correlations are taken over all 10
splits of all 133 datasets in which CatBoost, XGBoost, ResNet, and SAINT ran successfully. The 10
dataset attributes with the largest absolute correlation are listed for each pair of algorithms. AttributeCatBoost ResNet -0. Log of the standard deviation of the kurtosis of all features.
CatBoost ResNet -0. Log of the standard deviation of the skewness of all features.
CatBoost ResNet 0. Log of the median of the absolute value of the covariance between all feature
CatBoost ResNet 0. Log of the median of the standard deviation of all features.
CatBoost ResNet 0. Log of the median of the maximum value of all features.
CatBoost ResNet 0. Best performance of a naive Bayes classifier trained over 10-fold CV.CatBoost SAINT 0. Best performance over all 10 folds, of 10-fold CV of a single-node decision CatBoost SAINT 0. Average performance over 10-fold CV of a single-node decision tree fit using
CatBoost SAINT 0. Median performance over 10 folds, for 10-fold CV of a single-node decision CatBoost SAINT -0. Log of the worst performance over 10-fold CV of a single-node decision tree
CatBoost SAINT -0. Log of the kurtosis of the performance of a single-node decision tree fit using CatBoost SAINT -0. Log of the best performance of elite-nearest-neighbor over 10-fold CV.XGBoost ResNet -0. Log of the standard deviation of the kurtosis of all features.
XGBoost ResNet -0. Log of the standard deviation of the skewness of all features XGBoost ResNet 0. Log of the median value of the absolute covariance between all pairs of
XGBoost ResNet 0. Log of the median standard deviation of all features.
XGBoost ResNet 0. Best performance of a naive Bayes classifier over 10-fold CV.
XGBoost ResNet 0. Log of the best performance of a naive Bayes classifier over 10-fold CV.XGBoost SAINT -0. Noisiness of the features: ( i [S][i][ −] i [MI] [(] [i, y] [))] [/] i [MI] [(] [i, y] [)][, where] [ S][i]is the entropy of feature i, and MI ( i, y ) is the mutual information between XGBoost SAINT -0. Log of the standard deviation of the kurtosis of all features.
XGBoost SAINT -0. Log of the standard deviation of the skewness of all features.
XGBoost SAINT -0. Best performance over 10-fold CV of a single-node decision tree fit using XGBoost SAINT -0. Log of the standard deviation of the absolute correlation between all pairs of XGBoost SAINT 0. Log of the range of the performance of a decision tree trained on a randombetween pairs of algorithms (the loss of Alg. minus Alg. 2). Correlations are taken over all 10
splits of all 133 datasets in which CatBoost, XGBoost, ResNet, and SAINT ran successfully. The 10
dataset attributes with the largest absolute correlation are listed for each pair of algorithms. AttributeCatBoost ResNet 0. Log of the ratio of number of features to dataset size.
CatBoost ResNet -0. Log of the ratio of dataset size to number of features.
CatBoost ResNet -0. Range of the relative frequency of each target class.
CatBoost ResNet -0. Interquartile range of the relative frequency of all target classes.CatBoost SAINT -0. Standard deviation of the relative frequency of all target classes.
CatBoost SAINT 0. Kurtosis of the relative frequency of all target classes.
CatBoost SAINT -0. Log of the median relative frequency of all target classes.
CatBoost SAINT 0. Skewness of the relative frequency of target classes.XGBoost ResNet -0. Log of the ratio of dataset size to number of features.
XGBoost ResNet 0. Log of the ratio of number of features to dataset size.
XGBoost ResNet 0. Log of the minimum relative frequency of all target classes.XGBoost SAINT -0. Standard deviation of the relative frequency of all target classes.
XGBoost SAINT -0. Interquartile range of the relative frequency of all target classes.
XGBoost SAINT 0. Log of the ratio of dataset size to number of features.
XGBoost SAINT -0. Log of the ratio of number of features to dataset size.
XGBoost SAINT -0. Range of the relative frequency of all target classes.
XGBoost SAINT -0. Mean of the relative frequency of all target classes.
XGBoost SAINT -0. Median of the relative frequency of all target classes.