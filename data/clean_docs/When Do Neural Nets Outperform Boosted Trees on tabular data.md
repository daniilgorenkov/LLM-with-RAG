Abstract**## **1
Introduction**Tabular datasets are data organized into rows and columns, consisting of distinct features that are typically continuous, categorical, or ordinal. They are the oldest and among the most ubiquitous dataset types in machine learning in practice , due to their numerous applications across medicine , finance , online advertising , and many other areas .Despite recent advances in neural network (NN) architectures for tabular data , there is still an active debate over whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with multiple works arguing either for  or against  NNs. This is in stark contrast to other areas such as computer vision and natural language understanding, in which NNs have far outpaced competing methods .Nearly all prior studies of tabular data use fewer than 50 datasets or do not properly tune baselines , putting the generalizability of these findings into question. Furthermore, the bottom line of many prior works is to answer the question, ‘which performs better, NNs or GBDTs, in terms of the average rank across datasets’ without searching for more fine-grained insights.“NNs vs. GBDTs” Metafeatures<br>Algorithms Datasets TabZilla Suite<br>NNs GBDTs<br>Baselines<br>36 ‘Hardest’ Datasets<br>No family dominates<br>No datasets s.t.<br>19

Datasets Size baselines win<br>Neural nets, OpenML Suite,<br>GBDTs, CC-18, GBDTs handle No datasets s.t.<br>baselines addtl. OpenML irregularity better many methods win<br>ΔAlgo. selection Neural nets Large variety<br>500K total evaluations HPO is just as important as handle small 10 folds per dataset<br>choosing NNs/GBDTs datasets better<br>Irregularity<br>ΔHPO<br>**----- End of picture text -----**<br>In this work, we take a completely different approach by focusing on the following points. First, we question the importance of the ‘NN vs. GBDT’ debate, by investigating the significance of algorithm selection. Second, we analyze what properties of a dataset make NNs or GBDTs better-suited to perform well. We take a data-driven approach to answering these questions, conducting the largest tabular data analysis to date, by comparing **19 algorithms each with up to 30 hyperparameter settings, across 176 datasets** , including datasets from the OpenML-CC18 suite  and the OpenML Benchmarking Suite . To assess performance differences across datasets, we consider dozens of metafeatures. We use 10 folds for each dataset to further reduce the uncertainty of our results.We find that for a surprisingly high fraction of datasets, either a simple baseline (such as a decision tree or KNN) performs on par with the top algorithms; furthermore, for roughly one-third of all datasets, light hyperparameter tuning on CatBoost or ResNet increases performance more than choosing among GBDTs and NNs. These results show that for many tabular datasets, it is not necessary to try out many different NNs and GBDTs: **in many cases, a strong baseline or a well-tuned GBDT will suffice** . While NNs are the best approach for a non-negligible fraction of the datasets in this study, we do find that GBDTs outperform NNs on average over all datastes.Next, we run analyses to discover what properties of datasets explain which methods, or families of methods, do or do not succeed. We compute the correlations of various metafeatures with algorithm performance, and we demonstrate that these correlations are predictive. Our main findings are as follows (also see Figure 5): **dataset** **regularity** **is predictive of NNs outperforming GBDTs** (for example, feature distributions that are less skewed and less heavy-tailed). Furthermore, GBDTs tend to perform better on larger datasets.Finally, with the goal of accelerating tabular data research, we release the **TabZilla Benchmark Suite** : a collection of the ‘hardest’ of the 176 datasets we studied. We select datasets on which a simple baseline does not win, as well as datasets such that most algorithms do not reach top performance.- We conduct the largest analysis of tabular data to date, comparing 19 methods on 176 datasets, with more than half a million models trained. We show that for a surprisingly high fraction of datasets, either a simple baseline performs the best, or light hyperparameter tuning of a GBDT is more important than choosing among NNs and GBDTs, suggesting that the ‘NN vs. GBDT’ debate is overemphasized.- After analyzing dozens of metafeatures, we present a number of insights into the properties that make a dataset better-suited for GBDTs or NNs.- We release the TabZilla Suite: a collection of 36 ‘hard’ datasets, with the goal of accelerating tabular data research. We open-source our benchmark suite, codebase, and raw results.**Related work.** Tabular datasets are the oldest and among the most common dataset types in machine learning in practice , due to their wide variety of applications . GBDTs  iteratively build an ensemble of decision trees, with each new tree fitting the residual of the loss from the previous trees, using gradient descent to minimize the losses. XGBoost , LightGBM , and Catboost  are three widely-used, high-performing variants. Borisov et al. described three types of tabular data approaches for neural networks : data transformation methods , architecture-based methods  (including transformers ), and regularization-based methods . Several recent works compare GBDTs to NNs on tabular data, often finding that either NNs  or GBDTs  perform best.Perhaps the most related work to ours is by Grinsztajn et al. , who investigate why tree-based methods outperform neural nets on tabular data. There are a few differences between their work and ours. First, they consider seven algorithms and 45 datasets, compared to our 19 algorithms and 176 datasets. Second, their dataset sizes range from 3 000 to 10 000, or seven that are exactly 50 000, in contrast to our dataset sizes which range from 32 to 1 025 009 . Additionally, they further control their study, for example by upper bounding the ratio of size to features, by removing high-cardinality categorical feature, and by removing low-cardinality numerical features. While this has the benefit of being a more controlled study, their analysis misses out on some of our observations, such as GBDTs performing better than NNs on ‘irregular’ datasets. Finally, while Grinsztajn et al. focused in depth on a few metafeatures such as dataset smoothness and number of uninformative features, our work considers orders of magnitude more metafeatures. Again, while each approach has its own strengths, our work is able to discover more potential insights, correlations, and takeaways for practitioners. To the best of our knowledge, the only related work has considered more than 50 datasets is TabPFN , which considered 179 datasets which are size 2 000 or smaller. See Appendix C for a longer discussion of related work.## **2
Analysis of Algorithms for Tabular Data**In this section, we present a large-scale study of techniques for tabular data across a wide variety of datasets. Our analysis seeks to answer the following two questions.1. How do algorithms (and algorithm families) compare across a wide variety of datasets?2. What properties of a dataset are associated with algorithms (and families) outperforming others?**Algorithms and datasets implemented.** We present results for 19 algorithms, including popular recent techniques and baselines. The methods include three GBDTs: CatBoost , LightGBM , and XGBoost ; 11 neural networks: DANet , FT-Transformer , two MLPs , NODE , ResNet , SAINT , STG , TabNet , TabPFN , and VIME ; and five baselines: Decision Tree , KNN , Logistic Regression , Random Forest , and SVM . We choose these algorithms because of their popularity, diversity, and strong performance.We run the algorithms on 176 classification datasets from OpenML . Our aim is to include most classification datasets from popular recent papers that study tabular data , including datasets from the OpenML-CC18 suite , the OpenML Benchmarking Suite , and additional OpenML datasets. Due to the scale of our experiments (538 650 total models trained), we limit the run-time for each experiment (described below), which precluded the use of datasets of size larger than 1.1M. Table 6 shows summary statistics for all datasets. CC-18 and OpenML Benchmarking Suite are both seen as the go-to standards for conducting a fair, diverse evaluation across algorithms due to their rigorous selection criteria and wide diversity of datasets . To the best of our knowledge, our 19 algorithms and 176 datasets are the largest number of either algorithms or datasets (with the exception of TabPFN ) considered by recent tabular dataset literature, and the largest number available in a single open-source repository.**Metafeatures.** We extract metafeatures using the Python library PyMFE , which contains 965 metafeatures. The categories of metafeatures include: ‘general’ (such as number of datapoints, classes, or numeric/categorical features), ‘statistical’ (such as the min, mean, or max skewness, or kurtosis, of all feature distributions), ‘information theoretic’ (such as the Shannon entropy of the target), ‘landmarking’ (the performance of a baseline such as 1-Nearest Neighbor on a subsample of the dataset), and ‘model-based’ (summary statistics for some model fit on the data, such as number of leaf nodes in a decision tree model). Since some of these features have long-tailed distributions, we also include the log of each strictly-positive metafeature in our analysis.**Experimental design.** For each dataset, we use the ten train/test folds provided by OpenML, which allows our results on the test folds to be compared with other works that used the same OpenML datasets. Since we also need validation splits in order to run hyperparameter tuning, we divide each training fold into a training and validation set. For each algorithm, and for each dataset split, we run the algorithm for up to 10 hours. During this time, we train and evaluate the algorithm with at most 30 hyperparameter sets (one default set and 29 random sets, using Optuna ). Each parameterized algorithm is given at most two hours on a 32GiB V100 to complete a single train/evaluation cycle. In line with prior work, our main metric of interest is accuracy , and we report the test performance of the hyperparameter setting that had the maximum performance on the validation set. We also consider log loss, which is highly correlated with accuracy but contains significantly fewer ties. We also include results for F1-score and ROC AUC in Appendix D. Similar to prior work , whenever we average across datasets, we use the average distance to the minimum (ADTM) metric, which consists of 0-1 scaling (after selecting the best hyperparameters, which helps protect against outliers ). Finally, in order to see the variance of each method on different folds of the same dataset, we report the average (scaled) standard deviation of each method across all 10 folds.## **2.  Relative Algorithm Performance**In this section, we answer the question, “How do individual algorithms, and families of algorithms, perform across a wide variety of datasets?” We especially consider whether the difference between GBDTs and NNs is significant.**No individual algorithm dominates.** We start by comparing the average rank of all algorithms across all datasets, while excluding datasets which ran into memory or timeout issues on a nontrivial number of algorithms using the experimental setup described above. Therefore, we consider a setof 104 datasets (and we include results on all 176 datasets in the next section and in Appendix D.2). As mentioned in the previous section, for each algorithm and dataset split, we report the test set performance after tuning on the validation set; see Table 1.Surprisingly, nearly every algorithm ranks first on at least one dataset and last on at least one other dataset . As expected, baseline methods tend to perform poorly while neural nets and GBDTs tend to perform better on average. The fact that the best out of all algorithms, CatBoost, only achieved an average rank of 5.06, shows that there is not a single approach that dominates across most datasets. In Table 2, we compute the same table for the 57 datasets with size at most 1250 (training set at most 1000), so that we can include TabPFN  in our rankings. **We find that TabPFN achieves the best average performance of all algorithms, while also having the fastest training time.** However, with an average rank of 4.88, it still does not dominate all other approaches across different datasets. Furthermore, the inference time for TabPFN is higher than other algorithms.**Performance vs. runtime.** In Figure 2, we plot the accuracy vs. runtime for all algorithms, averaged across all datasets. Overall, neural nets require the longest runtime, and often outperform baseline methods. On the other hand, GBDTs simultaneously require little runtime while also achieving strong performance: they consistently outperform baseline methods, and consistently require less runtime than neural nets. There is one caveat: our experiments train each neural net for a pre-determined number of epochs (100 in most cases), with early stopping if there is no improvement for 20 epochs. It is possible that these neural nets can achieve1.  XGBoost CatBoost FTTransformer SAINT<br>0.  DecisionTree<br>0.  SVM Res Net<br>0.  RandomForest<br>VIME<br>0.  KNN LinearModel<br>0.  neural baseline gbdt<br>10 2 10 1 100 101 102<br>Training time per 1000 instances (sec)<br>Acaruccy<br>d ilNeamroz<br>**----- End of picture text -----**<br>strong performance with less runtime, e.g., with a more aggressive early-stopping criterion.Log Loss<br>18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1<br>VIME CatBoost<br>DecisionTree XGBoost<br>KNN SAINT<br>MLP ResNet<br>NODE LightGBM<br>MLP-rtdl SVM<br>TabNet FTTransformer<br>LinearModel RandomForest<br>STG DANet<br>**----- End of picture text -----**<br>High - Performing Algorithm Family over 176
Datasets<br>Threshold = 0.  100<br>Neural Nets GBDTs<br>10 1<br>46 15 60 10 2<br>(26%) (9%) (34%)<br>13 10 3<br>(7%)<br>(59%) (58%) 10 4<br>10 5<br>25<br>(14%) 0 10 4 10 2 100<br>Baselines CatBoost tuning improvement<br>|T<br>D<br>B<br>G<br>-<br>N<br>N<br>lt u<br>fae<br>d<br>t<br>s<br>e<br>|B<br>**----- End of picture text -----**<br>**Statistically significant performance differences.** Table 1 shows that many algorithms have similar performance. Next, we determine statistically significant ( p < 0 . 05) performance differences between algorithms across the 104 datasets described above. First, we use a Friedman test to determine whether performance differences between each algorithm are significant ; we can reject the null hypothesis (p < 0 . 05) for this test; the p-value is less than 10 [−] . We then use a Wilcoxon signed-rank test to determine which pairs of algorithms have significant performance differences (p<0 . 05) . With the Wilcoxon tests we use a Holm-Bonferroni correction to account for multiple comparisons . Due to the presence of many ties in the test accuracy metric, we use the test log loss metric. See Figure 3 (and see Figure 7 for the F1 score). In these figures, the average rank of each algorithm is shown on the horizontal axis; if differences between algorithms are not significant (p ≥ 0 . 05), then algorithms are shown connected by a horizontal bar. We find that CatBoost outperforms all other algorithms across 104 datasets.**GBDTs vs. NNs.** Although Table 11 tells us which individual methods perform best on average, now we consider the age-old question, ‘are GBDTs better than NNs for tabular data?’ We split the 19 algorithms into three families : GBDTs (CatBoost, XGBoost, LightGBM), NNs (the 11 listed above), and baselines (Decision Tree, KNN, LinearModel, RandomForest, SVM). We say that analgorithm is ‘high-performing’ if it achieves a 0-1 scaled test accuracy of at least 0 . 99, and then we determine which algorithm families (GBDTs, NNs, baselines) have a high-performing algorithm; see Figure 4. Surprisingly, the three-way Venn diagram is relatively balanced among GBDTs, NNs, and baselines, although GBDTs overall have the edge. In Appendix D.2, we run the same analysis, using a threshold of 0 . 9999. In this case, GBDTs are the sole high-performing algorithm family for most datasets. Since these wins are by less than 0.01%, they may not be significant to practitioners.**Algorithm selection vs. tuning.** Next, we determine whether it is more important to select the best possible algorithm family, or to simply run light hyperparameter tuning on an algorithm that performs well in general, such as CatBoost or ResNet. We consider a scenario in which a practitioner can decide to (a) test several algorithms using their default hyperparameters, or (b) optimize the hyperparameters of a single model, such as CatBoost or ResNet. We compute whether (a) or (b) leads to better performance. Specifically, we measure the performance difference between the bestperforming GBDT and NN using their default hyperparameters, as well as the performance difference between CatBoost with the default hyperparameters vs. CatBoost tuned via 30 iterations of random search on a validation set; see Figure 4 (right), and see Appendix D.  for the same analysis with ResNet. Surprisingly, light hyperparameter tuning yields a greater performance improvement than GBDT-vs-NN selection for about one-third of all datasets. Once again, this suggests that for a large fraction of datasets, it is not necessary to determine whether GBDTs or NNs are better: light tuning on an algorithm such as CatBoost or ResNet can give just as much performance gain. In the next section, we explore why a dataset might be more amenable to a neural net or a GBDT.## **2.  Metafeature Analysis**In this section, we answer the question, “What properties of a dataset are associated with certain techniques, or families of techniques, outperforming others?” We answer this question by computing the correlation of metafeatures with three different quantities related to the performance difference between algorithm families, the performance difference between pairs of algorithms, and the relative performance of individual algorithms.In order to assess the difference in performance between NNs and GBDTs, we calculate the difference in normalized log loss between the best NN and the best GBDT, which we refer to as ∆ ℓℓ . We compute the correlation of ∆ ℓℓ to various metafeatures across all datasets; see Figure 12 and Table 14 in Appendix D.3. Next, in order to determine the individual strengths and weaknesses of each algorithm, we compute the correlation of various metafeatures to the performance of an individual algorithm relative to all other algorithms; see Table 3 and Figure 5. Finally, we compute the correlation of metafeatures to the difference in performance between pairs of the top-performing algorithms from Section 2.1: CatBoost, XGBoost, SAINT, and ResNet. See Figure 5 and Table 19.In order to show that these metafeatures are predictive , we train and evaluate a meta-learning model using a leave-one-out approach: one dataset is held out for testing, while the remaining 175 datasets are used for training, averaged across all 176 possible test sets; see Appendix D.3. In the rest of this section, we state and discuss the main findings of our metafeature analysis.**Neural nets perform comparatively worse on larger datasets.** Throughout our metafeature analyses, we find that GBDTs perform comparatively better than NNs and baselines with larger datasets. Figure 5 shows that XGBoost achieves top performance compared to all 19 algorithms on the seven largest datasets, and GBDTs overall perform well. In Table 3, somewhat surprisingly, dataset size is the most negatively-correlated metafeature with the relative performance of both LightGBM and XGBoost. Finally, Table 14 shows that the GBDT family’s performance is also positively correlated with the ratio of size to the number of features. Notably, all of these analyses are relative to the performance of all algorithms, which includes the newly released TabPFN , a neural net that performs remarkably well on datasets of small size, due to its carefully-designed prior. On the other hand, GBDTs excel when the ratio of dataset size to number of features is high, because all split in the decision trees are computed using more datapoints. Some of our above findings are backed up by prior work, for example, Grinsztajn et al.  showed that increasing the ratio of (uninformative) features to dataset size, hurts the performance of ResNet, and our results indicate the same trend . On the other hand, NNs as a whole see the opposite trend , which at least is shown for FTTransformer in Grinsztajn et al. . We provide additional analyses in Appendix D.2.6. Note that, while the general trend shows GBDTs outperforming NNs on largerCatBoost LightGBM FTTransformer 0.2<br>XGBoost SAINT ResNet<br>0.0<br>107 0.2<br>0.4<br>105 0.6<br>0.8<br>103<br>1.0<br>0 1 2 3 4 5 6 103 105 0 2 4 6<br>Irregularity Num. Instances Irregularity<br>)te<br>NRse<br>secn (GB X -<br>Itasn. Ls sog<br>Nmu ffiLDo.<br>**----- End of picture text -----**<br>CatBoost: of featurei, andMI(i, y)is the mutual information between featureiand the targety.CatBoost: ResNet
Noisiness of the features:( i Si − i MI(i, y))/  i MI(i, y), whereSiis the entropy: Mean canonical correlation between any numeric feature and the target.
[0.25, 0.34]: [-0.28, -0.19]datasets, this does not imply that all GBDTs are better than all NNs on larger datasets. For example, TabPFN and TabNet are both neural nets, yet TabPFN performs particularly well on smaller datasets, and TabNet performs particularly well on larger datasets. **It is important to note that when choosing an algorithm for a new use-case, practitioners should focus on algorithm-specific analyses (such as in Table 1, Table 2, and Appendix D.2.6) rather than general ‘GBDT vs. NN’ trends.****GBDTs favor irregular datasets.** Another trend present throughout all three metafeature analyses is that GBDTs consistently favor ‘irregular’ datasets. When comparing pairs of the top GBDTs and NNs, we find that both CatBoost and XGBoost outperform ResNet and SAINT on datasets whose feature distributions are heavy-tailed, skewed, or have high variance . That is, some datasets’ feature distributions all have a similar amount of skewness, while other datasets’ feature distributions are more irregular, with a high range of skewness. It is the latter type of datasets on which GBDTs outperform NNs. We also find that GBDTs perform better when datasets are more class imbalanced (on SAINT in particular). In Figure 5, GBDTs perform best on the most irregular datasets, computed via a linear combination of five metafeatures each measuring the skewness or kurtosis of the feature distributions.**The bottom line.** Overall, we answer the title question of our paper: GBDTs outperform NNs on datasets that are more ‘irregular’, as well as large datasets, and datasets with a high ratio of size to number of features. When a practitioner is faced with a new dataset, based on all the analysis in Section 2, we give the following recommendation: first try simple baselines, and then conduct light hyperparameter tuning on CatBoost. Surprisingly, this often will already result in strong performance. As a next step, the practitioner can try NNs and other GBDTs that are most-correlated with strong performance based on the dataset’s metafeatures, using analyses such as Table 1 and Appendix D.2.6.Hardness Metrics: credit-g
Hardness Metrics: **0.26**
Hardness Metrics: **0.13**
Dataset: **0.12**
Dataset: 1 000
Attributes: 21
: 1.92

Algs.: ResNetHardness Metrics: jungle-chess
Hardness Metrics: **0.30**
Hardness Metrics: **0.18**
Dataset: **0.17**
Dataset: 44 819
Attributes: 7
: 0.08

Algs.: SAINTHardness Metrics: MiniBooNE
Hardness Metrics: **0.20**
Hardness Metrics: **0.09**
Dataset: 0.00
Dataset: 130 064
Attributes: 51
: 12162.65

Algs.: LightGBMHardness Metrics: albert
Hardness Metrics: **0.42**
Hardness Metrics: **0.28**
Dataset: 0.00
Dataset: 425 240
Attributes: 79
: 1686.90

Algs.: CatBoostHardness Metrics: electricity
Hardness Metrics: **0.46**
Hardness Metrics: **0.38**
Dataset: 0.00
Dataset: 45 312
Attributes: 9
: 2693.51

Algs.: LightGBMHardness Metrics: elevators
Hardness Metrics: **0.36**
Hardness Metrics: **0.08**
Dataset: 0.05
Dataset: 16 599
Attributes: 19
: 2986.50

Algs.: TabNetHardness Metrics: guillermo
Hardness Metrics: **0.35**
Hardness Metrics: **0.60**
Dataset: 0.00
Dataset: 20 000
Attributes: 4 297
: NaN

Algs.: XGBoostHardness Metrics: higgs
Hardness Metrics: **0.41**
Hardness Metrics: **0.10**
Dataset: 0.07
Dataset: 98 050
Attributes: 29
: 15.53

Algs.: ResNetHardness Metrics: nomao
Hardness Metrics: **0.22**
Hardness Metrics: **0.18**
Dataset: 0.00
Dataset: 34 465
Attributes: 119
: 1100.34

Algs.: LightGBMHardness Metrics: 100-plants-texture
Hardness Metrics: **0.20**
Hardness Metrics: **0.11**
Dataset: 0.00
Dataset: 1 599
Attributes: 65
: 17.66

Algs.: CatBoostHardness Metrics: poker-hand
Hardness Metrics: **0.58**
Hardness Metrics: **0.98**
Dataset: 0.00
Dataset: 1 025 009
Attributes: 11
: 0.08

Algs.: XGBoostHardness Metrics: Bioresponse
Hardness Metrics: 0.07
Hardness Metrics: **0.07**
Dataset: 0.00
Dataset: 3 751
Attributes: 1 777
: 328.77

Algs.: LightGBMHardness Metrics: GesturePhase
Hardness Metrics: 0.08
Hardness Metrics: **0.08**
Dataset: 0.00
Dataset: 9 872
Attributes: 33
: 52.18

Algs.: LightGBMHardness Metrics: SpeedDating
Hardness Metrics: 0.18
Hardness Metrics: **0.14**
Dataset: 0.00
Dataset: 8 378
Attributes: 121
: 36.43

Algs.: XGBoostHardness Metrics: ada-agnostic
Hardness Metrics: 0.12
Hardness Metrics: **0.11**
Dataset: 0.00
Dataset: 4 562
Attributes: 49
: NaN

Algs.: XGBoostHardness Metrics: airlines
Hardness Metrics: **0.20**
Hardness Metrics: **0.18**
Dataset: 0.00
Dataset: 539 382
Attributes: 8
: 2.01

Algs.: LightGBMHardness Metrics: artifcial-characters
Hardness Metrics: 0.13
Hardness Metrics: **0.11**
Dataset: 0.00
Dataset: 10 218
Attributes: 8
: 0.63

Algs.: XGBoostHardness Metrics: colic
Hardness Metrics: 0.13
Hardness Metrics: **0.11**
Dataset: 0.00
Dataset: 368
Attributes: 27
: 4.00

Algs.: CatBoostHardness Metrics: credit-approval
Hardness Metrics: 0.12
Hardness Metrics: **0.08**
Dataset: 0.00
Dataset: 690
Attributes: 16
: 74.77

Algs.: CatBoostHardness Metrics: jasmine
Hardness Metrics: 0.13
Hardness Metrics: **0.13**
Dataset: 0.00
Dataset: 2 984
Attributes: 145
: 47.60

Algs.: CatBoostHardness Metrics: kc1
Hardness Metrics: 0.14
Hardness Metrics: **0.07**
Dataset: 0.00
Dataset: 2 109
Attributes: 22
: 28.34

Algs.: CatBoostHardness Metrics: phoneme
Hardness Metrics: 0.10
Hardness Metrics: **0.15**
Dataset: 0.00
Dataset: 5 404
Attributes: 6
: 1.23

Algs.: XGBoostHardness Metrics: qsar-biodeg
Hardness Metrics: 0.08
Hardness Metrics: **0.08**
Dataset: 0.05
Dataset: 1 055
Attributes: 42
: 93.24

Algs.: TabPFNHardness Metrics: cnae-9
Hardness Metrics: 0.11
Hardness Metrics: 0.04
Dataset: **0.10**
Dataset: 1 080
Attributes: 857
: NaN

Algs.: TabTransformerHardness Metrics: monks-problems-2
Hardness Metrics: 0.04
Hardness Metrics: 0.00
Dataset: **0.17**
Dataset: 601
Attributes: 7
: NaN

Algs.: SAINT## **3
TabZilla Benchmark Suite**In order to accelerate tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 ‘hardest’ of the 176 datasets we studied in Section 2. We use the following three criteria.**Hard for baseline algorithms.** As discussed in Section 2.1, simple baselines perform very well on a surprisingly large fraction of the datasets in our experiments. Therefore, to select our suite of hard datasets, we remove any dataset such that a baseline (as defined in Section 2) achieved a normalized log loss within 20% of the top-performing algorithm. This criterion is not perfect; for example, if a dataset is so hard that all 19 algorithms fail to reach non-trivial performance, it would not satisfy the criterion. However, the criterion is a good proxy for dataset hardness given the available information.**Hard for most algorithms.** This criterion is designed to include datasets on which most algorithms were not able to reach top performance. In particular, a dataset satisfies the criterion if the fourth-best log loss out of 19 algorithms is at least 7% worse than the top log loss. In other words, this criterion will include datasets on which one, two, or three algorithms were able to stand out in terms of performance. For example, if ten algorithms were all able to achieve a performance within 7% of the top-performing algorithm, we can reasonably assume that the dataset might not be ‘hard’. Interestingly, this criterion subsumes the majority of the datasets from the previous criterion.**Hard for GBDTs.** The first two criteria result in datasets such that GBDTs primarily are the top-performing methods. This is not surprising in light of the overall performance of GBDTs that we showed in Section 2. However, for the field of tabular data to progress, focusing on datasets for which GBDTs already perform well would leave a blindspot on datasets for which GBDTs perform poorly. Therefore, we add all datasets for which GBDTs perform 10% worse than the top-performing algorithm, in order to achieve a greater diversity of datasets.**TabZilla Characteristics.** Table 4 shows all datasets, their statistics, and their top three algorithms. Based on the criteria alone, the dataset characteristics are diverse, with sizes ranging from 148 to over 1 million, as well as a large range of the variance of kurtosis of the features (one of our measures of irregularity). In Table 5, we compare the performance of all algorithms on the benchmark suite. The top five algorithms are XGBoost, CatBoost, LightGBM, ResNet, and SAINT, with mean ranks of 3.27, 3.86, 6.06, 6.14, and 6.37, respectively. In order to accelerate research in tabular data, we release TabZilla as a collection in OpenML, and we open-source all of our computed metafeatures and results. In Appendix B, we give the full dataset documentation, including a datasheet .## **4
Conclusions and Future Work**In this work, we conducted the largest tabular data analysis to date, by comparing 19 approaches across 176 datasets. We found that the ‘NN vs. GBDT’ debate is overemphasized: for a surprisingly high number of datasets, either a simple baseline method performs on par with all other methods, or light hyperparameter tuning on a GBDT increases performance more than choosing the best algorithm. On the other hand, on average, GBDTs do outperform NNs. We also analyzed what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, GBDTs are better than NNs at handling various types of data irregularity. Finally, based on our analysis, we released TabZilla, a collection of the 36 ‘hardest’ out of the 176 datasets we studied: hard for baselines, most algorithms, and GBDTs. The goal in releasing TabZilla is to accelerate tabular data research by focusing on improving the current blind spots in the literature.Our work provides a large set of tools to accelerate research on tabular data. For example, researchers developing a new neural net for tabular data can use our open-source repository to immediately compare their method to 19 algorithms across 176 datasets. Researchers can also use our metafeature analysis to improve the weaknesses of current or future algorithms; for example, making neural nets more robust to data irregularities is a natural next step. Furthermore, researchers studying ensemble methods can weight the models differently for each dataset based on its metafeatures. Finally, our open-source collection of extensive datasets and metafeatures can make it easier for researchers to design new meta-learned  or pre-trained models for tabular data . There are several interesting ideas for extensions such as regression datasets, time-series forecasting datasets, studying uncertainty quantification, studying the effect of the percentage of categorical features on NNs, and studying more comprehensive hyperparameter optimization including regularization methods.## **References**-  Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, and Geoffrey E Hinton. Neural additive models: Interpretable machine learning with neural nets. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2021.-  Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , pages 2623–2631, 2019.-  Sercan Ö Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) , 2021.-  Kumar Arun, Garg Ishan, and Kaur Sanmeet. Loan approval prediction based on machine learning approach. IOSR J. Comput. Eng , 18(3):18–21, 2016.-  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.-  Anna L Buczak and Erhan Guven. A survey of data mining and machine learning methods for cyber security intrusion detection. IEEE Communications surveys & tutorials , 18(2):1153–1176, 2015.-  Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR) , 41(3):1–58, 2009.-  Jintai Chen, Kuanlun Liao, Yao Wan, Danny Z Chen, and Jian Wu. Danets: Deep abstract networks for tabular data classification and regression. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) , 2022.-  Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining , pages 785–794, 2016.-  William Jay Conover. Practical nonparametric statistics , volume 350. john wiley & sons, 1999.-  Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning , 1995.-  David R Cox. The regression analysis of binary sequences. Journal of the Royal Statistical Society: Series B (Methodological) , 1958.-  Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR) , 2021.-  Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics , pages 1189–1232, 2001.-  Milton Friedman. The use of ranks to avoid the assumption of normality implicit in the analysis of variance. Journal of the american statistical association , 1937.-  Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM , 64(12):86–92, 2021.-  Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2021.-  Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2022.-  Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-machine based neural network for ctr prediction. In IJCAI , 2017.-  John T Hancock and Taghi M Khoshgoftaar. Survey on categorical data for neural networks. Journal of Big Data , 7(1):1–41, 2020.-  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.-  Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS) , 2023.-  Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer that solves small tabular classification problems in a second. In Proceedings of the International Conference on Learning Representations (ICLR) , 2023.-  Alan Jeffares, Tennison Liu, Jonathan Crabbé, Fergus Imrie, and Mihaela van der Schaar. Tangos: Regularizing tabular neural networks through gradient orthogonalization and specialization. In Proceedings of the International Conference on Learning Representations (ICLR) , 2023.-  Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data , 3(1):1–9, 2016.-  Arlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular datasets. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 34, 2021.-  Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2017.-  Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2012.-  Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. Transfer learning with deep tabular models. ICLR , 2023.-  Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. Transfer learning with deep tabular models. In Proceedings of the International Conference on Learning Representations (ICLR) , 2023.-  Guido Lindner and Rudi Studer. Ast: Support for algorithm selection with a cbr approach. In Proceedings of the Third European Conference on Principles of Data Mining and Knowledge Discovery , PKDD ’99, page 418–423, Berlin, Heidelberg, 1999. Springer-Verlag.-  Zachary C Lipton and Jacob Steinhardt. Troubling trends in machine learning scholarship: Some ml papers suffer from flaws that could mislead the public and stymie future research. Queue , 2019.-  H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from the trenches. In Proceedings of the Annual Conference on Knowledge Discovery and Data Mining (KDD) , pages 1222–1230, 2013.-  Sergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivious decision ensembles for deep learning on tabular data. In Proceedings of the International Conference on Learning Representations (ICLR) , 2020.-  Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS) , 2018.J. Ross Quinlan. Induction of decision trees. Machine learning , 1986.-  Matthew Richardson, Ewa Dominowska, and Robert Ragno. Predicting clicks: estimating the click-through rate for new ads. In Proceedings of the 16th international conference on World Wide Web , pages 521–530, 2007.-  Mostafa A Salama, Aboul Ella Hassanien, and Kenneth Revett. Employment of neural network and rough set in meta-learning. Memetic Computing , 5:165–177, 2013.-  Ira Shavitt and Eran Segal. Regularization learning networks: deep learning for tabular datasets. Advances in Neural Information Processing Systems , 31, 2018.-  Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion , 81:84–90, 2022.-  Dennis Ulmer, Lotta Meijerink, and Giovanni Cinà. Trust issues: Uncertainty estimation does not enable reliable ood detection on medical tabular data. In Machine Learning for Health , pages 341–354. PMLR, 2020.-  Christopher J Urban and Kathleen M Gates. Deep learning: A primer for psychologists. Psychological Methods , 2021.-  Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machine learning. ACM SIGKDD Explorations Newsletter , 2014.-  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems , pages 5998–6008, 2017.-  Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimization initializations. In 2015 IEEE international conference on data science and advanced analytics (DSAA) , 2015.-  Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and Yuval Kluger. Feature selection using stochastic gates. In Proceedings of the International Conference on Machine Learning (ICML) , 2020.-  Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. Vime: Extending the success of self-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems , 33:11033–11043, 2020.## **A Broader Societal Impact Statement**The goal of our work is to conduct an analysis of tabular data, including the significance of ‘GBDTs vs. NNs’, as well as to provide a large set of tools for researchers and practitioners working on tabular data. We do not see any negative broader societal impacts of our work. In fact, our work shows that on a surprisingly large fraction of datasets, it is not necessary to train a resource-intensive neural net: a simple baseline, or tuning CatBoost, is enough to reach top performance. We even predict which datasets are more amenable to GBDTs: larger datasets, datasets with a high size-to-number-offeatures ratio, and ‘irregular’ datasets. Our hope is that our work will have a positive impact for both practitioners and researchers: by providing the largest analysis and open-source codebase to date, along with a benchmark suite of ‘hard’ datasets, our work can both accelerate future research and make the comparisons in future work more rigorous and comprehensive.## **B Dataset Documentation**## **B.  Maintenance plan**We plan to actively maintain the benchmark suite, and we welcome contributions from the community.## **B.  Code of conduct**Our Code of Conduct is from the Contributor Covenant, version 2.0. See“We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.”## **B.  Datasheet**## **Motivation For Datasheet Creation***Why was the datasheet created? (e.g., was there a specific task in mind? was there a specific gap that needed to be filled?) The goal of releasing the TabZilla Benchmark Suite is to accelerate research in tabular data by introducing a set of ‘hard’ datasets. Specifically, simple baselines cannot reach top performance, and most algorithms (out of the 19 we tried) cannot reach top performance. We found that a surprisingly high percentage of datasets used in tabular research today are such that a simple baseline can reach just as high accuracy as the leading methods.*Has the dataset been used already? If so, where are the results so others can compare (e.g., links to published papers)? All of the individual datasets are already released in OpenML, and many have been used in prior work on tabular data. However, our work gathers these datasets into a single ‘hard’ suite.*What (other) tasks could the dataset be used for? All of these datasets are tabular classification datasets, and so to the best of our knowledge, they cannot be used for anything other than tabular classification.*Who funded the creation dataset? This benchmark suite was created by researchers at Abacus.AI, Stanford, Pinterest, University of Maryland, IIT Bombay, New York University, and Caltech. Funding for the dataset computation itself is from Abacus.AI.## **Datasheet Composition***What are the instances?(that is, examples; e.g., documents, images, people, countries) Are there multiple types of instances? (e.g., movies, users, ratings; people, interactions between them; nodes, edges) Each instance is a tabular datapoint. The makeup of each point depends on its dataset. For example, three of the datasets consist of poker hands, electricity usage, and plant textures.*How many instances are there in total (of each type, if appropriate)? See Table 4 for a breakdown of the number of instances for each dataset.*What data does each instance consist of ? “Raw” data (e.g., unprocessed text or images)? Features/attributes? Is there a label/target associated with instances? If the instances related to people, are subpopulations identified (e.g., by age, gender, etc.) and what is their distribution? The raw data is hosted on OpenML. In our repository, we also contain scripts for the standard preprocessing we ran before training tabular data models. The data are not related to people.*Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. There is no missing information.*Are relationships between individual instances made explicit (e.g., users’ movie ratings, social network links)? If so, please describe how these relationships are made explicit. There are no relationships between individual instances.*Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).We selected the datasets for our benchmark suite as follows. We started with 176 datasets, which we selected with the aim to include most classification datasets from popular recent papers that study tabular data , including datasets from the OpenML-CC18 suite , the OpenML Benchmarking Suite , and additional OpenML datasets . Due to the scale of our experiments (538 650 total models trained), we limited to datasets smaller than 1.1M. CC-18 and OpenML Benchmarking Suite are both seen as the go-to standards for conducting a fair, diverse evaluation across algorithms due to their rigorous selection criteria and wide diversity of datasets . Out of these 176 datasets, we selected 36 datasets for our suite as described in Section 3.*Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.We use the 10 folds from OpenML, and it is recommended to report performance averaged over these 10 folds, as we do and as OpenML does. If a validation set is required, we recommend additionally using the validation splits that we used, described in Section 2.*Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. There are no known errors, sources of noise, or redundancies.*Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. The dataset is self-contained.## **Collection Process***What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? We did not create the individual datasets. However, we selected the datasets for our benchmark suite as follows. We started with 176 datasets, which we selected with the aim to include most classification datasets from popular recent papers that study tabular data , including datasets from the OpenML-CC18 suite , the OpenML Benchmarking Suite , and additional OpenML datasets . Due to the scale of our experiments (538 650 total models trained), we limited to datasets smaller than 1.1M. CC-18 and OpenML Benchmarking Suite are both seen as the go-to standards for conducting a fair, diverse evaluation across algorithms due to their rigorous selection criteria and wide diversity of datasets .Out of these 176 datasets, we selected 36 datasets for our suite as described in Section 3.*How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. The datasets were selected using the three criteria from Section 3.*If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? As described earlier, the datasets were selected using the three criteria from Section 3.*Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The creation of the TabZilla Benchmark Suite was done by the authors of this work.*Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. The timeframe for constructing the TabZilla Benchmark Suite was from April 15, 2023 to June 1, 2023.## **Data Preprocessing***Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section.We include both the raw data and the preprocessed data. We preprocessed the data by imputing each NaN to the mean of the respective feature. We left all other preprocessing (such as scaling) to the algorithms themselves.*Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the “raw” data.*Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.*Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? If not, what are the limitations? We hope that the release of this benchmark suite will achieve our goal of accelerating research in tabular data, as well as making it easier for researchers and practitioners to devise and compare algorithms. Time will tell whether our suite will be adopted by the community.## **Dataset Distribution***When will the dataset be released/first distributed? What license (if any) is it distributed under? The benchmark suite is public as of June 1, 2023, distributed under the Apache License 2.0.*Are there any copyrights on the data? There are no copyrights on the data.*Are there any fees or access/export restrictions? There are no fees or restrictions.## **Dataset Maintenance***Who is supporting/hosting/maintaining the dataset? The authors of this work are supporting/hosting/maintaining the dataset.*Will the dataset be updated? If so, how often and by whom? We welcome updates from the tabular data community. If new algorithms are created, the authors may open a pull request to include their method.*If others want to extend/augment/build on this dataset, is there a mechanism for them to do so? If so, is there a process for tracking/assessing the quality of those contributions. What is the process for communicating/distributing these contributions to users? Others can create a pull request on GitHub with possible extensions to our benchmark suite, which will be approved case-by-case. For example, an author of a new hard tabular dataset may create a PR in our codebase with the new dataset. These updates will again be communicated on the GitHub README.## **Legal and Ethical Considerations***Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. There was no ethical review process. We note that our benchmark suite consists of existing datasets that are already publicly available on OpenML.*Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctorpatient confidentiality, data that includes the content of individuals non-public communications)? If so, please provide a description. The datasets do not contain any confidential data.*Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why None of the data might be offensive, insulting, threatening, or otherwise cause anxiety.*Does the dataset relate to people? If not, you may skip the remaining questions in this section. The datasets do not relate to people.## **C Additional Related Work****Gradient-boosted decision trees.** GBDTs iteratively build an ensemble of decision trees, with each new tree fitting the residual of the loss from the previous trees, using gradient descent to minimize the losses. GBDTs have been a powerful tool for modeling tabular data ever since their creation in 2001 , and numerous works propose high-performing GBDT variants. XGBoost (eXtreme Gradient Boosting)  uses weighted quantile sketching and sparsity-awareness, allowing it to scale to large datasets. LightGBM (Light Gradient Boosting Machine)  uses gradient-based one-sided sampling and exclusive feature bundling to create a faster and more lightweight GBDT implementation. CatBoost (Categorical Boosting)  introduces ordered boosting, a new method for handling categorical features, as well as better methods for handling missing values and outliers.**Neural networks for tabular data.** Borisov et al. described three types of tabular data approaches for neural networks . Data transformation methods  seek to encode the data into a format that is better-suited for neural nets. Architecture-based methods design specialized neural architectures for tabular data , a large sub-class of which are transformer-based architectures . Regularization-based methods specially tailor regularizers methods to improve the performance of a given architecture . Notably, one recent work designs a new framework for regularization in the tabular setting built on latent unit attributions , and another recent work shows that searching for the optimal combination/cocktail of 13 regularization techniques applied to a simple neural net achieves strong performance. While regularization was not the focus of our current work, including regularization methods would be an exciting direction for follow-up work.**GBDTs versus NNs.** Several recent works compare GBDTs to NNs on tabular data, often finding that either neural nets  or GBDTs  perform best. Shwartz-Ziv and Armon compare GBDTs and NNs on 30 datasets, finding that GBDTs perform better on average, and ensembling both achieves better performance . Kadra et al. compare GBDTs and NNs on 40 datasets, finding that properly-tuned neural networks perform best on average .Gorishniy et al.  introduce a ResNet-like  architecture, and FT-Transformer, a transformerbased  architecture. Across experiments on eleven datasets, they conclude that there is still no universal winner among GBDTs and NNs. Borisov et al.  compare classical machine learning methods with eleven deep learning approaches on five tabular datasets, concluding that GBDTs still have the edge. In the transfer learning setting, Levin et al.  find that neural networks have a decisive edge over GBDTs when pre-training data is available.Perhaps the most related work to ours is by Grinsztajn et al. , who investigate why tree-based methods outperform neural nets on tabular data. There are a few differences between their work and ours. First, they only consider seven algorithms and 45 datasets, compared to our 19 algorithms and 176 datasets. Second, their dataset sizes range from 3 000 to 10 000, or seven that are exactly 50 000, in contrast to our dataset sizes which range from 32 to 1 025 009 . Additionally, they# Inst. # Feats. # Classes Min-Max Class Freq.: mean 30567 223 6 0.48 std 106943 786 12 0.35 min 32 2 2 2e-05 25% 596 9 2 0.14 50% 2218 21 2 0.46 75% 11008 61 6 0.82 max 1025009 7200 100 1.00
# Feature Types: 206 25 17 781 144 119 0 0 0 4 0 0 10 0 0 50 2 8 7200 1555 1555further control their study, for example by upper bounding the ratio of size to features, by removing high-cardinality categorical feature, and by removing low-cardinality numerical features. While this has the benefit of being a more controlled study, their analysis misses out on some of our observations, such as GBDTs performing better than NNs on ‘irregular’ datasets. Finally, while Grinsztajn et al. focused in depth on a few metafeatures such as dataset smoothness and number of uninformative features, our work considers orders of magnitude more metafeatures. Again, while each approach has its own strengths, our work is able to discover more potential insights, correlations, and takeaways for practitioners.## **D Additional Experiments**We give additional experiments, including dataset statistics (Appendix D.1), additional results from Section 2.  (Appendix D.2), and additional results from Section 2.  (Appendix D.3).## **D.  Dataset statistics**Table 6 shows summary statistics for all 176 datasets used in our experiments. Roughly half of our datasets have a binary classification target (and as many as 100 target classes), and roughly half of all training sets have fewer than 2300 instances—though many datasets have tens of thousands of instances.## **D.  Additional experiments from Section 2.1**In this section, we give additional experiments from Section 2.1, including relative performance tables, training time analysis, and critical difference diagrams.## **D.2.  Relative performance tables**First, we show the ranking of all tuned algorithms according to each performance metric, averaged over datasets: log loss , F1 score , and ROC-AUC . These are similar to Table 1, but with different metrics. Rankings are calculated by first averaging tuned performance over all 10 splits of each dataset, and then ranking each algorithm for each dataset according to their average performance. For these rankings, a tie between two algorithms is indicated by the lowest (best) ranking. So if multiple algorithms achieve the highest average accuracy for a dataset, they both receive ranking 1.Some algorithms perform well even without hyperparameter tuning. Next, we calculate the ranking of all datasets using the same procedure as in Table 1 and the previous tables, but with their default hyperparameter set displayed as a separate algorithm. We show ranking results for test accuracy. See Table 10. Many of the best-performing algorithms, including CatBoost, XGBoost, LightGBM, and ResNet, perform fairly well both with and without hyperparameter tuning.Now, we present a table similar to Table 1, but with the full set of 176 datasets. See Table 11. Note that some algorithms did not complete on all 176 datasets, but the ordering of the algorithms is nearly the same as in Table 1. We also include partial results for NAM , DeepFM , and TabTransformer .## **D.2.  Training time analysis**In this section we analyze the relative training time required by each algorithm. Here we only consider algorithms with their default hyperparameters, so no tuning is used. Table 12 shows a ranking of all algorithms, according to the average training time per 1 000 training samples; as before, we separately present results for datasets of size less than or equal to 1250 in Table 13. These rankings are calculated by first taking the average training time per 1 000 samples over all 10 folds of all 176 datasets, and then ranking each algorithm for each dataset according to this average train time.## **D.2.  HPO Plot**In Figure 4, we plotted the performance improvement of hyperparameter tuning on CatBoost, compared to the absolute performance difference between the best neural net and the best GBDT using default hyperparameters. Now, we also give the same plot for ResNet. See Figure 6.## **D.2.  Critical difference diagrams**Figure 7 shows a critical difference plot according to F1 score. Note that we repeat the Friedman test four times with rankings of the same datasets and algorithms. However it is unlikely that our findings would change, given that p-values for these tests without correction are extremely small (p < 10 [−] ).We compare the performance of each algorithm family (GBDTs, NNs, and baselines). Here we use all 176 datasets. We use the same methodology here as in previous sections. See Figure 8 (log loss) and Figure 9 (F1 score).## **D.2.  Venn Diagrams**Recall from Section 2 that for our Venn diagram plots, we split the 19 algorithms into three families : GBDTs (CatBoost, XGBoost, LightGBM), NNs (listed in Section 2), and baselines (Decision Tree, KNN, LinearModel, RandomForest, SVM). To compare algorithm performance across datasets, we100<br>10 1<br>10 2<br>10 3<br>10 4<br>10 5<br>0 10 4 10 2 100 0 10 4 10 2 100<br>CatBoost tuning improvement ResNet tuning improvement<br>|T<br>D<br>B<br>G<br>N -<br>N<br>lt u<br>fae<br>d<br>t<br>s<br>e<br>|B<br>**----- End of picture text -----**<br>F1<br>18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1<br>VIME CatBoost<br>KNN XGBoost<br>LinearModel LightGBM<br>MLP ResNet<br>STG SAINT<br>TabNet RandomForest<br>DecisionTree FTTransformer<br>MLP-rtdl DANet<br>SVM NODE<br>**----- End of picture text -----**<br>Log Loss<br>3 2 1<br>baseline gbdt<br>neural<br>**----- End of picture text -----**<br>F1<br>3 2 1<br>baseline gbdt<br>neural<br>**----- End of picture text -----**<br>RandomForest (default): FTTransformer (default)
1: 1
35: 35
16.22: 18.01
15.0: 20.0
0.76: 0.71
0.82: 0.80
0.20: 0.21
0.14: 0.14
0.50: 25.92
0.39: 15.35
RandomForest (default): DecisionTree (default)
1: 1
35: 35
16.22: 21.88
15.0: 23.0
0.76: 0.63
0.82: 0.70
0.20: 0.24
0.14: 0.16
0.50: 0.02
0.39: 0.01use min-max scaling as described in Section 2. In Figure 4, we said that an algorithm is ‘highperforming’ if it achieves a scaled test-set accuracy of at least 0 . 99, and then we determine which algorithm families (GBDTs, NNs, baselines) have a high-performing algorithm. Now we compute the same Venn diagram, when tightening the definition of high-performing to 0 . 9999 scaled accuracy. See Figure 10. In this case, GBDTs are the sole high-performing algorithm family for 42% of datasets, while NNs are the sole high-performing algorithm family for 30% of datasets. However, since these differences are smaller than 0.1%, they may not be significant to practitioners.## **D.2.  Dataset size analysis**In this section, we investigate the association between dataset size and performance. In Section 2.  we show that, when compared to NNs and baselines, GBDTs perform relatively better with larger datasets; this is based on a negative correlations between normalized log loss and dataset size. However, it is more informative to compare performance of individual algorithms rather than algorithm families. For example, Figure 11 compares the rank of three algorithms: CatBoost, SAINT, and TabNet, with dataset size. In the left panel (CatBoost minus TabNet), CatBoost outperforms TabNet for all datasets up to size roughly 1 500. For larger datasets there is little difference between CatBoost andHigh - Performing Algorithm Family over 176
Datasets<br>Threshold = 0.9999<br>GBDTs<br>Neural Nets<br>(3502%) (35%) (4724%)<br>5 1<br>(3%) (1%)<br>32<br>(18%)<br>7<br>Baselines (4%)<br>**----- End of picture text -----**<br>TabNet; this indicates that CatBoost should be chosen over TabNet for smaller datasets, and that both algorithms are comparable for larger datasets On the other hand, the center panel (CatBoost minus SAINT), indicates that both CatBoost and SAINT have comparable performance for all datasets up to those with size 1 500; for larger datasets, CatBoost outperforms SAINT. This indicates that CatBoost should be chosen over SAINT for very large datasets, but for small datasets both algorithms are comparable.**The main takeaway** from these findings is that practitioners should not focus on choosing an algorithm family, such as NNs or GBDTs, to focus on. For example, TabPFN and TabNet are both neural nets, but TabPFN does comparatively better on smaller datasets, while TabNet does comparatively better on larger datasets. Rather they can consult our metadataset of results to decide which algorithm is appropriate for their specific use case. General trends are helpful, but not sufficient, for selecting an effective algorithm.## **D.  Additional experiments from Section 2.2**Recall from Section 2.  that ∆ ℓℓ denotes the difference in normalized log loss between the best neural net and the best GBDT method. Table 14 and Figure 12 shows the dataset properties with the largest absolute correlation with ∆ ℓℓ .To evaluate the predictive power of dataset properties, we train several decision tree models using the train/test procedure above, with a binary outcome: 1 if ∆ ℓℓ> 0 (the best neural net beats the best GBDT), and 0 otherwise. Table 15 shows the performance accuracy of decision trees trained on this task, with varying depth levels; we also include an XGBoost model for comparison. Finally, we include a visual depiction of a simple depth-3 decision tree, in Figure 13. The decision tree classifies which of the top five algorithms performs the best. Note that the decision splits are based purely on maximizing information gain at that point in the tree.CatBoost minus TabNet CatBoost minus SAINT SAINT minus TabNet<br>1.0<br>0.5<br>0.0<br>0.5<br>1.0<br>102 103 104 102 103 104 102 103 104<br>Dataset Size Dataset Size Dataset Size<br>bi)(CLLNTBt t teas unmsooa i)(SCLLT NIABt ts unmsooa bi)(SLLNTT NIAt eas unm<br>**----- End of picture text -----**<br>1.  corr. = 0.  corr. = 0.  corr. = - 0.  corr. = - 0.33<br>0.75<br>0.50<br>0.25<br>0.00<br>104 103 10 2 10 1 100 10 3<br>Dataset Size Inst -T o - Attributes Med. Canon. - Corr. Min. Class Freq.<br>)T<br>D<br>B<br>G<br>l -<br>aru<br>e<br>(N<br>ll<br>**----- End of picture text -----**<br>Is the number of instances <= 4211?<br>Yes No<br>Is the number of instances <= 1132? Is the min. of the sparsity metric<br>over all features <= 0.044?<br>Yes No Yes No<br>Is the number of numerical Is the median of the concentration Is the max. of the relative frequency of Is the ratio of features<br>features without NaNs <= 65? coefficient between all features and all target classes <= 0.792? to instances <= 0.003?<br>target classes <= 0.025?<br>Yes No Yes No Yes No Yes No<br>TabPFN ResNet XGBoost CatBoost XGBoost CatBoost SAINT ResNet<br>TabPFN 189 1 0 0 0 0 0 0<br>ResNet 111 28 60 13 32 24 1 11<br>SAINT 131 1 54 5 3 10 56 2<br>CatBoost 142 0 51 81 32 69 0 0<br>XGBoost 115 2 79 27 293 67 13 7<br>101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103 101 102 103<br>**----- End of picture text -----**<br>Finally, we present the metafeatures most-correlated with the difference in log loss between pairs of algorithms. We consider the two best-performing algorithms from each family: CatBoost, XGBoost, ResNet, and SAINT. See Table 19 and Table 20 for statistical and general metafeatures, respectively.## **D.  Experiments on Regression Datasets**While our experiments focus on classification datasets, the TabZilla codebase is also equipped to handle regression datasets—which have a continuous target variable rather than categorical or binary. We run experiments using 12 algorithms with 17 tabular regression datasets, using the same experiment design and parameters described in Section 2. Each algorithm is tuned for each dataset by maximizing the R-squared (R2) metric. The regression datasets used in these experiments have been used in recent studies of machine learning with tabular data ; each dataset corresponds to an OpenML task, and can be preprocessed exactly like the classification datasets used in other experiments. The datasets used in these experiments are “Bank-Note-AuthenticationUCI” (OpenML task 361002), “EgyptianSkulls” (5040), “Wine” (190420), “Wisconsin-breastcancer-cytology-features” (361003), “bodyfat” (5514), “california” (361089), “chscase-foot” (5012),“cleveland” (2285), “colleges” (359942), “cpu-small” (4883), “dataset-sales” (190418), “kin8nm” (2280), “liver-disorders” (52948), “meta” (4729), “mv” (4774), “pbc” (4850), and “veteran” (4828).Table 16 shows the rankings of 12 algorithms on these 17 regression datasets, according to the R2 metric calculated on the test set. The general conclusions are similar to our findings with classification datasets: most algorithms perform well and poorly on at least one dataset; however, GBDTs perform particularly well, especially CatBoost.## **D.  Additional Results with Quantile Scaling**In this section, we discuss dataset preprocessing. Different papers use a variety of different preprocessing methods, and there is also a wide range in the amount of ‘built-in’ preprocessing techniques inside the algorithms themselves. Therefore, our main results minimize confounding factors by having a consistent, lightweight preprocessing (imputing NaN values). In this section, we compare 13 algorithms on the tabzilla benchmark suite with and without quantile scaling, one of the most popular techniques, for all continuous features. We use QuantileTransformer from scikit-learn . We use the same computational setup and experiment design as in our main experiments. See Table 17. We find that quantile scaling improves the simple algorithms: decision tree, MLP, random forest, and SVM, while it has little effect on the high-performing algorithms.## **D.  Experiments with Additional Hyperparameter Optimization**In our main experiments, for hyperparameter optimization (HPO), we ran 30 iterations of random search for all algorithms. In this section, we test the impact of additional HPO for four algorithms: XGBoost, CatBoost, LightGBM, and RandomForest. We did not run additional HPO experiments on any neural net methods due to the substantial compute resources required.Table 18 shows the performance of these HPO experiments (algorithm suffix “(HPO)”), compared with with the performance of the default hyperparameters (suffix “(default)”), and the performance after 30 iterations of random hyperparameter search as in our main results (no suffix). As expected, additional hyperparameter tuning improves the performance of XGBoost, CatBoost, LightGBM, and RandomForest.## **D.  Forward Feature Selection for Identifying Important Dataset Attributes**In this section, we present a different method for determining which dataset attributes are related to performance differences between algorithms. Here we use greedy forward feature selection  to identify important dataset attributes. In these experiments, we study the problem of predicting the difference in normalized log loss between CatBoost and ResNet (two very effective GBDT and NN algorithms), using metafeatures.At a high level, greedy forward feature selection selects metafeatures sequentially which improve the performance of the meta-model. To evaluate performance we use leave-one-dataset-out cross validation: each dataset contributes 10 folds to the overall metadataset, so each fold includes 10 instances for validation and all remaining instances for training.1. Number of features normally-distributed, according to the Shapiro-Wilk test.2. Median value of the minimum of all features.3. Median value of the sparsity of all features.4. Interquartile range of the mean value of all features.5. Mean of the harmonic mean of all features.CatBoost: CatBoost
ResNet: ResNet
-0.23: -0.23
Log of the standard deviation of the kurtosis of all features.: Log of the standard deviation of the skewness of all features.CatBoost: CatBoost
ResNet: ResNet
-0.23: 0.22
Log of the standard deviation of the kurtosis of all features.: Log of the median of the absolute value of the covariance between all featureCatBoost: CatBoost
ResNet: ResNet
-0.23: 0.21
Log of the standard deviation of the kurtosis of all features.: Log of the median of the standard deviation of all features.CatBoost: CatBoost
ResNet: ResNet
-0.23: 0.21
Log of the standard deviation of the kurtosis of all features.: Log of the median of the variance of all features.CatBoost: CatBoost
ResNet: ResNet
-0.23: 0.20
Log of the standard deviation of the kurtosis of all features.: Log of the median of the maximum value of all features.CatBoost: CatBoost
ResNet: ResNet
-0.23: 0.20
Log of the standard deviation of the kurtosis of all features.: Best performance of a naive Bayes classifer trained over 10-fold CV.CatBoost: CatBoost
ResNet: SAINT
-0.23: 0.26
Log of the standard deviation of the kurtosis of all features.: Best performance over all 10 folds, of 10-fold CV of a single-node decisionCatBoost: CatBoost
ResNet: SAINT
-0.23: 0.25
Log of the standard deviation of the kurtosis of all features.: Average performance over 10-fold CV of a single-node decision tree ft usingCatBoost: CatBoost
ResNet: SAINT
-0.23: 0.24
Log of the standard deviation of the kurtosis of all features.: Median performance over 10 folds, for 10-fold CV of a single-node decisionCatBoost: CatBoost
ResNet: SAINT
-0.23: -0.24
Log of the standard deviation of the kurtosis of all features.: Log of the worst performance over 10-fold CV of a single-node decision treeCatBoost: CatBoost
ResNet: SAINT
-0.23: -0.23
Log of the standard deviation of the kurtosis of all features.: Log of the kurtosis of the performance of a single-node decision tree ft usingCatBoost: CatBoost
ResNet: SAINT
-0.23: -0.23
Log of the standard deviation of the kurtosis of all features.: Log of the best performance of elite-nearest-neighbor over 10-fold CV.CatBoost: XGBoost
ResNet: ResNet
-0.23: -0.28
Log of the standard deviation of the kurtosis of all features.: Log of the standard deviation of the kurtosis of all features.CatBoost: XGBoost
ResNet: ResNet
-0.23: -0.27
Log of the standard deviation of the kurtosis of all features.: Log of the standard deviation of the skewness of all featuresCatBoost: XGBoost
ResNet: ResNet
-0.23: 0.25
Log of the standard deviation of the kurtosis of all features.: Log of the median value of the absolute covariance between all pairs ofCatBoost: XGBoost
ResNet: ResNet
-0.23: 0.24
Log of the standard deviation of the kurtosis of all features.: Log of the median standard deviation of all features.CatBoost: XGBoost
ResNet: ResNet
-0.23: 0.23
Log of the standard deviation of the kurtosis of all features.: Log of the median maximum-value of all features.CatBoost: XGBoost
ResNet: ResNet
-0.23: 0.22
Log of the standard deviation of the kurtosis of all features.: Best performance of a naive Bayes classifer over 10-fold CV.CatBoost: XGBoost
ResNet: ResNet
-0.23: 0.22
Log of the standard deviation of the kurtosis of all features.: Log of the best performance of a naive Bayes classifer over 10-fold CV.CatBoost: XGBoost
ResNet: SAINT
-0.23: -0.23
Log of the standard deviation of the kurtosis of all features.: Noisiness of the features:( i Si − i MI(i, y))/  i MI(i, y), whereSiCatBoost: is the entropy of featurei, andMI(i, y)is the mutual information betweenCatBoost: XGBoost
ResNet: SAINT
-0.23: -0.22
Log of the standard deviation of the kurtosis of all features.: Log of the standard deviation of the kurtosis of all features.CatBoost: XGBoost
ResNet: SAINT
-0.23: -0.20
Log of the standard deviation of the kurtosis of all features.: Log of the standard deviation of the skewness of all features.CatBoost: XGBoost
ResNet: SAINT
-0.23: -0.20
Log of the standard deviation of the kurtosis of all features.: Best performance over 10-fold CV of a single-node decision tree ft usingCatBoost: XGBoost
ResNet: SAINT
-0.23: -0.19
Log of the standard deviation of the kurtosis of all features.: Log of the standard deviation of the absolute correlation between all pairs ofCatBoost: XGBoost
ResNet: SAINT
-0.23: 0.18
Log of the standard deviation of the kurtosis of all features.: Log of the range of the performance of a decision tree trained on a randomCatBoost: CatBoost
ResNet: ResNet
0.14: -0.14
Log of the ratio of number of features to dataset size.: Log of the ratio of dataset size to number of features.CatBoost: CatBoost
ResNet: ResNet
0.14: -0.06
Log of the ratio of number of features to dataset size.: Range of the relative frequency of each target class.CatBoost: CatBoost
ResNet: ResNet
0.14: -0.06
Log of the ratio of number of features to dataset size.: Log of the maximum frequency of any target class.CatBoost: CatBoost
ResNet: ResNet
0.14: -0.05
Log of the ratio of number of features to dataset size.: Interquartile range of the relative frequency of all target classes.CatBoost: CatBoost
ResNet: ResNet
0.14: -0.04
Log of the ratio of number of features to dataset size.: Maximum relative frequency of any target class.CatBoost: CatBoost
ResNet: SAINT
0.14: -0.19
Log of the ratio of number of features to dataset size.: Standard deviation of the relative frequency of all target classes.CatBoost: CatBoost
ResNet: SAINT
0.14: 0.18
Log of the ratio of number of features to dataset size.: Kurtosis of the relative frequency of all target classes.CatBoost: CatBoost
ResNet: SAINT
0.14: -0.18
Log of the ratio of number of features to dataset size.: Maximum relative frequency of all target classes.CatBoost: CatBoost
ResNet: SAINT
0.14: -0.17
Log of the ratio of number of features to dataset size.: Log of the median relative frequency of all target classes.CatBoost: CatBoost
ResNet: SAINT
0.14: 0.15
Log of the ratio of number of features to dataset size.: Skewness of the relative frequency of target classes.CatBoost: XGBoost
ResNet: ResNet
0.14: -0.20
Log of the ratio of number of features to dataset size.: Log of the ratio of dataset size to number of features.CatBoost: XGBoost
ResNet: ResNet
0.14: 0.20
Log of the ratio of number of features to dataset size.: Log of the ratio of number of features to dataset size.CatBoost: XGBoost
ResNet: ResNet
0.14: 0.11
Log of the ratio of number of features to dataset size.: Log of the minimum relative frequency of all target classes.CatBoost: XGBoost
ResNet: ResNet
0.14: 0.08
Log of the ratio of number of features to dataset size.: Median relative frequency of all target classes.CatBoost: XGBoost
ResNet: ResNet
0.14: 0.07
Log of the ratio of number of features to dataset size.: Minimum relative frequency of all target classes.CatBoost: XGBoost
ResNet: SAINT
0.14: -0.17
Log of the ratio of number of features to dataset size.: Standard deviation of the relative frequency of all target classes.CatBoost: XGBoost
ResNet: SAINT
0.14: -0.16
Log of the ratio of number of features to dataset size.: Interquartile range of the relative frequency of all target classes.CatBoost: XGBoost
ResNet: SAINT
0.14: 0.15
Log of the ratio of number of features to dataset size.: Log of the ratio of dataset size to number of features.CatBoost: XGBoost
ResNet: SAINT
0.14: -0.15
Log of the ratio of number of features to dataset size.: Log of the ratio of number of features to dataset size.CatBoost: XGBoost
ResNet: SAINT
0.14: -0.15
Log of the ratio of number of features to dataset size.: Maximum relative frequency of all target classes.CatBoost: XGBoost
ResNet: SAINT
0.14: -0.13
Log of the ratio of number of features to dataset size.: Range of the relative frequency of all target classes.CatBoost: XGBoost
ResNet: SAINT
0.14: -0.13
Log of the ratio of number of features to dataset size.: Mean of the relative frequency of all target classes.CatBoost: XGBoost
ResNet: SAINT
0.14: -0.12
Log of the ratio of number of features to dataset size.: Median of the relative frequency of all target classes.